{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *** Unified PCL Trainer for pendulum ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_wrapper\n",
    "import env_spec\n",
    "import objective_PCL\n",
    "import policy\n",
    "import baseline\n",
    "import replay_buffer\n",
    "import optimizers\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'objective_PCL' from '/home/adrian/PycharmProjects/PCL/objective_PCL.py'>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(policy)\n",
    "imp.reload(baseline)\n",
    "imp.reload(env_spec)\n",
    "imp.reload(gym_wrapper)\n",
    "imp.reload(objective_PCL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_str = 'Pendulum-v0'\n",
    "env_str = 'HalfCheetah-v1'\n",
    "env_gym = gym_wrapper.GymWrapper(env_str,\n",
    "                             distinct= 1,\n",
    "                             count= 1)\n",
    "\n",
    "env_spec_gym =  env_spec.EnvSpec(env_gym.get_one())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup required algorithm parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = '/home/adrian/Schreibtisch/Uni/Data-Innovation-Lab/tensorflowlogs'\n",
    "\n",
    "batch_size_episodes = 1\n",
    "replay_batch_size = 25 #--> replay batch size; defaults to batch_size\n",
    "batch_by_steps = False #--> ensure each training batch has batch_size * max_step steps\n",
    "\n",
    "cutoff_agent = 1000\n",
    "max_step = 4000\n",
    "unify_episodes = False\n",
    "\n",
    "total_number_of_steps = 1000\n",
    "critic_weight = 0.0\n",
    "policy_weight = 1.0\n",
    "learning_rate = 0.002\n",
    "# Clip_adv (Advantage) is required in objective class\n",
    "clip_adv = 1.0 # --> Clip advantages at this value --> Leave as 0 to not clip at all\n",
    "# Clip norm is required in objective class to clip the gradients\n",
    "clip_norm = 40\n",
    "# Entropy regularization paramter tau\n",
    "tau = 0.1 # --> If using decaying tau, this is the final value\n",
    "tau_decy = None # --> decay tau by this much every 100 steps\n",
    "tau_start = 0.1 # --> start tau at this value\n",
    "gamma = 0.995 # --> Discount factor\n",
    "rollout = 10 # --> Rollout length for PCL objective\n",
    "# If we use unified episodes we need to ensure our batch_size_episodes is 1\n",
    "unify_episodes = False #--> Make sure replay buffer holds entire episodes, even across distinct sampling steps\n",
    "batch_by_steps = False #--> ensure each training batch has batch_size * max_step steps'\n",
    "\n",
    "# Neural network settings now\n",
    "input_prev_actions = True #--> Required for unified PCL since Q(a.s) is modeled\n",
    "recurrent = True #--> Indicate that we are going to use a recurrent policy (Q function approximator)\n",
    "input_time_step = False #--> Indicator if the current time step scould also be an input to the model\n",
    "internal_dim = 64 #--> Internal RNN dimension \n",
    "fixed_std = False #--> fix the std in Gaussian distributions\n",
    "\n",
    "# If fixed we obtain the following std\n",
    "# log_std = tf.get_variable('std%d' % i, [1, sampling_dim // 2])\n",
    "\n",
    "# Settings related to value function if considered seperatly\n",
    "value_hidden_layers = 1 #--> number of hidden layers in value estimate\n",
    "\n",
    "# Settings related to the replay buffer\n",
    "replay_buffer_size = 20000\n",
    "replay_buffer_alpha = 0.1 #--> replay buffer alpha param\n",
    "replay_buffer_freq = 1 #--> replay buffer frequency (only supports -1/0/1)'\n",
    "eviction = 'fifo' #--> How to evict from replay buffer: rand/rank/fifo\n",
    "prioritize_by = 'step' #--> Prioritize replay buffer by \"rewards\" or \"step\"\n",
    "\n",
    "# The following are only required for Trust-PCL\n",
    "eps_lambda = 0.0 #--> start tau at this value\n",
    "update_eps_lambda = False #--> Update lambda automatically based on last 100 episodes\n",
    "max_divergence=0.001\n",
    "sample_from = 'target' \n",
    "update_eps_lambda = True\n",
    "target_network_lag=0.99\n",
    "max_divergence = 0.01\n",
    "\n",
    "# Determine what updates to use\n",
    "use_offline_batch = True\n",
    "use_online_batch = False\n",
    "\n",
    "# Sampling setup \n",
    "last_obs = env_spec_gym.initial_obs(1)\n",
    "last_act = env_spec_gym.initial_act(len(env_gym))\n",
    "last_pad = np.zeros(len(env_gym))\n",
    "\n",
    "start_episode = np.array([True] * len(env_gym))\n",
    "step_count = np.array([0] * len(env_gym))\n",
    "episode_running_rewards = np.zeros(len(env_gym))\n",
    "episode_running_lengths = np.zeros(len(env_gym))\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "total_rewards = []\n",
    "\n",
    "all_obs_global = []\n",
    "all_act_global = []\n",
    "all_pad_global = []\n",
    "rewards_global = []\n",
    "\n",
    "internal_state_global = np.array([initial_internal_state()] * len(env_gym))\n",
    "\n",
    "start_id_global = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup PCL-Objective object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Recurrent Policy - In the unified approach the output is also used as foundation to compute the values\n",
    "* Try recurrent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine unified baseline:\n",
    "* In case of a seperate network one needs to clarify the function of the \"input_policy_state\" paramter\n",
    "* If false the \"input\" looks like the following \"obs, action, time_step\"\n",
    "* If true the \"input\" looks like \"internal_policy_states, time_step\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "objective_pendulum = objective_PCL.PCL(learning_rate = learning_rate,\n",
    "                                     clip_norm = clip_norm,\n",
    "                                     policy_weight = policy_weight,\n",
    "                                     critic_weight = critic_weight,\n",
    "                                     tau = tau, \n",
    "                                     gamma = gamma,\n",
    "                                     rollout = rollout,\n",
    "                                     eps_lambda = eps_lambda,\n",
    "                                     clip_adv = clip_adv)\n",
    "\"\"\"\n",
    "policy_unified_pendulum = policy.Policy(env_spec_gym,\n",
    "                                       internal_dim,\n",
    "                                       recurrent = recurrent,\n",
    "                                       input_prev_actions = input_prev_actions)\n",
    "\"\"\"\n",
    "\n",
    "# Non recurrent policy\n",
    "policy_unified_pendulum = policy.MLPPolicy(env_spec_gym,\n",
    "                                       internal_dim,\n",
    "                                       recurrent = input_time_step,\n",
    "                                       input_prev_actions = input_prev_actions)\n",
    "\n",
    "\"\"\"\n",
    "# Unified model\n",
    "baseline_unified_pendulum = baseline.UnifiedBaseline(env_spec_gym,\n",
    "                                                    internal_dim,\n",
    "                                                    input_prev_actions=input_prev_actions,\n",
    "                                                    input_time_step=input_time_step,\n",
    "                                                    input_policy_state=recurrent,  # may want to change this\n",
    "                                                    n_hidden_layers=value_hidden_layers,\n",
    "                                                    hidden_dim=internal_dim,\n",
    "                                                    tau=tau,\n",
    "                                                    eps_lambda = eps_lambda)\n",
    "\"\"\"\n",
    "\n",
    "# Seperate Value function\n",
    "baseline_sep_pendulum = baseline.Baseline(env_spec_gym,\n",
    "                                          internal_dim,\n",
    "                                          input_prev_actions=input_prev_actions,\n",
    "                                          input_time_step=input_time_step,\n",
    "                                          input_policy_state=False,  # may want to change this\n",
    "                                          n_hidden_layers=value_hidden_layers,\n",
    "                                          hidden_dim=internal_dim,\n",
    "                                          tau=tau,\n",
    "                                          eps_lambda = eps_lambda)\n",
    "\n",
    "# Setup corresponding optimizer for value function\n",
    "value_opt_pendulum = optimizers.GradOptimization(\n",
    "                        learning_rate=learning_rate,\n",
    "                        max_iter=5,\n",
    "                        mix_frac=0.05)\n",
    "\n",
    "# Setup replay buffer\n",
    "replay_buffer_pendulum = replay_buffer.PrioritizedReplayBuffer(replay_buffer_size,\n",
    "                                                               alpha=replay_buffer_alpha,\n",
    "                                                               eviction_strategy=eviction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the unified case we do not need a value function optimizer \"get_value_opt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it es time to setup the tensorflow graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with setup of placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary placeholder\n",
    "\n",
    "avg_episode_reward = tf.placeholder(\n",
    "        tf.float32, [], 'avg_episode_reward')\n",
    "\n",
    "# sampling placeholders\n",
    "\n",
    "internal_state = tf.placeholder(tf.float32,\n",
    "                                    [None, policy_unified_pendulum.rnn_state_dim],\n",
    "                                    'internal_state')\n",
    "\n",
    "# One episode of observations (Time_Steps, Observation dimension)\n",
    "single_observation = []\n",
    "for i, (obs_dim, obs_type) in enumerate(zip(env_spec_gym.obs_dims, env_spec_gym.obs_types)):\n",
    "    if env_spec_gym.is_discrete(obs_type):\n",
    "        single_observation.append(\n",
    "            tf.placeholder(tf.int32, [None], 'obs%d' % i))\n",
    "    elif env_spec_gym.is_box(obs_type):\n",
    "        single_observation.append(\n",
    "            tf.placeholder(tf.float32, [None, obs_dim], 'obs%d' % i))\n",
    "    else:\n",
    "        assert False\n",
    "        \n",
    "# One episode of actions (Time_steps, action dimension)        \n",
    "single_action = []\n",
    "for i, (action_dim, action_type) in enumerate(zip(env_spec_gym.act_dims, env_spec_gym.act_types)):\n",
    "    if env_spec_gym.is_discrete(action_type):\n",
    "        single_action.append(\n",
    "            tf.placeholder(tf.int32, [None], 'act%d' % i))\n",
    "    elif env_spec_gym.is_box(action_type):\n",
    "        single_action.append(\n",
    "            tf.placeholder(tf.float32, [None, action_dim], 'act%d' % i))\n",
    "    else:\n",
    "        assert False\n",
    "        \n",
    "# training placeholders\n",
    "\n",
    "# Observations batch size many episodes of time length [batch size, time length, observation dim]\n",
    "observations = []\n",
    "for i, (obs_dim, obs_type) in enumerate(zip(env_spec_gym.obs_dims, env_spec_gym.obs_types)):\n",
    "    if env_spec_gym.is_discrete(obs_type):\n",
    "        observations.append(\n",
    "            tf.placeholder(tf.int32, [None, None], 'all_obs%d' % i))\n",
    "    else:\n",
    "        observations.append(\n",
    "            tf.placeholder(tf.float32, [None, None, obs_dim], 'all_obs%d' % i))\n",
    "        \n",
    "# Actions batch size many episodes of time length [batch size, time length, action dim]        \n",
    "actions = []\n",
    "for i, (action_dim, action_type) in enumerate(zip(env_spec_gym.act_dims, env_spec_gym.act_types)):\n",
    "    if env_spec_gym.is_discrete(action_type):\n",
    "        actions.append(\n",
    "            tf.placeholder(tf.int32, [None, None], 'all_act%d' % i))\n",
    "    if env_spec_gym.is_box(action_type):\n",
    "        actions.append(\n",
    "            tf.placeholder(tf.float32, [None, None, action_dim],\n",
    "                       'all_act%d' % i))\n",
    "        \n",
    "# Rewards of Batch Size many episodes of time length [batch size, time length]\n",
    "rewards = tf.placeholder(tf.float32, [None, None], 'rewards')\n",
    "# Indicator if episode has terminated \n",
    "terminated = tf.placeholder(tf.float32, [None], 'terminated')\n",
    "# Batch Size many episodes of time length indicators if episode has ended\n",
    "pads = tf.placeholder(tf.float32, [None, None], 'pads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('avg_episode_reward', avg_episode_reward)\n",
    "\n",
    "with tf.variable_scope('model', reuse=None):\n",
    "    # policy network\n",
    "    with tf.variable_scope('policy_net'):\n",
    "        # ,entropies, self_kls)\n",
    "        (policy_internal_states, logits, log_probs) = \\\n",
    "                policy_unified_pendulum.multi_step(observations,\n",
    "                                                internal_state,\n",
    "                                                actions)\n",
    "        out_log_probs = sum(log_probs)\n",
    "\n",
    "\n",
    "       # value network\n",
    "    with tf.variable_scope('value_net'):\n",
    "        (values,\n",
    "         regression_input,\n",
    "         regression_weight) = baseline_sep_pendulum.get_values(\n",
    "            observations, actions,\n",
    "            policy_internal_states, logits)\n",
    "\n",
    "    # target policy network\n",
    "    with tf.variable_scope('target_policy_net'):\n",
    "        (target_policy_internal_states,\n",
    "         target_logits, target_log_probs) = \\\n",
    "            policy_unified_pendulum.multi_step(observations,\n",
    "                                               internal_state,\n",
    "                                               actions)\n",
    "\n",
    "    # target value network\n",
    "    with tf.variable_scope('target_value_net'):\n",
    "        (target_values, _, _) = baseline_sep_pendulum.get_values(\n",
    "                            observations, actions,\n",
    "                            target_policy_internal_states, target_logits)\n",
    "    \n",
    "    \n",
    "\n",
    "    # construct copy op online --> target\n",
    "    all_vars = tf.trainable_variables()\n",
    "    online_vars = [p for p in all_vars if\n",
    "                 '/policy_net' in p.name or '/value_net' in p.name]\n",
    "    target_vars = [p for p in all_vars if\n",
    "                 'target_policy_net' in p.name or 'target_value_net' in p.name]\n",
    "    online_vars.sort(key=lambda p: p.name)\n",
    "    target_vars.sort(key=lambda p: p.name)\n",
    "    aa = target_network_lag\n",
    "    copy_op = tf.group(*[\n",
    "      target_p.assign(aa * target_p + (1 - aa) * online_p)\n",
    "    for online_p, target_p in zip(online_vars, target_vars)])    \n",
    "    \n",
    "    \n",
    "    # evaluate objective\n",
    "    (loss, raw_loss, regression_target,\n",
    "     gradient_ops, summary) = objective_pendulum.get(\n",
    "      rewards, pads,\n",
    "      values[:-1, :],\n",
    "      values[-1, :] * (1 - terminated),\n",
    "      log_probs,\n",
    "      target_log_probs)\n",
    "\n",
    "    regression_target = tf.reshape(regression_target, [-1])\n",
    "    \n",
    "    policy_vars = [\n",
    "        v for v in tf.trainable_variables()\n",
    "        if '/policy_net' in v.name]\n",
    "    value_vars = [\n",
    "        v for v in tf.trainable_variables()\n",
    "        if '/value_net' in v.name]\n",
    "\n",
    "\n",
    "# value optimizer\n",
    "if value_opt_pendulum is not None:\n",
    "    with tf.variable_scope('trust_region_value', reuse=None):\n",
    "        value_opt_pendulum.setup(\n",
    "            value_vars,\n",
    "            tf.reshape(values[:-1, :], [-1]),\n",
    "            regression_target,\n",
    "            tf.reshape(pads, [-1]),\n",
    "            regression_input, \n",
    "            regression_weight)\n",
    "    \n",
    "\n",
    " # we re-use variables for the sampling operations\n",
    "with tf.variable_scope('model', reuse=True):\n",
    "    scope = ('target_policy_net' if sample_from == 'target'\n",
    "            else 'policy_net')\n",
    "    with tf.variable_scope(scope):\n",
    "        next_internal_state, sampled_actions = \\\n",
    "            policy_unified_pendulum.sample_step(single_observation,\n",
    "                                internal_state,\n",
    "                                single_action)\n",
    "        greedy_next_internal_state, greedy_sampled_actions = \\\n",
    "            policy_unified_pendulum.sample_step(single_observation,\n",
    "                               internal_state,\n",
    "                               single_action,\n",
    "                                greedy=True)\n",
    "            \n",
    "   \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()    \n",
    "train_writer = tf.summary.FileWriter(\"/home/adrian/Schreibtisch/Uni/Data-Innovation-Lab/tensorflowlogs\", sess.graph)\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original sample episodes code from the paper code 'pcl_rl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_internal_state():\n",
    "    return np.zeros(policy_unified_pendulum.rnn_state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "(initial_state_, observations_, actions_, rewards_, terminated_, pads_) = sample_episodes_pcl(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pads_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "bad axis2 argument to swapaxes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'swapaxes'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-c9d91cd58929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# sample from env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     (initial_state_,\n\u001b[0;32m---> 13\u001b[0;31m      observations_, actions_, rewards_, terminated_, pads_) = sample_episodes_pcl(sess)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-a7cad731cb83>\u001b[0m in \u001b[0;36msample_episodes_pcl\u001b[0;34m(sess)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \"\"\"\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_batched_episodes_pcl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-a1e73dd17163>\u001b[0m in \u001b[0;36mconvert_to_batched_episodes_pcl\u001b[0;34m(episodes_, max_length)\u001b[0m\n\u001b[1;32m     21\u001b[0m                     for obs in zip(*observations_)]\n\u001b[1;32m     22\u001b[0m     actions_ = [np.swapaxes(act, 0, 1)\n\u001b[0;32m---> 23\u001b[0;31m                for act in zip(*actions_)]\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mrewards_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mpads_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpads_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-a1e73dd17163>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m                     for obs in zip(*observations_)]\n\u001b[1;32m     22\u001b[0m     actions_ = [np.swapaxes(act, 0, 1)\n\u001b[0;32m---> 23\u001b[0;31m                for act in zip(*actions_)]\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mrewards_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mpads_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpads_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mswapaxes\u001b[0;34m(a, axis1, axis2)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'swapaxes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# a downstream library like 'pandas'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: bad axis2 argument to swapaxes"
     ]
    }
   ],
   "source": [
    "global_step = tf.train.get_or_create_global_step()\n",
    "cur_step = 0\n",
    "\n",
    "losses_train = []\n",
    "rewards_train = []\n",
    "all_ep_rewards = []\n",
    "for step in range(10000):\n",
    "    \n",
    "\n",
    "    \n",
    "    # sample from env\n",
    "    (initial_state_,\n",
    "     observations_, actions_, rewards_, terminated_, pads_) = sample_episodes_pcl(sess)\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(step)\n",
    "        \n",
    "        \n",
    "    # Add sampled episodes to replay batch\n",
    "    add_to_replay_buffer_pcl(replay_buffer_pendulum, initial_state_, observations_, actions_, rewards_, terminated_, pads_, step)\n",
    "    \n",
    "    loss_step, summary_step = 0, None\n",
    "    \n",
    "    if update_eps_lambda:\n",
    "        episode_rewards_ = np.array(episode_rewards)\n",
    "        episode_lengths_ = np.array(episode_lengths)\n",
    "        eps_lambda_ = find_best_eps_lambda_(episode_rewards_, episode_lengths_)\n",
    "        sess.run(objective_pendulum.assign_eps_lambda,\n",
    "                feed_dict={objective_pendulum.new_eps_lambda: eps_lambda_})\n",
    "        \n",
    "    if use_online_batch:\n",
    "        outputs = [raw_loss, gradient_ops, summary]\n",
    "        feed_dict = {internal_state: initial_state_,\n",
    "                 rewards: rewards_,\n",
    "                 terminated: terminated_,\n",
    "                 pads: pads_,\n",
    "                 avg_episode_reward: np.mean(episode_rewards)}\n",
    "        for action_place, action in zip(actions, actions_):\n",
    "            feed_dict[action_place] = action\n",
    "        for obs_place, obs in zip(observations, observations_):\n",
    "            feed_dict[obs_place] = obs\n",
    "\n",
    "        loss_step, _, summary_step = sess.run(outputs, feed_dict=feed_dict)\n",
    "\n",
    "    if use_offline_batch:\n",
    "        replay_batch_, replay_probs_ = get_from_replay_buffer_pcl(replay_buffer_pendulum, replay_batch_size)\n",
    "       \n",
    "        #print(len(replay_buffer_pendulum))\n",
    "        # Check if replay batch is not none\n",
    "        if replay_batch_:\n",
    "            \n",
    "            (initial_state_, observations_, actions_, rewards_, terminated_, pads_) = replay_batch_\n",
    "        \n",
    "            outputs = [raw_loss, gradient_ops, summary]\n",
    "            feed_dict = {internal_state: initial_state_,\n",
    "                     rewards: rewards_,\n",
    "                     terminated: terminated_,\n",
    "                     pads: pads_,\n",
    "                     avg_episode_reward: np.mean(episode_rewards)}\n",
    "            for action_place, action in zip(actions, actions_):\n",
    "                feed_dict[action_place] = action\n",
    "            for obs_place, obs in zip(observations, observations_):\n",
    "                feed_dict[obs_place] = obs\n",
    "\n",
    "            loss_step, _, summary_step = sess.run(outputs, feed_dict=feed_dict)        \n",
    "          \n",
    "            # Perform value function update based on sampled data\n",
    "            if value_opt_pendulum is not None:\n",
    "\n",
    "                feed_dict_opt = {internal_state: initial_state_,\n",
    "                                 rewards: rewards_,\n",
    "                                 terminated: terminated_,\n",
    "                                 pads: pads_}\n",
    "                \n",
    "                for action_place, action in zip(actions, actions_):\n",
    "                    feed_dict_opt[action_place] = action\n",
    "                for obs_place, obs in zip(observations, observations_):\n",
    "                    feed_dict_opt[obs_place] = obs\n",
    "            \n",
    "                value_opt_pendulum.optimize(sess, feed_dict_opt)\n",
    "        \n",
    "    losses_train.append(loss_step)\n",
    "    rewards_train.append(total_rewards)\n",
    "    all_ep_rewards.extend(episode_rewards)\n",
    "    \n",
    "    if summary_step is not None:\n",
    "        train_writer.add_summary(summary_step, step)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_eps_lambda_(rewards, lengths):\n",
    "    \"\"\"Find the best lambda given a desired epsilon = FLAGS.max_divergence.\"\"\"\n",
    "    # perhaps not the best way to do this\n",
    "    global max_divergence\n",
    "    \n",
    "    desired_div = max_divergence * np.mean(lengths)\n",
    "    \n",
    "    def calc_divergence(eps_lambda):\n",
    "        max_reward = np.max(rewards)\n",
    "        logz = (max_reward / eps_lambda +\n",
    "            np.log(np.mean(np.exp((rewards - max_reward) / eps_lambda))))\n",
    "        exprr = np.mean(np.exp(rewards / eps_lambda - logz) *\n",
    "                    rewards / eps_lambda)\n",
    "        return exprr - logz\n",
    "\n",
    "    left = 0.0\n",
    "    right = 1000.0\n",
    "\n",
    "    if len(rewards) <= 8:\n",
    "        return (left + right) / 2\n",
    "\n",
    "    num_iter = max(4, 1 + int(np.log((right - left) / 0.1) / np.log(2.0)))\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        mid = (left + right) / 2\n",
    "        cur_div = calc_divergence(mid)\n",
    "        if cur_div > desired_div:\n",
    "            left = mid\n",
    "        else:\n",
    "            right = mid\n",
    "        \n",
    "    \n",
    "    return (left + right) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 64)\n",
      "(1, 201, 1, 3)\n",
      "(1, 201, 1, 1)\n",
      "(200, 1)\n",
      "(1,)\n",
      "(200, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(initial_state_))\n",
    "print(np.shape(observations_))\n",
    "print(np.shape(actions_))\n",
    "print(np.shape(rewards_))\n",
    "print(np.shape(terminated_))\n",
    "print(np.shape(pads_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get value estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand tf.scan:\n",
    "* fn: The callable to be performed \n",
    "* initializer:  A tensor or (possibly nested) sequence of tensors, initial value for the accumulator, and the expected output type of fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The callable fn takes two tensors as arguments:\n",
    "* The first argument is the accumulated value computed from the preceding invocation of fn --> (next_state, tuple(actions), tuple(logits), tuple(log_probs))\n",
    "* The second argument is the one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episodes_pcl(sess):\n",
    "    \"\"\"Sample steps from the environment until we have enough for a batch.\"\"\"\n",
    "\n",
    "    # define global variables\n",
    "    global max_step, env_gym, start_id, total_rewards, episode_running_rewards, episode_running_lengths\n",
    "    global episode_running_rewards, episode_running_lengths, start_episode, cutoff_agent, step_count\n",
    "    global episode_rewards, episode_lengths\n",
    "    \n",
    "    # check if last batch ended with episode that was not terminated\n",
    "    #if unify_episodes:\n",
    "    #    all_new_ep = self.start_episode[0]\n",
    "\n",
    "    # sample episodes until we either have enough episodes or enough steps\n",
    "    episodes = []\n",
    "    total_steps = 0\n",
    "    while total_steps < max_step * len(env_gym):\n",
    "        (initial_state_,\n",
    "        observations, actions, rewards,\n",
    "        pads) = _sample_episodes_pcl(sess)\n",
    "\n",
    "        observations = zip(*observations)\n",
    "        actions = zip(*actions)\n",
    "\n",
    "        terminated = np.array(env_gym.dones)\n",
    "\n",
    "        total_rewards = np.sum(np.array(rewards[start_id_global:]) *\n",
    "                                  (1 - np.array(pads[start_id_global:])), axis=0)\n",
    "        episode_running_rewards *= 1 - start_episode\n",
    "        episode_running_lengths *= 1 - start_episode\n",
    "        episode_running_rewards += total_rewards\n",
    "        episode_running_lengths += np.sum(1 - np.array(pads[start_id_global:]), axis=0)\n",
    "\n",
    "        episodes.extend(convert_from_batched_episodes_pcl(\n",
    "          initial_state_, observations, actions, rewards,\n",
    "          terminated, pads))\n",
    "        total_steps += np.sum(1 - np.array(pads))\n",
    "\n",
    "        # set next starting episodes\n",
    "        start_episode = np.logical_or(terminated,\n",
    "                                         step_count >= cutoff_agent)\n",
    "        episode_rewards = episode_running_rewards[start_episode].tolist()\n",
    "        episode_rewards.extend(episode_rewards)\n",
    "        episode_lengths.extend(episode_running_lengths[start_episode].tolist())\n",
    "        # ToDo: Check why 100\n",
    "        episode_rewards = episode_rewards[-100:]\n",
    "        episode_lengths = episode_lengths[-100:]\n",
    "\n",
    "        \"\"\"\n",
    "        if (self.save_trajectories_file is not None and\n",
    "          (self.best_batch_rewards is None or\n",
    "           np.mean(self.total_rewards) > self.best_batch_rewards)):\n",
    "        self.best_batch_rewards = np.mean(self.total_rewards)\n",
    "        my_episodes = self.convert_from_batched_episodes(\n",
    "          initial_state, observations, actions, rewards,\n",
    "          terminated, pads)\n",
    "        with gfile.GFile(self.save_trajectories_file, 'w') as f:\n",
    "            pickle.dump(my_episodes, f)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if not self.batch_by_steps:\n",
    "        return (initial_state,\n",
    "                observations, actions, rewards,\n",
    "                terminated, pads)        \n",
    "        \"\"\"\n",
    "        \n",
    "    return convert_to_batched_episodes_pcl(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_episodes_pcl(sess, greedy=False):\n",
    "    \"\"\"Sample episodes from environment using model.\"\"\"\n",
    "    \n",
    "    # Define global variables\n",
    "    global start_episode, env_gym, step_count, internal_state, last_obs, last_act, last_pad, env_spec_gym, start_id\n",
    "    global internal_state_global, all_obs_global, all_act_global, all_pad_global, rewards_global, last_act\n",
    "    global last_obs, max_step\n",
    "    \n",
    "    # reset environments as necessary\n",
    "    obs_after_reset = env_gym.reset_if(start_episode)\n",
    "\n",
    "    for i, obs in enumerate(obs_after_reset):\n",
    "        if obs is not None:\n",
    "            step_count[i] = 0\n",
    "            internal_state_global[i] = initial_internal_state()\n",
    "            for j in range(len(env_spec_gym.obs_dims)):\n",
    "                last_obs[j][i] = obs[j]\n",
    "            for j in range(len(env_spec_gym.act_dims)):\n",
    "                last_act[j][i] = -1\n",
    "            last_pad[i] = 0\n",
    "\n",
    "    # maintain episode as a single unit if the last sampling\n",
    "    # batch ended before the episode was terminated\n",
    "    \"\"\"\n",
    "    if unify_episodes:\n",
    "        assert len(obs_after_reset) == 1\n",
    "        new_ep = obs_after_reset[0] is not None\n",
    "    else:\n",
    "        new_ep = True\n",
    "    \"\"\"\n",
    "    new_ep = True\n",
    "    \n",
    "    # ToDo: Make start_id global\n",
    "    start_id_global = 0 if new_ep else len(all_obs_global[:])\n",
    "\n",
    "    initial_state = internal_state_global\n",
    "    all_obs = [] if new_ep else all_obs_global[:]\n",
    "    all_act = ([last_act] if new_ep else all_act_global[:])\n",
    "    all_pad = [] if new_ep else all_pad_global[:]\n",
    "    rewards = [] if new_ep else rewards_global[:]\n",
    "\n",
    "    # start stepping in the environments\n",
    "    step = 0\n",
    "    while not env_gym.all_done():\n",
    "        step_count += 1 - np.array(env_gym.dones)\n",
    "\n",
    "        next_internal_state, sampled_actions = sample_step_pcl(\n",
    "          sess, last_obs, internal_state_global, last_act,\n",
    "          greedy=greedy)\n",
    "\n",
    "        env_actions = env_spec_gym.convert_actions_to_env(sampled_actions)\n",
    "        next_obs, reward, next_dones, _ = env_gym.step(env_actions)\n",
    "\n",
    "        all_obs.append(last_obs)\n",
    "        all_act.append(sampled_actions)\n",
    "        all_pad.append(last_pad)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        internal_state_global = next_internal_state\n",
    "        last_obs = next_obs\n",
    "        last_act = sampled_actions\n",
    "        last_pad = np.array(next_dones).astype('float32')\n",
    "\n",
    "        step += 1\n",
    "        if max_step and step >= max_step:\n",
    "            break\n",
    "\n",
    "    all_obs_global = all_obs[:]\n",
    "    all_act_global = all_act[:]\n",
    "    all_pad_global = all_pad[:]\n",
    "    rewards_global = rewards[:]\n",
    "\n",
    "    # append final observation\n",
    "    all_obs_global.append(last_obs)\n",
    "\n",
    "    return initial_state, all_obs, all_act, rewards, all_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_step_pcl(sess, single_observation_, internal_state_, single_action_, greedy=False):\n",
    "    \"\"\"Sample batch of steps from policy.\"\"\"\n",
    "    global greedy_next_internal_state, greedy_sampled_actions, next_internal_state, sampled_actions\n",
    "    global internal_state, single_action, single_observation\n",
    "    \n",
    "    if greedy:\n",
    "        outputs = [greedy_next_internal_state, greedy_sampled_actions]\n",
    "    else:\n",
    "        outputs = [next_internal_state, sampled_actions]\n",
    "\n",
    "    feed_dict = {internal_state: internal_state_}\n",
    "    for action_place, action in zip(single_action, single_action_):\n",
    "        feed_dict[action_place] = action\n",
    "    for obs_place, obs in zip(single_observation, single_observation_):\n",
    "        feed_dict[obs_place] = obs\n",
    "\n",
    "    return sess.run(outputs, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_batched_episodes_pcl(initial_state_, observations_, actions_, rewards_, terminated_, pads_):\n",
    "    \"\"\"Convert time-major batch of episodes to batch-major list of episodes.\"\"\"\n",
    "\n",
    "    rewards_ = np.array(rewards_)\n",
    "    pads_ = np.array(pads_)\n",
    "    observations_ = [np.array(obs) for obs in observations_]\n",
    "    actions_ = [np.array(act) for act in actions_]\n",
    "\n",
    "    total_rewards_ = np.sum(rewards_ * (1 - pads_), axis=0)\n",
    "    total_length_ = np.sum(1 - pads_, axis=0).astype('int32')\n",
    "\n",
    "    episodes_ = []\n",
    "    num_episodes_ = rewards_.shape[1]\n",
    "    for i in range(num_episodes_):\n",
    "        length = total_length_[i]\n",
    "        ep_initial = initial_state_[i]\n",
    "        ep_obs = [obs[:length, i, ...] for obs in observations_]\n",
    "        ep_act = [act[:length + 1, i, ...] for act in actions_]\n",
    "        ep_rewards = rewards_[:length, i]\n",
    "        \n",
    "        episodes_.append(\n",
    "          [ep_initial, ep_obs, ep_act, ep_rewards, terminated_[i]])\n",
    "    \n",
    "    return episodes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_batched_episodes_pcl(episodes_, max_length=None):\n",
    "    \"\"\"Convert batch-major list of episodes to time-major batch of episodes.\"\"\"\n",
    "    lengths = [len(ep[-2]) for ep in episodes_]\n",
    "    max_length = max_length or max(lengths)\n",
    "\n",
    "    new_episodes = []\n",
    "    for ep, length in zip(episodes_, lengths):\n",
    "        initial_, observations_, actions_, rewards_, terminated_ = ep\n",
    "        observations_ = [np.resize(obs, [max_length + 1] + list(obs.shape)[1:])\n",
    "                      for obs in observations_]\n",
    "        actions = [np.resize(act, [max_length + 1] + list(act.shape)[1:])\n",
    "                     for act in actions_]\n",
    "        pads_ = np.array([0] * length + [1] * (max_length - length))\n",
    "        rewards_ = np.resize(rewards_, [max_length]) * (1 - pads_)\n",
    "        new_episodes.append([initial_, observations_, actions_, rewards_,\n",
    "                           terminated_, pads_])\n",
    "\n",
    "    (initial_, observations_, actions_, rewards_,\n",
    "     terminated_, pads_) = zip(*new_episodes)\n",
    "    observations_ = [np.swapaxes(obs, 0, 1)\n",
    "                    for obs in zip(*observations_)]\n",
    "    actions_ = [np.swapaxes(act, 0, 1)\n",
    "               for act in zip(*actions_)]\n",
    "    rewards_ = np.transpose(rewards_)\n",
    "    pads_ = np.transpose(pads_)\n",
    "\n",
    "    return (initial_, observations_, actions_, rewards_, terminated_, pads_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_replay_buffer_pcl(replay_buffer_, initial_state,\n",
    "                        observations, actions, rewards,\n",
    "                        terminated, pads, step):\n",
    "    \n",
    "    global prioritize_by\n",
    "    \n",
    "    \"\"\"Add batch of episodes to replay buffer.\"\"\"\n",
    "    if replay_buffer_ is None:\n",
    "        return\n",
    "\n",
    "    rewards = np.array(rewards)\n",
    "    pads = np.array(pads)\n",
    "    total_rewards = np.sum(rewards * (1 - pads), axis=0)\n",
    "\n",
    "    episodes = convert_from_batched_episodes_pcl(\n",
    "      initial_state, observations, actions, rewards,\n",
    "      terminated, pads)\n",
    "\n",
    "    priorities = (total_rewards if prioritize_by == 'reward'\n",
    "                 else step)\n",
    "    \n",
    "    #if not self.unify_episodes or self.all_new_ep:\n",
    "    slast_idxs = replay_buffer_.add(episodes, priorities)\n",
    "    #else:\n",
    "      # If we are unifying episodes, we attempt to\n",
    "      # keep them unified in the replay buffer.\n",
    "      # The first episode sampled in the current batch is a\n",
    "      # continuation of the last episode from the previous batch\n",
    "    #self.replay_buffer.add(episodes[:1], priorities, self.last_idxs[-1:])\n",
    "    #if len(episodes) > 1:\n",
    "     #   self.replay_buffer.add(episodes[1:], priorities)\n",
    "\n",
    "def get_from_replay_buffer_pcl(replay_buffer_, batch_size):\n",
    "    \"\"\"Sample a batch of episodes from the replay buffer.\"\"\"\n",
    "    if replay_buffer_ is None or len(replay_buffer_) < 1 * batch_size:\n",
    "        return None, None\n",
    "\n",
    "    desired_count = batch_size * max_step\n",
    "    # in the case of batch_by_steps, we sample larger and larger\n",
    "    # amounts from the replay buffer until we have enough steps.\n",
    "    while True:\n",
    "        if batch_size > len(replay_buffer_):\n",
    "            batch_size = len(replay_buffer_)\n",
    "        episodes, probs = replay_buffer_.get_batch(batch_size)\n",
    "        count = sum(len(ep[-2]) for ep in episodes)\n",
    "        if count >= desired_count or not batch_by_steps:\n",
    "            break\n",
    "        if batch_size == len(replay_buffer_):\n",
    "            return None, None\n",
    "        batch_size *= 1.2\n",
    "\n",
    "    return (convert_to_batched_episodes_pcl(episodes), probs)\n",
    "\n",
    "def seed_replay_buffer_pcl(replay_buffer_, episodes):\n",
    "    \"\"\"Seed the replay buffer with some episodes.\"\"\"\n",
    "    if replay_buffer_ is None:\n",
    "        return\n",
    "\n",
    "    # just need to add initial state\n",
    "    for i in range(len(episodes)):\n",
    "        episodes[i] = [initial_internal_state()] + episodes[i]\n",
    "\n",
    "    replay_buffer_.seed_buffer(episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
