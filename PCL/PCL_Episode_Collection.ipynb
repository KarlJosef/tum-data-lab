{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import threading\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From scratch - PCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME = 'Pendulum-v0'\n",
    "OUTPUT_GRAPH = True\n",
    "LOG_DIR = '/home/adrian/Schreibtisch/Uni/Data-Innovation-Lab/tensorflowlogs'\n",
    "N_WORKERS = multiprocessing.cpu_count()\n",
    "MAX_EP_STEP = 200\n",
    "MAX_GLOBAL_EP = 500\n",
    "GLOBAL_NET_SCOPE = 'Global_Net'\n",
    "UPDATE_GLOBAL_ITER = 10\n",
    "GAMMA = 0.9\n",
    "ENTROPY_BETA = 0.01\n",
    "LR_A = 0.0001    # learning rate for actor\n",
    "LR_C = 0.001    # learning rate for critic\n",
    "GLOBAL_RUNNING_R = []\n",
    "GLOBAL_EP = 0\n",
    "\n",
    "# PCL specific\n",
    "TAU = 0.1\n",
    "ROLLOUT = 10\n",
    "\n",
    "env = gym.make(GAME)\n",
    "\n",
    "N_S = env.observation_space.shape[0]\n",
    "N_A = env.action_space.shape[0]\n",
    "A_BOUND = [env.action_space.low, env.action_space.high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spaces(object):\n",
    "    discrete = 0\n",
    "    box = 1\n",
    "\n",
    "def get_space(space):\n",
    "    if hasattr(space, 'n'):\n",
    "        return space.n, spaces.discrete, None\n",
    "    elif hasattr(space, 'shape'):\n",
    "        return np.prod(space.shape), spaces.box, (space.low, space.high)\n",
    "\n",
    "def get_spaces(spaces):\n",
    "    if hasattr(spaces, 'spaces'):\n",
    "        return zip(*[get_space(space) for space in spaces.spaces])\n",
    "    else:\n",
    "        return [(ret,) for ret in get_space(spaces)]\n",
    "\n",
    "def sampling_dim(dim, typ):\n",
    "    if typ == spaces.discrete:\n",
    "        return dim\n",
    "    elif typ == spaces.box:\n",
    "        return 2 * dim  # Gaussian mean and std\n",
    "    else:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(1,)\n",
      "((array([-1., -1., -8.]), array([ 1.,  1.,  8.])),)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "obs_space = env.observation_space\n",
    "obs_dims, obs_types, obs_info = get_spaces(obs_space)\n",
    "act_space = env.action_space\n",
    "act_dims, act_types, act_info = get_spaces(act_space)\n",
    "total_obs_dim = sum(obs_dims)\n",
    "total_sampled_act_dim = sum(act_dims)\n",
    "print(obs_dims)\n",
    "print(obs_types)\n",
    "print(obs_info)\n",
    "print(N_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1]"
      ]
     },
     "execution_count": 886,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[total_obs_dim, total_sampled_act_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dims_and_types = zip(obs_dims, obs_types)\n",
    "act_dims_and_types = zip(act_dims, act_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sampling_act_dim = sum(sampling_dim(dim, typ) for dim, typ in act_dims_and_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,), (1,), ((array([-2.]), array([ 2.])),)]"
      ]
     },
     "execution_count": 882,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_spaces(act_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchy of calls to collect data\n",
    "* samlple_episodes\n",
    "* sample_single_episodes\n",
    "* sample_step\n",
    "\n",
    "Output of sample_episodes is a batch of batchsize_episodes many episodes\n",
    "* This batch is then feed to update model\n",
    "\n",
    "Update model itself is divided into two parts\n",
    "* update_on_policy --> Using current batch\n",
    "* upadte_off_policy --> Using batch drawn from a replay buffer\n",
    "\n",
    "Upadte_on_policy and update_off_policy are taking this batch off episodes and call\n",
    "* train_step\n",
    "\n",
    "Train step is receiving data of the form\n",
    "* observations (steps, batch_size, obs_dim)\n",
    "* internal_state ()\n",
    "* actions (steps, batch_sitze, act_dim)\n",
    "* rewards (steps, batch_size)\n",
    "* terminated (batch_size)\n",
    "* pads (steps, batch_size)\n",
    "\n",
    "Output of this call is\n",
    "* raw_loss\n",
    "* gradient_ops\n",
    "* summary\n",
    "\n",
    "All of this outputs are returned evaluating the objective objective.get()\n",
    "* gradient_ops = self.training_ops(loss, learning_rate=self.learning_rate)\n",
    "* loss = (self.policy_weight * policy_loss + self.critic_weight * critic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define Batches in Terms of Batch of episodes not in terms of single steps  \n",
    "* First dimension --> Length of episode --> Steps performed  \n",
    "* Second dimension --> Batch_Size (Count of episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_state = tf.placeholder(tf.float32, [None, N_S], 'S')\n",
    "single_action = tf.placeholder(tf.float32, [None, N_A], 'A')\n",
    "internal_state_tf = tf.placeholder(tf.float32, [None, 10],'internal_state')\n",
    "\n",
    "# Those are needed to perform update of neural network\n",
    "states_episodic = tf.placeholder(tf.float32, [None, None, N_S], 'all_obs_1')\n",
    "actions_episodic = tf.placeholder(tf.float32, [None, None, N_A], 'all_act_1')\n",
    "\n",
    "rewards_episodic = tf.placeholder(tf.float32, [None, None], 'rewards')\n",
    "terminated_episodic = tf.placeholder(tf.float32, [None], 'terminated')\n",
    "pads_episodic = tf.placeholder(tf.float32, [None, None], 'pads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with setting up data pipeline to sample episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Batchsize (Epsidodes) and steps to perfom in every episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps_in_env = 150\n",
    "episode_length = 10\n",
    "\n",
    "global INTERNAL_STATE, LAST_OBS, LAST_ACT, LAST_PAD\n",
    "INTERNAL_STATE = list(env.observation_space.sample())\n",
    "LAST_OBS =  list(env.observation_space.sample())\n",
    "LAST_ACT = [env.action_space.sample().tolist(),  env.action_space.sample().tolist()]\n",
    "LAST_PAD = [0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. sample_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an observation get new action\n",
    "def sample_step(single_observation):\n",
    "    # Normally call somethin like this \n",
    "    \"\"\"\n",
    "    s = s[np.newaxis, :]\n",
    "    return SESS.run(self.A, {self.s: s})[0]\n",
    "    \"\"\"\n",
    "    # return random action.\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calls model.sample_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_step(self, sess,\n",
    "              single_observation, internal_state, single_action,\n",
    "              greedy=False):\n",
    "    \"\"\"Sample batch of steps from policy.\"\"\"\n",
    "    if greedy:\n",
    "        outputs = [self.greedy_next_internal_state, self.greedy_sampled_actions]\n",
    "    else:\n",
    "        outputs = [self.next_internal_state, self.sampled_actions]\n",
    "\n",
    "    feed_dict = {self.internal_state: internal_state}\n",
    "    for action_place, action in zip(self.single_action, single_action):\n",
    "        feed_dict[action_place] = action\n",
    "    for obs_place, obs in zip(self.single_observation, single_observation):\n",
    "        feed_dict[obs_place] = obs\n",
    "\n",
    "    return sess.run(outputs, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. sample_single_episode --> requires global variables internal_state, last_act\n",
    "* internal_state allows to not reset the environment after one episode got collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_single_episode(episode_length):\n",
    "    \n",
    "    global INTERNAL_STATE, LAST_OBS, LAST_ACT, LAST_PAD\n",
    "    env.reset()\n",
    "    \n",
    "    # ToDo: Figure out how initial state is gernated by the recurrent neural network\n",
    "    \"\"\"\n",
    "    next_internal_state, sampled_actions = self.model.sample_step(\n",
    "          sess, self.last_obs, self.internal_state, self.last_act,\n",
    "          greedy=greedy)\n",
    "    \"\"\"\n",
    "    #initial_state = INTERNAL_STATE\n",
    "    all_obs = [] \n",
    "    all_act = [LAST_ACT]\n",
    "    all_pad = []\n",
    "    rewards = [] \n",
    "    done = [False, False]\n",
    "    \n",
    "    step = 0\n",
    "    while not done[0] and not done[1]:\n",
    "        \n",
    "        sampled_action = sample_step(LAST_OBS)\n",
    "        # Convert action to gym --> (see env_spec.convert_action_to_gym)\n",
    "        #env_actions = sampled_action[0]\n",
    "        \n",
    "        next_obs, reward, next_dones, tt1 = env.step(sampled_action)\n",
    "        next_obs = next_obs.tolist()\n",
    "        next_obs1, reward1, next_dones1, tt2 = env.step(sampled_action)\n",
    "        next_obs1 = next_obs1.tolist()\n",
    "        \n",
    "        outputs = [[next_obs, reward, next_dones, tt1], [next_obs1, reward1, next_dones1, tt2]]\n",
    "        # Convert observations to list\n",
    "        \n",
    "        \n",
    "        next_obs, reward, next_dones, tt = zip(*outputs)     \n",
    "        done = next_dones\n",
    "        \n",
    "        all_obs.append(LAST_OBS)\n",
    "        all_act.append([sampled_action.tolist(), sampled_action.tolist()])\n",
    "        all_pad.append(LAST_PAD)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        LAST_OBS = next_obs\n",
    "        LAST_ACT = [sampled_action.tolist(), sampled_action.tolist()]\n",
    "        LAST_PAD = np.array(next_dones).astype('float32')\n",
    "        \n",
    "        step +=1\n",
    "        # Required for unsolved environments like Pendulum\n",
    "        # Done escape will trigger if the environment needs to reset at some time e.g. Cart-Pole\n",
    "        if episode_length <= step:\n",
    "            break\n",
    "            \n",
    "    # append final observation\n",
    "    all_obs.append(LAST_OBS)\n",
    "    \n",
    "    return  all_obs, all_act, rewards, all_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17,)\n",
      "(17, 2, 1)\n",
      "(16, 2)\n",
      "(16, 2)\n"
     ]
    }
   ],
   "source": [
    "observations, actions, rewards, pads = sample_single_episode(16)\n",
    "terminated = np.array([False, False])\n",
    "print(np.shape(observations))\n",
    "print(np.shape(actions))\n",
    "print(np.shape(rewards))\n",
    "print(np.shape(pads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. sample_episodes\n",
    "* Ouput is a list of dimension (episodes, 4)\n",
    "* With 4 = observations, actions, rewards, terminated indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episodes(max_steps_in_env, episode_length):\n",
    "    \n",
    "    episodes = []\n",
    "    total_steps = 0\n",
    "    \n",
    "    while total_steps < max_steps_in_env * 1:\n",
    "    \n",
    "        observations, actions, rewards, pads = sample_single_episode(episode_length)\n",
    "        terminated = np.array([False, False])\n",
    "        \n",
    "        episodes.extend(convert_from_batched_episodes(observations, actions, rewards, terminated, pads))\n",
    "        \n",
    "        total_steps += np.sum(1 - np.array(pads))\n",
    "        \n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = sample_episodes(max_steps_in_env, episode_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[array([ 0.20545786, -0.97866596,  3.64683662]),\n",
       "  array([ 0.46616799, -0.88469622, -0.73742387]),\n",
       "  array([ 0.28114144, -0.95966634, -2.30974903]),\n",
       "  array([-0.04467913, -0.99900139, -3.63462885]),\n",
       "  array([-0.48737655, -0.8731919 , -4.97115319]),\n",
       "  array([-0.88093592, -0.47323557, -5.81918304]),\n",
       "  array([-0.99503088,  0.09956684, -5.94598188]),\n",
       "  array([-0.78630792,  0.61783481, -5.56286559]),\n",
       "  array([-0.4027277 ,  0.91531983, -4.62252086]),\n",
       "  array([-0.05963747,  0.9982201 , -3.12262975])],\n",
       " [array([ 0.2737358]),\n",
       "  array([-1.9248408]),\n",
       "  array([ 0.47054199]),\n",
       "  array([ 0.44838289]),\n",
       "  array([ 0.46773599]),\n",
       "  array([ 1.77499231]),\n",
       "  array([ 0.7272812]),\n",
       "  array([-0.5619684]),\n",
       "  array([-0.25187218]),\n",
       "  array([ 0.79052478]),\n",
       "  array([-1.75909811])],\n",
       " array([ -1.1081043 ,  -1.65538253,  -2.93138487,  -5.21727999,\n",
       "         -8.64856242, -12.15809796, -10.90376566,  -7.59973161,\n",
       "         -4.77168498,  -2.815184  ]),\n",
       " False]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.shape(episodes))\n",
    "episodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 16, 3)\n",
      "(11, 16, 1)\n",
      "(10, 16)\n",
      "(16,)\n",
      "(10, 16)\n"
     ]
    }
   ],
   "source": [
    "observations, actions, rewards, terminated, pads = convert_to_batched_episodes(episodes)\n",
    "print(np.shape(observations))\n",
    "print(np.shape(actions))\n",
    "print(np.shape(rewards))\n",
    "print(np.shape(terminated))\n",
    "print(np.shape(pads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is than called with the output of the convert_to_batched_episodes function  \n",
    "* train(sess, observations, initial_state, actions, rewards, terminated, pads)\n",
    "* This functions than calls model.train_step (ess, observations, initial_state, actions, rewards, terminated, pads, avg_episode_reward=np.mean(self.episode_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {self.internal_state: internal_state,\n",
    "             self.rewards: rewards,\n",
    "             self.terminated: terminated,\n",
    "             self.pads: pads,\n",
    "             self.avg_episode_reward: avg_episode_reward,\n",
    "             self.actions: actions\n",
    "             self.observations: observations}\n",
    "\n",
    "outputs = [self.raw_loss, self.gradient_ops, self.summary]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Tensorflow-objects get created by the call of objective.get which does compute the loss function. This fucntion takes the following elements:\n",
    "* rewards\n",
    "* pads\n",
    "* values\n",
    "* final_values\n",
    "* log_probs\n",
    "* prev_log_probs --> Used by TRPO\n",
    "* target_log_probs --> Only used by Trust-PCL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is calculated and then passed on to the training_ops\n",
    "* gradient_ops = self.training_ops(loss, learning_rate=self.learning_rate)\n",
    "* Gradient ops return the apply_gradient obs and when called updates the model\n",
    "* Needs:\n",
    "* params = tf.trainable_variables()\n",
    "* grads = tf.gradients(loss, params)\n",
    "* loss --> list of tensors to be differentiated\n",
    "* params --> list of tensors to be used for differentiation\n",
    "* Brings objective and model specification together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    " def training_ops(self, loss, learning_rate=None):\n",
    "    \"\"\"Gradient ops.\"\"\"\n",
    "    opt = self.get_optimizer(learning_rate) #--> tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=2e-4)\n",
    "    params = tf.trainable_variables()\n",
    "    grads = tf.gradients(loss, params)\n",
    "\n",
    "    if self.clip_norm:\n",
    "        grads, global_norm = tf.clip_by_global_norm(grads, self.clip_norm)\n",
    "        tf.summary.scalar('grad_global_norm', global_norm)\n",
    "\n",
    "    return opt.apply_gradients(zip(grads, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we used the following as our input to train_step:\n",
    "* rewards\n",
    "* pads \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate where the following come from:\n",
    "* values --> Input observations, actions, \n",
    "* log_probs --> Input observations, internal_state, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get log_probs\n",
    "(self.policy_internal_states, self.logits, self.log_probs, self.entropies, self.self_kls) = \\\n",
    "                    self.policy.multi_step(self.observations,\n",
    "                                           self.internal_state,\n",
    "                                           self.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get values\n",
    "(self.values, self.regression_input, self.regression_weight) = \\\n",
    "                    self.baseline.get_values(\n",
    "                            self.observations,\n",
    "                            self.actions,\n",
    "                            self.policy_internal_states, \n",
    "                            self.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help function to bundle observations, actions, rewards and pads into one object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_batched_episodes(observations, actions, rewards, terminated, pads):\n",
    "    \"\"\"Convert time-major batch of episodes to batch-major list of episodes.\"\"\"\n",
    "\n",
    "    rewards = np.array(rewards)\n",
    "    if len(np.shape(rewards)) == 1:\n",
    "        rewards = np.reshape(rewards, [np.shape(rewards)[0], 1])\n",
    "        \n",
    "    pads = np.array(pads)\n",
    "    if len(np.shape(pads)) == 1:\n",
    "        rewards = np.reshape(pads, [np.shape(pads)[0], 1])\n",
    "        \n",
    "    observations = [np.array(obs) for obs in observations]\n",
    "    actions = [np.array(act) for act in actions]\n",
    "\n",
    "    \n",
    "    total_rewards = np.sum(rewards * (1 - pads), axis=0)\n",
    "    total_length = np.sum(1 - pads, axis=0).astype('int32')\n",
    "    \n",
    "    if isinstance(total_length, np.integer):\n",
    "        total_length =  [total_length]\n",
    "        \n",
    "    episodes = []\n",
    "    \n",
    "    if len(np.shape(observations)) == 2:\n",
    "        length = total_length[0]\n",
    "        ep_obs = observations[:length]\n",
    "        ep_act = actions[:length+1]\n",
    "        ep_rewards = rewards[:length, 0]\n",
    "        episodes.append([ep_obs, ep_act, ep_rewards, terminated])\n",
    "        return episodes\n",
    "    \n",
    "    num_episodes = rewards.shape[1]\n",
    "    for i in range(num_episodes):\n",
    "        length = total_length[i]\n",
    "        ep_obs = [obs[i] for obs in observations][:length]\n",
    "        ep_act = [act[i] for act in actions][:length+1]\n",
    "        ep_rewards = rewards[:length, i]\n",
    "\n",
    "        episodes.append(\n",
    "          [ep_obs, ep_act, ep_rewards, terminated[i]])\n",
    "\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help function to convert batach-major list of episodes to time-major batch of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_batched_episodes(episodes, max_length=None):\n",
    "    \"\"\"Convert batch-major list of episodes to time-major batch of episodes.\"\"\"\n",
    "    lengths = [len(ep[-2]) for ep in episodes]\n",
    "    max_length = max_length or max(lengths)\n",
    "\n",
    "    new_episodes = []\n",
    "    for ep, length in zip(episodes, lengths):\n",
    "        observations, actions, rewards, terminated = ep\n",
    "        observations = np.vstack(observations)\n",
    "        actions = np.vstack(actions)\n",
    "        pads = np.array([0] * length + [1] * (max_length - length))\n",
    "        rewards = np.resize(rewards, [max_length]) * (1 - pads)\n",
    "        new_episodes.append([observations, actions, rewards, terminated, pads])\n",
    "\n",
    "    (observations, actions, rewards, terminated, pads) = zip(*new_episodes)\n",
    "    observations = np.swapaxes(observations, 0, 1)\n",
    "    actions = np.swapaxes(actions, 0, 1)\n",
    "    rewards = np.transpose(rewards)\n",
    "    pads = np.transpose(pads)\n",
    "\n",
    "    return (observations, actions, rewards, terminated, pads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow code to evaluate rolling discount on rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_1 = tf.concat([tf.zeros([5 - 1, 16]), rewards_episodic], 0)\n",
    "\n",
    "discount_filter = tf.reshape(0.9 ** tf.range(float(5)), [-1, 1, 1])\n",
    "expanded_values = tf.concat([rewards_1, tf.zeros([5 - 1, tf.shape(rewards_1)[1]])], 0)\n",
    "expanded_dims = tf.expand_dims(tf.transpose(expanded_values), -1)\n",
    "conv1 = tf.nn.conv1d(expanded_dims, discount_filter, stride=1, padding='VALID')\n",
    "conv1_sq = tf.squeeze(conv1, -1)\n",
    "conv_values = tf.transpose(conv1_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "SESS = tf.Session()\n",
    "feed_dict = {rewards_episodic: rewards}\n",
    "conv1_sq, expanded_dims, conv1, filter_, catched_discounted_sum, expanded_ = SESS.run([conv1_sq, expanded_dims, conv1, discount_filter, conv_values, expanded_values], feed_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
