{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *** Implement Unified PCL ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we need to define all placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One episode of observations (Time_Steps, Observation dimension)\n",
    "self.single_observation = tf.placeholder(tf.float32, [None, obs_dim], 'obs%d' % i)\n",
    "\n",
    "# One episode of actions (Time_steps, action dimension)\n",
    "self.single_action = tf.placeholder(tf.float32, [None, action_dim], 'act%d' % i)\n",
    "\n",
    "# Observations batch size many episodes of time length [batch size, time length, observation dim]\n",
    "self.observations = tf.placeholder(tf.float32, [None, None, obs_dim], 'all_obs%d' % i)\n",
    "\n",
    "# Actions batch size many episodes of time length [batch size, time length, action dim]\n",
    "self.actions = tf.placeholder(tf.float32, [None, None, action_dim], 'all_act%d' % i)\n",
    "\n",
    "# Rewards of Batch Size many episodes of time length [batch size, time length]\n",
    "self.rewards = tf.placeholder(tf.float32, [None, None], 'rewards')\n",
    "\n",
    "# Indicator if episode has terminated \n",
    "self.terminated = tf.placeholder(tf.float32, [None], 'terminated')\n",
    "\n",
    "# Batch Size many episodes of time length indicators if episode has ended\n",
    "self.pads = tf.placeholder(tf.float32, [None, None], 'pads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define computation graph of policy evaluation\n",
    "* Internal States are related to the states of the RNN-Network\n",
    "* Logits are the output of the neural network (mu, sigma)\n",
    "* Log Probs are log probabilities of the policy at state \"obs\" --> log(N(mu,sigma))\n",
    "* Entropy is only used by Actor-Critic Objective as reguralizer in the objective\n",
    "* KL-Divergence is only used by TRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy network\n",
    "with tf.variable_scope('policy_net'):\n",
    "    (self.policy_internal_states, self.logits, self.log_probs, self.entropies, self.self_kls) = \\\n",
    "                self.policy.multi_step(self.observations,\n",
    "                                       self.internal_state,\n",
    "                                       self.actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To receive the required value fucntions we need to define a graph for them:\n",
    "* 1 Option - seperate neural network\n",
    "* 2 Option - Calculated by the same network as the actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unified PCL:\n",
    "* regression_input and regression_weight are not used since they are required if we have a seperate NN for the value function\n",
    "* policy_internal_states are also only used to get the input into the value function if the value function is recurrent and we consider seperate models\n",
    "* The values are only calculated based on the logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate value function\n",
    "* regression_input and regression_weigh are used to update the model\n",
    "* The policy_internal_states is considered as input if we have a recurrent model\n",
    "* The actions are also considered as input if required\n",
    "* We can also conside the time step as input into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value network\n",
    "with tf.variable_scope('value_net'):\n",
    "    (self.values,\n",
    "     self.regression_input,\n",
    "     self.regression_weight) = self.baseline.get_values(\n",
    "        self.observations, \n",
    "        self.actions,\n",
    "        self.policy_internal_states, \n",
    "        self.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the objective based on the computed:\n",
    "* rewards of the rollouts of the current policy\n",
    "* value function evaluations of the current value function\n",
    "* pads and terminated indicator\n",
    "* log probs of the policy at state and action t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following input is required if we consider trust pcl:\n",
    "* target Log Probs --> are the log probs sampled by the target network $ \\tilde{\\theta} \\leftarrow \\alpha \\tilde{\\theta} + (1 - \\alpha) \\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs never used:\n",
    "* logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs used by other algorithms:\n",
    " * entropy --> reguralizer A3C\n",
    " * prev_log_probs --> TRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate objective\n",
    "(self.loss, self.raw_loss, self.regression_target, self.gradient_ops, self.summary) = self.objective.get(\n",
    "                      self.rewards, self.pads,\n",
    "                      self.values[:-1, :],\n",
    "                      self.values[-1, :] * (1 - self.terminated),\n",
    "                      self.log_probs, self.prev_log_probs, self.target_log_probs,\n",
    "                      self.entropies,\n",
    "                      self.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the sampling operations\n",
    "* We can sample from the current policy \"poliyc_net\" or from our target policy \"target_policy_net\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy_sample step function calls the single_step function and returns the current next state of the RNN and the sampled\n",
    "actions. There are to options to sample the actions:\n",
    "* Greedy --> take the mean value as the action to take\n",
    "* More exploration --> take the mean disturbed by the standard deviation as next action (means + std * tf.random_normal([batch_size, act_dim]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we re-use variables for the sampling operations\n",
    "with tf.variable_scope('model', reuse=True):\n",
    "    scope = ('target_policy_net' if self.sample_from == 'target'\n",
    "               else 'policy_net')\n",
    "    with tf.variable_scope(scope):\n",
    "        self.next_internal_state, self.sampled_actions = \\\n",
    "            self.policy.sample_step(self.single_observation,\n",
    "                                self.internal_state,\n",
    "                                self.single_action)\n",
    "        self.greedy_next_internal_state, self.greedy_sampled_actions = \\\n",
    "            self.policy.sample_step(self.single_observation,\n",
    "                                self.internal_state,\n",
    "                                self.single_action,\n",
    "                                greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
