{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import threading\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy neural network  \n",
    "\n",
    "Implements network which takes in input and produces actions  \n",
    "and log probabilities given a sampling distribution parameterization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME = 'Pendulum-v0'\n",
    "env = gym.make(GAME)\n",
    "\n",
    "class spaces(object):\n",
    "    discrete = 0\n",
    "    box = 1\n",
    "\n",
    "def get_space(space):\n",
    "    if hasattr(space, 'n'):\n",
    "        return space.n, spaces.discrete, None\n",
    "    elif hasattr(space, 'shape'):\n",
    "        return np.prod(space.shape), spaces.box, (space.low, space.high)\n",
    "\n",
    "def get_spaces(spaces):\n",
    "    if hasattr(spaces, 'spaces'):\n",
    "        return zip(*[get_space(space) for space in spaces.spaces])\n",
    "    else:\n",
    "        return [(ret,) for ret in get_space(spaces)]\n",
    "\n",
    "def sampling_dim(dim, typ):\n",
    "    if typ == spaces.discrete:\n",
    "        return dim\n",
    "    elif typ == spaces.box:\n",
    "        return 2 * dim  # Gaussian mean and std\n",
    "    else:\n",
    "        assert False\n",
    "        \n",
    "obs_space = env.observation_space\n",
    "obs_dims, obs_types, obs_info = get_spaces(obs_space)\n",
    "act_space = env.action_space\n",
    "act_dims, act_types, act_info = get_spaces(act_space)\n",
    "total_obs_dim = sum(obs_dims)\n",
    "total_sampled_act_dim = sum(act_dims)\n",
    "\n",
    "obs_dims_and_types = zip(obs_dims, obs_types)\n",
    "act_dims_and_types = zip(act_dims, act_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate where log_probs com from and network evaluation is performed.  \n",
    "Starting point in model tensorflow setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get log_probs\n",
    "(self.policy_internal_states, self.logits, self.log_probs, self.entropies, self.self_kls) = \\\n",
    "                    self.policy.multi_step(self.observations,\n",
    "                                           self.internal_state,\n",
    "                                           self.actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two different kinds of baseline are availabe:\n",
    "* Recurrent policy\n",
    "* MLPPolicy --> Non-recurrent policy --> inherits from Recurrent Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input of neural network is dependend of if prev. action is also considered as input:\n",
    "* If not only dim_obs\n",
    "* If yes dim_obs + dim_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_dim(self):\n",
    "    return (self.env_spec.total_obs_dim + \n",
    "            self.env_spec.total_sampled_act_dim * self.input_prev_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output dimension is in the continouse case 2 * act_dim to model Gaussian means and sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_dim(self):\n",
    "    return self.env_spec.total_sampling_act_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fucntion core(obs, prev_internal_sate, prev_actions) implements the core neural network:\n",
    "* taking in inputs \n",
    "* outputting sampling distribution parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first get Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = tf.shape(obs[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next check if recurrent network is applied - If not initialize prev_internal_state with:\n",
    "* [batch_size, rnn_state_dim] of zeros\n",
    "* Otherwise for every Episode in the Batch one rnn_state_dim is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not self.recurrent:\n",
    "      prev_internal_state = tf.zeros([batch_size, self.rnn_state_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next an RNN-Cell is created via \"get_cell()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = self.get_cell()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells input dim is half the dimension of the internal (rnn) dim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.cell_input_dim = self.internal_dim // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards an tf.contrib.rnn.LSTMCell is created\n",
    "* state_ist_tuple: If True, accepted and returned states are 2-tuples of the c_state and m_state\n",
    "* (optional) Python boolean describing whether to reuse variables in an existing scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.LSTMCell(self.cell_input_dim,\n",
    "                               state_is_tuple=False,\n",
    "                                reuse=tf.get_variable_scope().reuse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rnn models return:\n",
    "* outputs [batch_size, max_time, output_size] --> actually desired output\n",
    "* The final state [batch_size] + cell.state_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM Cell is wrapped into an \"OutputProjectionWrapper\"\n",
    "* Maps from cell_input_dim to output dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "            cell,\n",
    "            self.output_dim,\n",
    "            reuse=tf.get_variable_scope().reuse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the One-Layer Lstm Cell with Output Layer is created and we are back at the \"core\" function\n",
    "* go on now with the weight creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.get_variable('input_bias', [self.cell_input_dim], initializer=self.vector_init)\n",
    "cell_input = tf.nn.bias_add(tf.zeros([batch_size, self.cell_input_dim]), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate now through all observation kinds and get type (discrete or box) and dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.get_variable('w_state%d' % i, [obs_dim, self.cell_input_dim], initializer=self.matrix_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of box - like pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " cell_input += tf.matmul(obs[i], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Fall, dass die vorhergehende Action ebenfalls inkludiert werden soll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.get_variable('w_prev_action%d' % i, [act_dim, self.cell_input_dim], initializer=self.matrix_init)\n",
    "cell_input += tf.matmul(prev_actions[i], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend wird der Cell Input in die LSTM Zelle gefuettert sowie der vorhergehende internal state, sollte die Episode noch nicht abgeschlossen wurden sein:\n",
    "* Output isr von der Form [batch_size, rnn_state_dim] also für jede episode ein interne dimension\n",
    "* Output [batch_size, max_len, total_sampling_action_dim] --> Im Fall vom Pendulum (mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, next_state = cell(cell_input, prev_internal_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this functionallity availabe one can compute actions and corresponding log_probs with the following Hierarchy:\n",
    "* multi_step --> Calculate log-probs and other calculations on batch of episodes\n",
    "* single_step --> Single RNN step.  Equivalently, single-time-step sampled actions.\n",
    "* sample_actions --> Sample all actions given output of core network\n",
    "* sample_action --> Sample an action from a distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with sample_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_logits = output[:, start_idx:start_idx + act_dim] # Get Mu, Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = self.sample_action(act_logits,\n",
    "                         sampling_dim,\n",
    "                         act_dim,\n",
    "                         act_type,\n",
    "                         greedy=greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = logits[:, :sampling_dim / 2]\n",
    "std = logits[:, sampling_dim / 2:]\n",
    "# If greedy \n",
    "act = means\n",
    "# Else\n",
    "act = means + std * tf.random_normal([batch_size, act_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finallys \"log_prob_action\" based on the output of the NN, the choosen  act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_log_prob = self.log_prob_action(act, act_logits, sampling_dim, act_dim, act_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = logits[:, :sampling_dim / 2]\n",
    "std = logits[:, sampling_dim / 2:]\n",
    "# Log normal distribution\n",
    "act_log_prob = (- 0.5 * tf.log(2 * np.pi * tf.square(std))- 0.5 * tf.square(action - means) / tf.square(std))\n",
    "# If action is means\n",
    "# act_log_prob = - 0.5 * tf.log(2 * np.pi * tf.square(std))\n",
    "act_log_prob = tf.reduce_sum(act_log_prob, -1) --> In case multidimensional action got applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampled action, the log probs, the logits (Output of the neural net) as well as entropy and self_kl (KL-Divergence) are returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those outputs and the next internal state of the RNN-Network are collected by the \"single_step\" function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi_step function \"Calculate log-probs and other calculations on batch of episodes.\" and calls \"single_step\" for all observation and action pairs via tf.scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(internal_states, _, logits, log_probs, entropies, self_kls) = tf.scan(\n",
    "                            self.single_step,\n",
    "                            (all_obs, all_actions),\n",
    "                            initializer=self.get_initializer(\n",
    "                            batch_size, initial_state, initial_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Therefore log_probs are calculated based on the output of the NN (logits)\n",
    "* If one considers the unified version of the PCL the  values are also calculated by the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_transform(q_values, tau):\n",
    "    max_q = tf.reduce_max(q_values, -1, keep_dims=True)\n",
    "    return tf.squeeze(max_q, [-1]) + tau * tf.log(\n",
    "                    tf.reduce_sum(tf.exp((q_values - max_q) / tau), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via the following call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = f_transform((self.tau + self.eps_lambda) * reshaped_policy_logits[0], (self.tau + self.eps_lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input to the function $$(\\tau + \\lambda) * Output_{NN}$$  \n",
    "If we consider der unified PCL version the values can be computed in the following way:\n",
    "$$V_p(s) =  \\tau \\sum_a exp \\{ \\frac{Q_p(s,a)}{\\tau}\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
