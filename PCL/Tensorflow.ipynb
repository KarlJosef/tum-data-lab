{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import threading\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import threading\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "GAME = 'Pendulum-v0'\n",
    "OUTPUT_GRAPH = True\n",
    "LOG_DIR = '/home/adrian/Schreibtisch/Uni/Data-Innovation-Lab/tensorflowlogs'\n",
    "N_WORKERS = multiprocessing.cpu_count()\n",
    "MAX_EP_STEP = 200\n",
    "MAX_GLOBAL_EP = 1500\n",
    "GLOBAL_NET_SCOPE = 'Global_Net'\n",
    "UPDATE_GLOBAL_ITER = 5\n",
    "GAMMA = 0.9\n",
    "ENTROPY_BETA = 0.01\n",
    "LR_A = 0.0001    # learning rate for actor\n",
    "LR_C = 0.001    # learning rate for critic\n",
    "GLOBAL_RUNNING_R = []\n",
    "GLOBAL_EP = 0\n",
    "\n",
    "env = gym.make(GAME)\n",
    "\n",
    "N_S = env.observation_space.shape[0]\n",
    "N_A = env.action_space.shape[0]\n",
    "A_BOUND = [env.action_space.low, env.action_space.high]\n",
    "\n",
    "\n",
    "class ACNet(object):\n",
    "    def __init__(self, scope, globalAC=None):\n",
    "\n",
    "        if scope == GLOBAL_NET_SCOPE:   # get global network\n",
    "            with tf.variable_scope(scope):\n",
    "                self.s = tf.placeholder(tf.float32, [None, N_S], 'S')\n",
    "                self.a_params, self.c_params = self._build_net(scope)[-2:]\n",
    "        else:   # local net, calculate losses\n",
    "            with tf.variable_scope(scope):\n",
    "                self.s = tf.placeholder(tf.float32, [None, N_S], 'S')\n",
    "                self.a_his = tf.placeholder(tf.float32, [None, N_A], 'A')\n",
    "                self.v_target = tf.placeholder(tf.float32, [None, 1], 'Vtarget')\n",
    "\n",
    "                mu, sigma, self.v, self.a_params, self.c_params = self._build_net(scope)\n",
    "\n",
    "                td = tf.subtract(self.v_target, self.v, name='TD_error')\n",
    "                with tf.name_scope('c_loss'):\n",
    "                    self.c_loss = tf.reduce_mean(tf.square(td))\n",
    "\n",
    "                with tf.name_scope('wrap_a_out'):\n",
    "                    mu, sigma = mu * A_BOUND[1], sigma + 1e-4\n",
    "\n",
    "                normal_dist = tf.distributions.Normal(mu, sigma)\n",
    "\n",
    "                with tf.name_scope('a_loss'):\n",
    "                    log_prob = normal_dist.log_prob(self.a_his)\n",
    "                    exp_v = log_prob * td\n",
    "                    entropy = normal_dist.entropy()  # encourage exploration\n",
    "                    self.exp_v = ENTROPY_BETA * entropy + exp_v\n",
    "                    self.a_loss = tf.reduce_mean(-self.exp_v)\n",
    "\n",
    "                with tf.name_scope('choose_a'):  # use local params to choose action\n",
    "                    self.A = tf.clip_by_value(tf.squeeze(normal_dist.sample(1), axis=0), A_BOUND[0], A_BOUND[1])\n",
    "                with tf.name_scope('local_grad'):\n",
    "                    self.a_grads = tf.gradients(self.a_loss, self.a_params)\n",
    "                    self.c_grads = tf.gradients(self.c_loss, self.c_params)\n",
    "\n",
    "            with tf.name_scope('sync'):\n",
    "                with tf.name_scope('pull'):\n",
    "                    self.pull_a_params_op = [l_p.assign(g_p) for l_p, g_p in zip(self.a_params, globalAC.a_params)]\n",
    "                    self.pull_c_params_op = [l_p.assign(g_p) for l_p, g_p in zip(self.c_params, globalAC.c_params)]\n",
    "                with tf.name_scope('push'):\n",
    "                    self.update_a_op = OPT_A.apply_gradients(zip(self.a_grads, globalAC.a_params))\n",
    "                    self.update_c_op = OPT_C.apply_gradients(zip(self.c_grads, globalAC.c_params))\n",
    "\n",
    "    def _build_net(self, scope):\n",
    "        w_init = tf.random_normal_initializer(0., .1)\n",
    "        with tf.variable_scope('critic'):   # only critic controls the rnn update\n",
    "            cell_size = 64\n",
    "            s = tf.expand_dims(self.s, axis=1,\n",
    "                               name='timely_input')  # [time_step, feature] => [time_step, batch, feature]\n",
    "            rnn_cell = tf.contrib.rnn.BasicRNNCell(cell_size)\n",
    "            self.init_state = rnn_cell.zero_state(batch_size=1, dtype=tf.float32)\n",
    "            outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "                cell=rnn_cell, inputs=s, initial_state=self.init_state, time_major=True)\n",
    "            cell_out = tf.reshape(outputs, [-1, cell_size], name='flatten_rnn_outputs')  # joined state representation\n",
    "            l_c = tf.layers.dense(cell_out, 50, tf.nn.relu6, kernel_initializer=w_init, name='lc')\n",
    "            v = tf.layers.dense(l_c, 1, kernel_initializer=w_init, name='v')  # state value\n",
    "\n",
    "        with tf.variable_scope('actor'):  # state representation is based on critic\n",
    "            l_a = tf.layers.dense(cell_out, 80, tf.nn.relu6, kernel_initializer=w_init, name='la')\n",
    "            mu = tf.layers.dense(l_a, N_A, tf.nn.tanh, kernel_initializer=w_init, name='mu')\n",
    "            sigma = tf.layers.dense(l_a, N_A, tf.nn.softplus, kernel_initializer=w_init, name='sigma')\n",
    "        a_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')\n",
    "        c_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')\n",
    "        return mu, sigma, v, a_params, c_params\n",
    "\n",
    "    def update_global(self, feed_dict):  # run by a local\n",
    "        SESS.run([self.update_a_op, self.update_c_op], feed_dict)  # local grads applies to global net\n",
    "\n",
    "    def pull_global(self):  # run by a local\n",
    "        SESS.run([self.pull_a_params_op, self.pull_c_params_op])\n",
    "\n",
    "    def choose_action(self, s, cell_state):  # run by a local\n",
    "        s = s[np.newaxis, :]\n",
    "        a, cell_state = SESS.run([self.A, self.final_state], {self.s: s, self.init_state: cell_state})\n",
    "        return a[0], cell_state\n",
    "\n",
    "\n",
    "class Worker(object):\n",
    "    def __init__(self, name, globalAC):\n",
    "        self.env = gym.make(GAME).unwrapped\n",
    "        self.name = name\n",
    "        self.AC = ACNet(name, globalAC)\n",
    "\n",
    "    def work(self):\n",
    "        global GLOBAL_RUNNING_R, GLOBAL_EP\n",
    "        total_step = 1\n",
    "        buffer_s, buffer_a, buffer_r = [], [], []\n",
    "        while not COORD.should_stop() and GLOBAL_EP < MAX_GLOBAL_EP:\n",
    "            s = self.env.reset()\n",
    "            ep_r = 0\n",
    "            rnn_state = SESS.run(self.AC.init_state)    # zero rnn state at beginning\n",
    "            keep_state = rnn_state.copy()       # keep rnn state for updating global net\n",
    "            for ep_t in range(MAX_EP_STEP):\n",
    "                if self.name == 'W_0':\n",
    "                    self.env.render()\n",
    "\n",
    "                a, rnn_state_ = self.AC.choose_action(s, rnn_state)  # get the action and next rnn state\n",
    "                s_, r, done, info = self.env.step(a)\n",
    "                done = True if ep_t == MAX_EP_STEP - 1 else False\n",
    "\n",
    "                ep_r += r\n",
    "                buffer_s.append(s)\n",
    "                buffer_a.append(a)\n",
    "                buffer_r.append((r+8)/8)    # normalize\n",
    "\n",
    "                if total_step % UPDATE_GLOBAL_ITER == 0 or done:   # update global and assign to local net\n",
    "                    if done:\n",
    "                        v_s_ = 0   # terminal\n",
    "                    else:\n",
    "                        v_s_ = SESS.run(self.AC.v, {self.AC.s: s_[np.newaxis, :], self.AC.init_state: rnn_state_})[0, 0]\n",
    "                    buffer_v_target = []\n",
    "                    for r in buffer_r[::-1]:    # reverse buffer r\n",
    "                        v_s_ = r + GAMMA * v_s_\n",
    "                        buffer_v_target.append(v_s_)\n",
    "                    buffer_v_target.reverse()\n",
    "\n",
    "                    buffer_s, buffer_a, buffer_v_target = np.vstack(buffer_s), np.vstack(buffer_a), np.vstack(buffer_v_target)\n",
    "\n",
    "                    feed_dict = {\n",
    "                        self.AC.s: buffer_s,\n",
    "                        self.AC.a_his: buffer_a,\n",
    "                        self.AC.v_target: buffer_v_target,\n",
    "                        self.AC.init_state: keep_state,\n",
    "                    }\n",
    "\n",
    "                    self.AC.update_global(feed_dict)\n",
    "                    buffer_s, buffer_a, buffer_r = [], [], []\n",
    "                    self.AC.pull_global()\n",
    "                    keep_state = rnn_state_.copy()   # replace the keep_state as the new initial rnn state_\n",
    "\n",
    "                s = s_\n",
    "                rnn_state = rnn_state_  # renew rnn state\n",
    "                total_step += 1\n",
    "\n",
    "                if done:\n",
    "                    if len(GLOBAL_RUNNING_R) == 0:  # record running episode reward\n",
    "                        GLOBAL_RUNNING_R.append(ep_r)\n",
    "                    else:\n",
    "                        GLOBAL_RUNNING_R.append(0.9 * GLOBAL_RUNNING_R[-1] + 0.1 * ep_r)\n",
    "                    print(\n",
    "                        self.name,\n",
    "                        \"Ep:\", GLOBAL_EP,\n",
    "                        \"| Ep_r: %i\" % GLOBAL_RUNNING_R[-1],\n",
    "                          )\n",
    "                    GLOBAL_EP += 1\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_1 Ep: 0 | Ep_r: -1314\n",
      "W_3 Ep: 1 | Ep_r: -1330\n",
      "W_2 Ep: 2 | Ep_r: -1359\n",
      "W_3 Ep: 3 | Ep_r: -1386\n",
      "W_1 Ep: 4 | Ep_r: -1388\n",
      "W_2 Ep: 5 | Ep_r: -1420\n",
      "W_1 Ep: 6 | Ep_r: -1416\n",
      "W_2 Ep: 7 | Ep_r: -1399\n",
      "W_3 Ep: 8 | Ep_r: -1397\n",
      "W_1 Ep: 9 | Ep_r: -1400\n",
      "W_2 W_3 Ep: 10 Ep:| Ep_r: -1427 \n",
      "10 | Ep_r: -1419\n",
      "W_0 Ep: 12 | Ep_r: -1433\n",
      "W_2 W_1 Ep: Ep:13 | Ep_r: -1439\n",
      " 13 W_3 Ep:| Ep_r: -1432\n",
      " 14 | Ep_r: -1433\n",
      "W_2 Ep: 16 | Ep_r: -1382\n",
      "W_1 Ep: 17 | Ep_r: -1370\n",
      "W_3 Ep: 18 | Ep_r: -1390\n",
      "W_2 Ep: 19 | Ep_r: -1369\n",
      "W_3 Ep: 20W_1 | Ep_r: -1336 \n",
      "Ep: 21 | Ep_r: -1331\n",
      "W_0 Ep: 22 | Ep_r: -1311\n",
      "W_2 Ep: 23 | Ep_r: -1292\n",
      "W_3W_1 Ep: 24 | Ep_r: -1277\n",
      " Ep: 24 | Ep_r: -1255\n",
      "W_2 Ep: 26 | Ep_r: -1299\n",
      "W_3 Ep: 27 | Ep_r: -1316\n",
      "W_1 Ep: 28 | Ep_r: -1282\n",
      "W_2 Ep: 29 | Ep_r: -1261\n",
      "W_1 Ep: 30 | Ep_r: -1229\n",
      "W_3 Ep: 31 | Ep_r: -1214\n",
      "W_2 Ep: 32 | Ep_r: -1197\n",
      "W_1 Ep: 33 | Ep_r: -1172\n",
      "W_3 Ep: 34 | Ep_r: -1147\n",
      "W_0 Ep: 35 | Ep_r: -1140\n",
      "W_2 Ep: 36 | Ep_r: -1146\n",
      "W_1 Ep: 37 | Ep_r: -1168\n",
      "W_3 Ep: 38 | Ep_r: -1200\n",
      "W_2 Ep: 39 | Ep_r: -1257\n",
      "W_1 Ep: 40 | Ep_r: -1306\n",
      "W_3 Ep: 41 | Ep_r: -1348\n",
      "W_2 Ep: 42 | Ep_r: -1304\n",
      "W_1 Ep: 43 | Ep_r: -1296\n",
      "W_3 Ep: 44 | Ep_r: -1265\n",
      "W_2 Ep: 45 | Ep_r: -1242\n",
      "W_0 Ep: 46 | Ep_r: -1229\n",
      "W_1 Ep: 47 | Ep_r: -1193\n",
      "W_3 Ep: 48 | Ep_r: -1202\n",
      "W_2 Ep: 49 | Ep_r: -1220\n",
      "W_1 Ep: 50 | Ep_r: -1204\n",
      "W_3 Ep: 51 | Ep_r: -1186\n",
      "W_2 Ep: 52 | Ep_r: -1154\n",
      "W_1 Ep: 53 | Ep_r: -1151\n",
      "W_3 Ep: 54 | Ep_r: -1124\n",
      "W_2 Ep: 55 | Ep_r: -1121\n",
      "W_1 Ep: 56 | Ep_r: -1099\n",
      "W_3 Ep: 57 | Ep_r: -1093\n",
      "W_0 Ep: 58 | Ep_r: -1063\n",
      "W_1 Ep: 59 | Ep_r: -1063\n",
      "W_2 Ep: 60 | Ep_r: -1046\n",
      "W_3 Ep: 61 | Ep_r: -1030\n",
      "W_1 Ep: 62 | Ep_r: -1017\n",
      "W_2 Ep: 63 | Ep_r: -991\n",
      "W_3 Ep: 64 | Ep_r: -983\n",
      "W_1 Ep: 65 | Ep_r: -1013\n",
      "W_2 Ep: 66 | Ep_r: -1017\n",
      "W_3 Ep: 67 | Ep_r: -1007\n",
      "W_1 Ep: 68 | Ep_r: -1019\n",
      "W_2 Ep: 69 | Ep_r: -997\n",
      "W_3 Ep: 70 | Ep_r: -997\n",
      "W_0 Ep: 71 | Ep_r: -997\n",
      "W_1 Ep: 72 | Ep_r: -1009\n",
      "W_2 Ep: 73 | Ep_r: -985\n",
      "W_3 Ep: 74 | Ep_r: -1010\n",
      "W_1 Ep: 75 | Ep_r: -1012\n",
      "W_3 Ep: 76 | Ep_r: -1014\n",
      "W_2 Ep: 77 | Ep_r: -1006\n",
      "W_1 Ep: 78 | Ep_r: -1019\n",
      "W_3 Ep: 79 | Ep_r: -1019\n",
      "W_2 Ep: 80 | Ep_r: -1032\n",
      "W_1 Ep: 81 | Ep_r: -1046\n",
      "W_2W_3 Ep: 82 | Ep_r: -1053\n",
      " Ep: 82 | Ep_r: -1068\n",
      "W_0 Ep: 84 | Ep_r: -1062\n",
      "W_1 Ep: 85 | Ep_r: -1045\n",
      "W_3 Ep: 86 | Ep_r: -1030\n",
      "W_2 Ep: 87 | Ep_r: -1033\n",
      "W_1 Ep: 88 | Ep_r: -1024\n",
      "W_3 Ep: 89 | Ep_r: -1013\n",
      "W_2 Ep: 90 | Ep_r: -975\n",
      "W_1 Ep: 91 | Ep_r: -954\n",
      "W_2 Ep: 92 | Ep_r: -980\n",
      "W_3 Ep: 93 | Ep_r: -983\n",
      "W_0 Ep: 94 | Ep_r: -992\n",
      "W_1 Ep: 95 | Ep_r: -969\n",
      "W_2 Ep: 96 | Ep_r: -956\n",
      "W_3 Ep: 97 | Ep_r: -952\n",
      "W_1 Ep: 98 | Ep_r: -969\n",
      "W_2 Ep: 99 | Ep_r: -1011\n",
      "W_3 Ep: 100 | Ep_r: -997\n",
      "W_1 Ep: 101 | Ep_r: -1018\n",
      "W_2 Ep: 102 | Ep_r: -1025\n",
      "W_3 Ep: 103 | Ep_r: -1008\n",
      "W_1 Ep: 104 | Ep_r: -992\n",
      "W_2 Ep: 105 | Ep_r: -984\n",
      "W_3 Ep: 106 | Ep_r: -978\n",
      "W_0 Ep: 107 | Ep_r: -969\n",
      "W_1 Ep: 108 | Ep_r: -964\n",
      "W_2 Ep: 109 | Ep_r: -944\n",
      "W_3 Ep: 110 | Ep_r: -938\n",
      "W_1 Ep: 111 | Ep_r: -964\n",
      "W_2 Ep: 112 | Ep_r: -973\n",
      "W_3 Ep: 113 | Ep_r: -952\n",
      "W_1 Ep: 114 | Ep_r: -966\n",
      "W_2 Ep: 115 | Ep_r: -980\n",
      "W_3 Ep: 116 | Ep_r: -971\n",
      "W_1 Ep: 117 | Ep_r: -993\n",
      "W_2 Ep: 118 | Ep_r: -996\n",
      "W_3 Ep: 119 | Ep_r: -974\n",
      "W_0 Ep: 120 | Ep_r: -982\n",
      "W_1 Ep: 121 | Ep_r: -949\n",
      "W_2 Ep: 122 | Ep_r: -944\n",
      "W_3 Ep: 123 | Ep_r: -929\n",
      "W_1 Ep: 124 | Ep_r: -914\n",
      "W_2 Ep: 125 | Ep_r: -900\n",
      "W_3 Ep: 126 | Ep_r: -888\n",
      "W_2 Ep: 127 | Ep_r: -869\n",
      "W_1 Ep: 128 | Ep_r: -860\n",
      "W_3 Ep: 129 | Ep_r: -865\n",
      "W_0 Ep: 130 | Ep_r: -853\n",
      "W_1 Ep: 131 | Ep_r: -902\n",
      "W_2 Ep: 132 | Ep_r: -928\n",
      "W_3 Ep: 133 | Ep_r: -966\n",
      "W_1 Ep: 134 | Ep_r: -987\n",
      "W_2 Ep: 135 | Ep_r: -996\n",
      "W_3 Ep: 136 | Ep_r: -974\n",
      "W_1 Ep: 137 | Ep_r: -942\n",
      "W_2 Ep: 138 | Ep_r: -935\n",
      "W_3 Ep: 139 | Ep_r: -931\n",
      "W_1 Ep: 140 | Ep_r: -903\n",
      "W_2 Ep: 141 | Ep_r: -928\n",
      "W_0 Ep: 142 | Ep_r: -902\n",
      "W_3 Ep: 143 | Ep_r: -903\n",
      "W_1 Ep: 144 | Ep_r: -891\n",
      "W_2 Ep: 145 | Ep_r: -895\n",
      "W_3 Ep: 146 | Ep_r: -883\n",
      "W_1 Ep: 147 | Ep_r: -906\n",
      "W_2 Ep: 148 | Ep_r: -891\n",
      "W_3 Ep: 149 | Ep_r: -903\n",
      "W_1 Ep: 150 | Ep_r: -911\n",
      "W_2 Ep: 151 | Ep_r: -927\n",
      "W_3 Ep: 152 | Ep_r: -945\n",
      "W_0 Ep: 153 | Ep_r: -920\n",
      "W_2 Ep: 154 | Ep_r: -920\n",
      "W_1 Ep: 155 | Ep_r: -924\n",
      "W_3 Ep: 156 | Ep_r: -910\n",
      "W_2 Ep: 157 | Ep_r: -897\n",
      "W_1 Ep: 158 | Ep_r: -885\n",
      "W_3 Ep: 159 | Ep_r: -892\n",
      "W_2 Ep: 160 | Ep_r: -868\n",
      "W_1 Ep: 161 | Ep_r: -872\n",
      "W_3 Ep: 162 | Ep_r: -846\n",
      "W_0 Ep: 163 | Ep_r: -840\n",
      "W_2 Ep: 164 | Ep_r: -821\n",
      "W_1 Ep: 165 | Ep_r: -838\n",
      "W_3 Ep: 166 | Ep_r: -843\n",
      "W_2 Ep: 167 | Ep_r: -851\n",
      "W_1 Ep: 168 | Ep_r: -844\n",
      "W_3 Ep: 169 | Ep_r: -812\n",
      "W_1 Ep: 170 | Ep_r: -843\n",
      "W_2 Ep: 171 | Ep_r: -826\n",
      "W_3 Ep: 172 | Ep_r: -857\n",
      "W_1 Ep: 173 | Ep_r: -864\n",
      "W_2 Ep: 174 | Ep_r: -839\n",
      "W_3 Ep: 175 | Ep_r: -820\n",
      "W_0 Ep: 176 | Ep_r: -862\n",
      "W_2 Ep: 177 | Ep_r: -867\n",
      "W_1 Ep: 178 | Ep_r: -858\n",
      "W_3 Ep: 179 | Ep_r: -855\n",
      "W_2 Ep: 180 | Ep_r: -832\n",
      "W_1 Ep: 181 | Ep_r: -801\n",
      "W_3 Ep: 182 | Ep_r: -798\n",
      "W_2 Ep: 183 | Ep_r: -804\n",
      "W_1 Ep: 184 | Ep_r: -790\n",
      "W_3 Ep: 185 | Ep_r: -820\n",
      "W_0 Ep: 186 | Ep_r: -790\n",
      "W_2 Ep: 187 | Ep_r: -793\n",
      "W_1 Ep: 188 | Ep_r: -809\n",
      "W_3 Ep: 189 | Ep_r: -781\n",
      "W_2 Ep: 190 | Ep_r: -796\n",
      "W_1 Ep: 191 | Ep_r: -813\n",
      "W_3 Ep: 192 | Ep_r: -826\n",
      "W_2 Ep: 193 | Ep_r: -809\n",
      "W_1 Ep: 194 | Ep_r: -821\n",
      "W_3 Ep: 195 | Ep_r: -792\n",
      "W_2 Ep: 196 | Ep_r: -812\n",
      "W_1 Ep: 197 | Ep_r: -825\n",
      "W_3 Ep: 198 | Ep_r: -825\n",
      "W_0 Ep: 199 | Ep_r: -837\n",
      "W_2 Ep: 200 | Ep_r: -853\n",
      "W_1 Ep: 201 | Ep_r: -828\n",
      "W_3 Ep: 202 | Ep_r: -811\n",
      "W_2 Ep: 203 | Ep_r: -832\n",
      "W_1 Ep: 204 | Ep_r: -841\n",
      "W_3 Ep: 205 | Ep_r: -845\n",
      "W_2 Ep: 206 | Ep_r: -801\n",
      "W_1 Ep: 207 | Ep_r: -786\n",
      "W_3 Ep: 208 | Ep_r: -759\n",
      "W_0 Ep: 209 | Ep_r: -724\n",
      "W_2 Ep: 210 | Ep_r: -707\n",
      "W_1 Ep: 211 | Ep_r: -663\n",
      "W_3 Ep: 212 | Ep_r: -664\n",
      "W_2 Ep: 213 | Ep_r: -691\n",
      "W_1 Ep: 214 | Ep_r: -698\n",
      "W_3 Ep: 215 | Ep_r: -748\n",
      "W_2 Ep: 216 | Ep_r: -741\n",
      "W_1 Ep: 217 | Ep_r: -762\n",
      "W_3 Ep: 218 | Ep_r: -732\n",
      "W_0 Ep: 219 | Ep_r: -698\n",
      "W_1 Ep: 220 | Ep_r: -682\n",
      "W_2 Ep: 221 | Ep_r: -679\n",
      "W_3 Ep: 222 | Ep_r: -650\n",
      "W_1 Ep: 223 | Ep_r: -688\n",
      "W_2 Ep: 224 | Ep_r: -672\n",
      "W_3 Ep: 225 | Ep_r: -659\n",
      "W_1 Ep: 226 | Ep_r: -649\n",
      "W_2 Ep: 227 | Ep_r: -684\n",
      "W_3 Ep: 228 | Ep_r: -695\n",
      "W_1 Ep: 229 | Ep_r: -698\n",
      "W_2 Ep: 230 | Ep_r: -710\n",
      "W_3 Ep: 231 | Ep_r: -692\n",
      "W_0 Ep: 232 | Ep_r: -676\n",
      "W_1 Ep: 233 | Ep_r: -662\n",
      "W_2 Ep: 234 | Ep_r: -698\n",
      "W_3 Ep: 235 | Ep_r: -694\n",
      "W_1 Ep: 236 | Ep_r: -731\n",
      "W_2 Ep: 237 | Ep_r: -743\n",
      "W_3 Ep: 238 | Ep_r: -735\n",
      "W_1 Ep: 239 | Ep_r: -721\n",
      "W_2 Ep: 240 | Ep_r: -715\n",
      "W_3 Ep: 241 | Ep_r: -723\n",
      "W_1 Ep: 242 | Ep_r: -732\n",
      "W_2 Ep: 243 | Ep_r: -705\n",
      "W_3 Ep: 244 | Ep_r: -702\n",
      "W_0 Ep: 245 | Ep_r: -701\n",
      "W_1 Ep: 246 | Ep_r: -685\n",
      "W_2 Ep: 247 | Ep_r: -670\n",
      "W_3 Ep: 248 | Ep_r: -658\n",
      "W_1 Ep: 249 | Ep_r: -632\n",
      "W_2 Ep: 250 | Ep_r: -655\n",
      "W_3 Ep: 251 | Ep_r: -684\n",
      "W_1 Ep: 252 | Ep_r: -704\n",
      "W_2 Ep: 253 | Ep_r: -711\n",
      "W_3 Ep: 254 | Ep_r: -737\n",
      "W_0 Ep: 255 | Ep_r: -754\n",
      "W_1 Ep: 256 | Ep_r: -758\n",
      "W_2 Ep: 257 | Ep_r: -749\n",
      "W_3 Ep: 258 | Ep_r: -728\n",
      "W_1 Ep: 259 | Ep_r: -710\n",
      "W_2 Ep: 260 | Ep_r: -706\n",
      "W_3 Ep: 261 | Ep_r: -706\n",
      "W_1 Ep: 262 | Ep_r: -699\n",
      "W_2 Ep: 263 | Ep_r: -682\n",
      "W_3 Ep: 264 | Ep_r: -680\n",
      "W_1 Ep: 265 | Ep_r: -693\n",
      "W_2 Ep: 266 | Ep_r: -703\n",
      "W_0 Ep: 267 | Ep_r: -728\n",
      "W_3 Ep: 268 | Ep_r: -734\n",
      "W_1 Ep: 269 | Ep_r: -744\n",
      "W_2 Ep: 270 | Ep_r: -741\n",
      "W_3 Ep: 271 | Ep_r: -770\n",
      "W_1 Ep: 272 | Ep_r: -747\n",
      "W_2 Ep: 273 | Ep_r: -767\n",
      "W_3 Ep: 274 | Ep_r: -780\n",
      "W_1 Ep: 275 | Ep_r: -783\n",
      "W_2 Ep: 276 | Ep_r: -787\n",
      "W_3 Ep: 277 | Ep_r: -776\n",
      "W_0 Ep: 278 | Ep_r: -764\n",
      "W_1 Ep: 279 | Ep_r: -755\n",
      "W_2 Ep: 280 | Ep_r: -719\n",
      "W_3 Ep: 281 | Ep_r: -674\n",
      "W_1 Ep: 282 | Ep_r: -649\n",
      "W_2 Ep: 283 | Ep_r: -654\n",
      "W_3 Ep: 284 | Ep_r: -636\n",
      "W_1 Ep: 285 | Ep_r: -615\n",
      "W_2 Ep: 286 | Ep_r: -621\n",
      "W_3 Ep: 287 | Ep_r: -625\n",
      "W_2 Ep: 288 | Ep_r: -618\n",
      "W_1 Ep: 289 | Ep_r: -636\n",
      "W_0 Ep: 290 | Ep_r: -628\n",
      "W_3 Ep: 291 | Ep_r: -612\n",
      "W_2 Ep: 292 | Ep_r: -591\n",
      "W_1 Ep: 293 | Ep_r: -546\n",
      "W_3 Ep: 294 | Ep_r: -572\n",
      "W_2 Ep: 295 | Ep_r: -586\n",
      "W_1 Ep: 295 | Ep_r: -615\n",
      "W_3 Ep: 297 | Ep_r: -639\n",
      "W_2 Ep: 298 | Ep_r: -614\n",
      "W_1 Ep: 299 | Ep_r: -559\n",
      "W_3 Ep: 300 | Ep_r: -596\n",
      "W_0 Ep: 301 | Ep_r: -607\n",
      "W_2 Ep: 302 | Ep_r: -616\n",
      "W_1 Ep: 303 | Ep_r: -643\n",
      "W_3 Ep: 304 | Ep_r: -618\n",
      "W_2 Ep: 305 | Ep_r: -625\n",
      "W_1 Ep: 306 | Ep_r: -613\n",
      "W_3 Ep: 307 | Ep_r: -612\n",
      "W_2 Ep: 308 | Ep_r: -616\n",
      "W_1 Ep: 309 | Ep_r: -582\n",
      "W_3 Ep: 310 | Ep_r: -564\n",
      "W_2 Ep: 311 | Ep_r: -535\n",
      "W_1 Ep: 312 | Ep_r: -567\n",
      "W_0 Ep: 313 | Ep_r: -598\n",
      "W_3 Ep: 314 | Ep_r: -578\n",
      "W_2 Ep: 315 | Ep_r: -573\n",
      "W_1 Ep: 316 | Ep_r: -543\n",
      "W_3 Ep: 317 | Ep_r: -541\n",
      "W_2 Ep: 318 | Ep_r: -555\n",
      "W_1 Ep: 319 | Ep_r: -513\n",
      "W_3 Ep: 320 | Ep_r: -516\n",
      "W_2 Ep: 321 | Ep_r: -547\n",
      "W_1 Ep: 322 | Ep_r: -542\n",
      "W_3 Ep: 323 | Ep_r: -556\n",
      "W_0 Ep: 324 | Ep_r: -570\n",
      "W_2 Ep: 325 | Ep_r: -581\n",
      "W_1W_3 Ep:  326Ep:  | Ep_r: -623\n",
      "326 | Ep_r: -582\n",
      "W_2 Ep: 328 | Ep_r: -612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_1 Ep: 329 | Ep_r: -617\n",
      "W_3 Ep: 330 | Ep_r: -608\n",
      "W_2 Ep: 331 | Ep_r: -592\n",
      "W_1 Ep: 332 | Ep_r: -609\n",
      "W_3 Ep: 333 | Ep_r: -601\n",
      "W_2 Ep: 334 | Ep_r: -594\n",
      "W_0 Ep: 335 | Ep_r: -593\n",
      "W_1 Ep: 336 | Ep_r: -587\n",
      "W_3 Ep: 337 | Ep_r: -578\n",
      "W_2 Ep: 338 | Ep_r: -587\n",
      "W_1 Ep: 339 | Ep_r: -555\n",
      "W_3 Ep: 340 | Ep_r: -569\n",
      "W_2 Ep: 341 | Ep_r: -605\n",
      "W_1 Ep: 342 | Ep_r: -585\n",
      "W_3 Ep: 343 | Ep_r: -579\n",
      "W_2 Ep: 344 | Ep_r: -605\n",
      "W_3 Ep: 345 | Ep_r: -612\n",
      "W_1 Ep: 346 | Ep_r: -619\n",
      "W_0 Ep: 347 | Ep_r: -600\n",
      "W_2 Ep: 348 | Ep_r: -606\n",
      "W_3 Ep: 349 | Ep_r: -612\n",
      "W_1 Ep: 350 | Ep_r: -604\n",
      "W_2 Ep: 351 | Ep_r: -583\n",
      "W_3 Ep: 352 | Ep_r: -608\n",
      "W_1 Ep: 353 | Ep_r: -649\n",
      "W_2 Ep: 354 | Ep_r: -683\n",
      "W_3 Ep: 355 | Ep_r: -668\n",
      "W_1 Ep: 356 | Ep_r: -696\n",
      "W_2 Ep: 357 | Ep_r: -687\n",
      "W_3 Ep: 358 | Ep_r: -687\n",
      "W_1 Ep: 359 | Ep_r: -671\n",
      "W_0 Ep: 360 | Ep_r: -671\n",
      "W_2 Ep: 361 | Ep_r: -661\n",
      "W_1 Ep: 362 | Ep_r: -674\n",
      "W_3 Ep: 363 | Ep_r: -620\n",
      "W_2 Ep: 364 | Ep_r: -585\n",
      "W_1 Ep: 365 | Ep_r: -594\n",
      "W_3 Ep: 366 | Ep_r: -604\n",
      "W_2 Ep: 367 | Ep_r: -571\n",
      "W_1 Ep: 368 | Ep_r: -544\n",
      "W_3 Ep: 369 | Ep_r: -503\n",
      "W_2 Ep: 370 | Ep_r: -521\n",
      "W_1 Ep: 371 | Ep_r: -509\n",
      "W_3 Ep: 372 | Ep_r: -539\n",
      "W_0 Ep: 373 | Ep_r: -565\n",
      "W_2 Ep: 374 | Ep_r: -590\n",
      "W_1 Ep: 375 | Ep_r: -595\n",
      "W_3 Ep: 376 | Ep_r: -602\n",
      "W_2 Ep: 377 | Ep_r: -594\n",
      "W_1 Ep: 378 | Ep_r: -612\n",
      "W_3 Ep: 379 | Ep_r: -591\n",
      "W_2 Ep: 380 | Ep_r: -607\n",
      "W_1 Ep: 381 | Ep_r: -612\n",
      "W_3 Ep: 382 | Ep_r: -617\n",
      "W_2 Ep: 383 | Ep_r: -610\n",
      "W_1 Ep: 384 | Ep_r: -589\n",
      "W_0 Ep: 385 | Ep_r: -604\n",
      "W_3 Ep: 386 | Ep_r: -577\n",
      "W_2 Ep: 387 | Ep_r: -533\n",
      "W_1 Ep: 388 | Ep_r: -547\n",
      "W_3 Ep: 389 | Ep_r: -506\n",
      "W_2 Ep: 390 | Ep_r: -544\n",
      "W_1 Ep: 391 | Ep_r: -550\n",
      "W_3 Ep: 392 | Ep_r: -561\n",
      "W_2 Ep: 393 | Ep_r: -560\n",
      "W_1 Ep: 394 | Ep_r: -573\n",
      "W_3 Ep: 395 | Ep_r: -582\n",
      "W_2 Ep: 396 | Ep_r: -538\n",
      "W_0 Ep: 397 | Ep_r: -563\n",
      "W_3 Ep: 398 | Ep_r: -560\n",
      "W_1 Ep: 399 | Ep_r: -551\n",
      "W_2 Ep: 400 | Ep_r: -562\n",
      "W_3 Ep: 401 | Ep_r: -560\n",
      "W_1 Ep: 402 | Ep_r: -558\n",
      "W_2 Ep: 403 | Ep_r: -571\n",
      "W_3 Ep: 404 | Ep_r: -588\n",
      "W_1 Ep: 405 | Ep_r: -603\n",
      "W_2 Ep: 406 | Ep_r: -570\n",
      "W_3 Ep: 407 | Ep_r: -540\n",
      "W_1 Ep: 408 | Ep_r: -577\n",
      "W_2 Ep: 409 | Ep_r: -576\n",
      "W_0 Ep: 410 | Ep_r: -544\n",
      "W_3 Ep: 411 | Ep_r: -568\n",
      "W_1 Ep: 412 | Ep_r: -578\n",
      "W_2 Ep: 413 | Ep_r: -577\n",
      "W_3 Ep: 414 | Ep_r: -589\n",
      "W_1 Ep: 415 | Ep_r: -599\n",
      "W_2 Ep: 416 | Ep_r: -605\n",
      "W_3 Ep: 417 | Ep_r: -614\n",
      "W_1 Ep: 418 | Ep_r: -648\n",
      "W_2 Ep: 419 | Ep_r: -623\n",
      "W_3 Ep: 420 | Ep_r: -601\n",
      "W_1 Ep: 421 | Ep_r: -607\n",
      "W_2 Ep: 422 | Ep_r: -600\n",
      "W_0 Ep: 423 | Ep_r: -593\n",
      "W_3 Ep: 424 | Ep_r: -609\n",
      "W_1 Ep: 425 | Ep_r: -562\n",
      "W_2 Ep: 426 | Ep_r: -582\n",
      "W_3 Ep: 427 | Ep_r: -593\n",
      "W_1 Ep: 428 | Ep_r: -618\n",
      "W_2 Ep: 429 | Ep_r: -626\n",
      "W_1 Ep: 430 | Ep_r: -630\n",
      "W_3 Ep: 431 | Ep_r: -607\n",
      "W_2 Ep: 432 | Ep_r: -586\n",
      "W_1 Ep: 433 | Ep_r: -608\n",
      "W_3 Ep: 434 | Ep_r: -596\n",
      "W_0 Ep: 435 | Ep_r: -598\n",
      "W_2 Ep: 436 | Ep_r: -591\n",
      "W_1 Ep: 437 | Ep_r: -545\n",
      "W_3 Ep: 438 | Ep_r: -518\n",
      "W_2 Ep: 439 | Ep_r: -534\n",
      "W_1 Ep: 440 | Ep_r: -547\n",
      "W_3 Ep: 441 | Ep_r: -545\n",
      "W_2 Ep: 442 | Ep_r: -531\n",
      "W_1 Ep: 443 | Ep_r: -506\n",
      "W_3 Ep: 444 | Ep_r: -542\n",
      "W_2 Ep: 445 | Ep_r: -553\n",
      "W_1 Ep: 446 | Ep_r: -553\n",
      "W_3 Ep: 447 | Ep_r: -524\n",
      "W_0 Ep: 448 | Ep_r: -513\n",
      "W_2 Ep: 449 | Ep_r: -517\n",
      "W_1 Ep: 450 | Ep_r: -533\n",
      "W_3 Ep: 451 | Ep_r: -520\n",
      "W_2 Ep: 452 | Ep_r: -508\n",
      "W_1 Ep: 453 | Ep_r: -457\n",
      "W_3 Ep: 454 | Ep_r: -425\n",
      "W_2 Ep: 455 | Ep_r: -410\n",
      "W_3 Ep: 456 | Ep_r: -383\n",
      "W_1 Ep: 457 | Ep_r: -399\n",
      "W_2 Ep: 458 | Ep_r: -425\n",
      "W_3 Ep: 459 | Ep_r: -435\n",
      "W_1 Ep: 460 | Ep_r: -472\n",
      "W_0 Ep: 461 | Ep_r: -467\n",
      "W_2 Ep: 462 | Ep_r: -448\n",
      "W_3 Ep: 463 | Ep_r: -493\n",
      "W_1 Ep: 464 | Ep_r: -499\n",
      "W_2 Ep: 465 | Ep_r: -529\n",
      "W_3 Ep: 466 | Ep_r: -529\n",
      "W_1 Ep: 467 | Ep_r: -545\n",
      "W_2 Ep: 468 | Ep_r: -504\n",
      "W_3 Ep: 469 | Ep_r: -492\n",
      "W_1 Ep: 470 | Ep_r: -510\n",
      "W_2 Ep: 471 | Ep_r: -472\n",
      "W_1 Ep: 472 | Ep_r: -452\n",
      "W_3 Ep: 473 | Ep_r: -424\n",
      "W_0 Ep: 474 | Ep_r: -435\n",
      "W_2 Ep: 475 | Ep_r: -418\n",
      "W_1 Ep: 476 | Ep_r: -449\n",
      "W_3 Ep: 477 | Ep_r: -431\n",
      "W_2 Ep: 478 | Ep_r: -441\n",
      "W_1 Ep: 479 | Ep_r: -463\n",
      "W_3 Ep: 480 | Ep_r: -457\n",
      "W_2 Ep: 481 | Ep_r: -451\n",
      "W_1 Ep: 482 | Ep_r: -475\n",
      "W_3 Ep: 483 | Ep_r: -469\n",
      "W_2 Ep: 484 | Ep_r: -422\n",
      "W_1 Ep: 485 | Ep_r: -393\n",
      "W_0 Ep: 486 | Ep_r: -368\n",
      "W_3 Ep: 487 | Ep_r: -397\n",
      "W_2 Ep: 488 | Ep_r: -449\n",
      "W_1 Ep: 489 | Ep_r: -459\n",
      "W_3 Ep: 490 | Ep_r: -468\n",
      "W_2 Ep: 491 | Ep_r: -504\n",
      "W_1 Ep: 492 | Ep_r: -507\n",
      "W_3 Ep: 493 | Ep_r: -483\n",
      "W_2 Ep: 494 | Ep_r: -448\n",
      "W_1 Ep: 495 | Ep_r: -417\n",
      "W_3 Ep: 496 | Ep_r: -432\n",
      "W_2 Ep: 497 | Ep_r: -389\n",
      "W_0 Ep: 498 | Ep_r: -403\n",
      "W_1 Ep: 499 | Ep_r: -377\n",
      "W_3 Ep: 500 | Ep_r: -410\n",
      "W_2 Ep: 501 | Ep_r: -408\n",
      "W_1 Ep: 502 | Ep_r: -452\n",
      "W_3 Ep: 503 | Ep_r: -447\n",
      "W_2 Ep: 504 | Ep_r: -429\n",
      "W_1 Ep: 505 | Ep_r: -441\n",
      "W_3 Ep: 506 | Ep_r: -453\n",
      "W_2 Ep: 507 | Ep_r: -477\n",
      "W_1 Ep: 508 | Ep_r: -469\n",
      "W_3 Ep: 509 | Ep_r: -490\n",
      "W_2 Ep: 510 | Ep_r: -454\n",
      "W_0 Ep: 511 | Ep_r: -450\n",
      "W_1 Ep: 512 | Ep_r: -458\n",
      "W_3 Ep: 513 | Ep_r: -455\n",
      "W_2 Ep: 514 | Ep_r: -409\n",
      "W_1 Ep: 515 | Ep_r: -381\n",
      "W_3 Ep: 516 | Ep_r: -356\n",
      "W_2 Ep: 517 | Ep_r: -347\n",
      "W_1 Ep: 518 | Ep_r: -375\n",
      "W_3 Ep: 519 | Ep_r: -404\n",
      "W_2 Ep: 520 | Ep_r: -452\n",
      "W_1 Ep: 521 | Ep_r: -447\n",
      "W_3 Ep: 522 | Ep_r: -469\n",
      "W_0 Ep: 523 | Ep_r: -497\n",
      "W_2 Ep: 524 | Ep_r: -516\n",
      "W_1 Ep: 525 | Ep_r: -491\n",
      "W_3 Ep: 526 | Ep_r: -507\n",
      "W_2 Ep: 527 | Ep_r: -497\n",
      "W_1 Ep: 528 | Ep_r: -489\n",
      "W_3 Ep: 529 | Ep_r: -480\n",
      "W_2 Ep: 530 | Ep_r: -503\n",
      "W_1 Ep: 531 | Ep_r: -493\n",
      "W_3 Ep: 532 | Ep_r: -484\n",
      "W_2 Ep: 533 | Ep_r: -517\n",
      "W_1 Ep: 534 | Ep_r: -554\n",
      "W_3 Ep: 535 | Ep_r: -552\n",
      "W_0 Ep: 536 | Ep_r: -524\n",
      "W_2 Ep: 537 | Ep_r: -498\n",
      "W_1 Ep: 538 | Ep_r: -504\n",
      "W_3 Ep: 539 | Ep_r: -507\n",
      "W_2 Ep: 540 | Ep_r: -515\n",
      "W_1 Ep: 541 | Ep_r: -530\n",
      "W_3 Ep: 542 | Ep_r: -560\n",
      "W_2 Ep: 543 | Ep_r: -585\n",
      "W_3W_1 Ep: Ep:  544 544 | Ep_r: -575\n",
      "| Ep_r: -544\n",
      "W_2 Ep: 546 | Ep_r: -530\n",
      "W_1 Ep: 547 | Ep_r: -491\n",
      "W_3 Ep: 548 | Ep_r: -468\n",
      "W_0 Ep: 549 | Ep_r: -475\n",
      "W_2 Ep: 550 | Ep_r: -504\n",
      "W_1 Ep: 551 | Ep_r: -507\n",
      "W_3 Ep: 552 | Ep_r: -531\n",
      "W_2 Ep: 553 | Ep_r: -491\n",
      "W_1 Ep: 554 | Ep_r: -524\n",
      "W_3 Ep: 555 | Ep_r: -512\n",
      "W_2 Ep: 556 | Ep_r: -500\n",
      "W_1 Ep: 557 | Ep_r: -520\n",
      "W_3 Ep: 558 | Ep_r: -507\n",
      "W_2 Ep: 559 | Ep_r: -483\n",
      "W_1 Ep: 560 | Ep_r: -448\n",
      "W_0 Ep: 561 | Ep_r: -417\n",
      "W_3 Ep: 562 | Ep_r: -441\n",
      "W_2 Ep: 563 | Ep_r: -437\n",
      "W_1 Ep: 564 | Ep_r: -473\n",
      "W_3 Ep: 565 | Ep_r: -468\n",
      "W_2 Ep: 566 | Ep_r: -476\n",
      "W_1 Ep: 567 | Ep_r: -471\n",
      "W_3 Ep: 568 | Ep_r: -454\n",
      "W_2 Ep: 569 | Ep_r: -448\n",
      "W_1 Ep: 570 | Ep_r: -476\n",
      "W_3 Ep: 571 | Ep_r: -470\n",
      "W_2 Ep: 572 | Ep_r: -488\n",
      "W_0 Ep: 573 | Ep_r: -495\n",
      "W_1 Ep: 574 | Ep_r: -459\n",
      "W_3 Ep: 575 | Ep_r: -467\n",
      "W_2 Ep: 576 | Ep_r: -434\n",
      "W_1 Ep: 577 | Ep_r: -430\n",
      "W_3 Ep: 578 | Ep_r: -388\n",
      "W_2 Ep: 579 | Ep_r: -376\n",
      "W_1 Ep: 580 | Ep_r: -405\n",
      "W_3 Ep: 581 | Ep_r: -419\n",
      "W_2 Ep: 582 | Ep_r: -431\n",
      "W_1 Ep: 583 | Ep_r: -454\n",
      "W_3 Ep: 584 | Ep_r: -451\n",
      "W_0 Ep: 585 | Ep_r: -478\n",
      "W_2 Ep: 586 | Ep_r: -458\n",
      "W_1 Ep: 587 | Ep_r: -487\n",
      "W_3 Ep: 588 | Ep_r: -465\n",
      "W_2 Ep: 589 | Ep_r: -461\n",
      "W_1 Ep: 590 | Ep_r: -484\n",
      "W_3 Ep: 591 | Ep_r: -463\n",
      "W_2 Ep: 592 | Ep_r: -486\n",
      "W_1 Ep: 593 | Ep_r: -480\n",
      "W_3 Ep: 594 | Ep_r: -486\n",
      "W_2 Ep: 595 | Ep_r: -492\n",
      "W_3 Ep: 596 | Ep_r: -524\n",
      "W_1 Ep: 597 | Ep_r: -555\n",
      "W_0 Ep: 598 | Ep_r: -526\n",
      "W_2 Ep: 599 | Ep_r: -487\n",
      "W_3 Ep: 600 | Ep_r: -479\n",
      "W_1 Ep: 601 | Ep_r: -471\n",
      "W_2 Ep: 602 | Ep_r: -477\n",
      "W_1 Ep: 603 | Ep_r: -504\n",
      "W_3 Ep: 603 | Ep_r: -508\n",
      "W_2 Ep: 605 | Ep_r: -471\n",
      "W_1 Ep: 606 | Ep_r: -495\n",
      "W_3 Ep: 607 | Ep_r: -477\n",
      "W_2 Ep: 608 | Ep_r: -483\n",
      "W_3 Ep: 609 | Ep_r: -462\n",
      "W_1 Ep: 610 | Ep_r: -443\n",
      "W_0 Ep: 611 | Ep_r: -413\n",
      "W_2 Ep: 612 | Ep_r: -435\n",
      "W_3 Ep: 613 | Ep_r: -419\n",
      "W_1 Ep: 614 | Ep_r: -390\n",
      "W_2 Ep: 615 | Ep_r: -365\n",
      "W_3 Ep: 616 | Ep_r: -369\n",
      "W_1 Ep: 617 | Ep_r: -386\n",
      "W_2 Ep: 618 | Ep_r: -401\n",
      "W_3 Ep: 619 | Ep_r: -402\n",
      "W_1 Ep: 620 | Ep_r: -389\n",
      "W_2 Ep: 621 | Ep_r: -377\n",
      "W_3 Ep: 622 | Ep_r: -369\n",
      "W_1 Ep: 623 | Ep_r: -380\n",
      "W_0 Ep: 624 | Ep_r: -409\n",
      "W_2 Ep: 625 | Ep_r: -465\n",
      "W_3 Ep: 626 | Ep_r: -509\n",
      "W_1 Ep: 627 | Ep_r: -498\n",
      "W_2 Ep: 628 | Ep_r: -461\n",
      "W_3 Ep: 629 | Ep_r: -455\n",
      "W_1 Ep: 630 | Ep_r: -451\n",
      "W_2 Ep: 631 | Ep_r: -446\n",
      "W_3 Ep: 632 | Ep_r: -415\n",
      "W_1 Ep: 633 | Ep_r: -444\n",
      "W_2 Ep: 634 | Ep_r: -466\n",
      "W_3 Ep: 635 | Ep_r: -470\n",
      "W_1 Ep: 636 | Ep_r: -465\n",
      "W_0 Ep: 637 | Ep_r: -445\n",
      "W_2 Ep: 638 | Ep_r: -428\n",
      "W_3 Ep: 639 | Ep_r: -452\n",
      "W_1 Ep: 640 | Ep_r: -491\n",
      "W_2 Ep: 641 | Ep_r: -456\n",
      "W_1 Ep: 642 | Ep_r: -491\n",
      "W_3 Ep: 643 | Ep_r: -526\n",
      "W_2 Ep: 644 | Ep_r: -527\n",
      "W_1 Ep: 645 | Ep_r: -522\n",
      "W_3 Ep: 646 | Ep_r: -535\n",
      "W_2 Ep: 647 | Ep_r: -549\n",
      "W_1 Ep: 648 | Ep_r: -529\n",
      "W_0 Ep: 649 | Ep_r: -561\n",
      "W_3 Ep: 650 | Ep_r: -574\n",
      "W_2 Ep: 651 | Ep_r: -583\n",
      "W_1 Ep: 652 | Ep_r: -579\n",
      "W_3 Ep: 653 | Ep_r: -561\n",
      "W_2 Ep: 654 | Ep_r: -571\n",
      "W_1 Ep: 655 | Ep_r: -593\n",
      "W_3 Ep: 656 | Ep_r: -622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_2 Ep: 657 | Ep_r: -625\n",
      "W_1 Ep: 658 | Ep_r: -643\n",
      "W_3 Ep: 659 | Ep_r: -646\n",
      "W_0 Ep: 660 | Ep_r: -623\n",
      "W_2 Ep: 661 | Ep_r: -588\n",
      "W_1 Ep: 662 | Ep_r: -529\n",
      "W_3 Ep: 663 | Ep_r: -543\n",
      "W_2 Ep: 664 | Ep_r: -530\n",
      "W_1 Ep: 665 | Ep_r: -552\n",
      "W_3 Ep: 666 | Ep_r: -566\n",
      "W_2 Ep: 667 | Ep_r: -563\n",
      "W_1 Ep: 668 | Ep_r: -593\n",
      "W_3 Ep: 669 | Ep_r: -600\n",
      "W_2 Ep: 670 | Ep_r: -626\n",
      "W_3 Ep: 671 | Ep_r: -564\n",
      "W_1 Ep: 672 | Ep_r: -562\n",
      "W_0 Ep: 673 | Ep_r: -533\n",
      "W_2 Ep: 674 | Ep_r: -550\n",
      "W_3 Ep: 675 | Ep_r: -549\n",
      "W_1 Ep: 676 | Ep_r: -507\n",
      "W_2 Ep: 677 | Ep_r: -470\n",
      "W_3 Ep: 678 | Ep_r: -463\n",
      "W_1 Ep: 679 | Ep_r: -442\n",
      "W_2 Ep: 680 | Ep_r: -411\n",
      "W_3 Ep: 681 | Ep_r: -397\n",
      "W_1 Ep: 682 | Ep_r: -422\n",
      "W_2 Ep: 683 | Ep_r: -462\n",
      "W_3 Ep: 684 | Ep_r: -482\n",
      "W_1 Ep: 685 | Ep_r: -493\n",
      "W_0 Ep: 686 | Ep_r: -519\n",
      "W_2 Ep: 687 | Ep_r: -549\n",
      "W_3 Ep: 688 | Ep_r: -534\n",
      "W_1 Ep: 689 | Ep_r: -493\n",
      "W_2 Ep: 690 | Ep_r: -487\n",
      "W_3 Ep: 691 | Ep_r: -438\n",
      "W_1 Ep: 692 | Ep_r: -421\n",
      "W_2 Ep: 693 | Ep_r: -442\n",
      "W_3 Ep: 694 | Ep_r: -398\n",
      "W_1 Ep: 695 | Ep_r: -371\n",
      "W_2 Ep: 696 | Ep_r: -348\n",
      "W_3 Ep: 697 | Ep_r: -327\n",
      "W_1 Ep: 698 | Ep_r: -295\n",
      "W_0 Ep: 699 | Ep_r: -293\n",
      "W_2 Ep: 700 | Ep_r: -277\n",
      "W_3 Ep: 701 | Ep_r: -308\n",
      "W_1 Ep: 702 | Ep_r: -358\n",
      "W_2 Ep: 703 | Ep_r: -348\n",
      "W_3 Ep: 704 | Ep_r: -314\n",
      "W_1 Ep: 705 | Ep_r: -296\n",
      "W_2 Ep: 706 | Ep_r: -349\n",
      "W_3 Ep: 707 | Ep_r: -354\n",
      "W_1 Ep: 708 | Ep_r: -372\n",
      "W_2 Ep: 709 | Ep_r: -378\n",
      "W_3 Ep: 710 | Ep_r: -381\n",
      "W_1 Ep: 711 | Ep_r: -376\n",
      "W_0 Ep: 712 | Ep_r: -377\n",
      "W_2 Ep: 713 | Ep_r: -407\n",
      "W_3 Ep: 714 | Ep_r: -394\n",
      "W_1 Ep: 715 | Ep_r: -382\n",
      "W_2 Ep: 716 | Ep_r: -412\n",
      "W_3 Ep: 717 | Ep_r: -385\n",
      "W_1 Ep: 718 | Ep_r: -373\n",
      "W_2 Ep: 719 | Ep_r: -402\n",
      "W_3 Ep: 720 | Ep_r: -439\n",
      "W_1 Ep: 721 | Ep_r: -421\n",
      "W_2 Ep: 722 | Ep_r: -447\n",
      "W_3 Ep: 723 | Ep_r: -456\n",
      "W_1 Ep: 724 | Ep_r: -491\n",
      "W_0 Ep: 725 | Ep_r: -510\n",
      "W_3 Ep: 726 | Ep_r: -473\n",
      "W_2 Ep: 727 | Ep_r: -493\n",
      "W_1 Ep: 728 | Ep_r: -471\n",
      "W_3 Ep: 729 | Ep_r: -424\n",
      "W_2 Ep: 730 | Ep_r: -420\n",
      "W_1 Ep: 731 | Ep_r: -392\n",
      "W_3 Ep: 732 | Ep_r: -379\n",
      "W_2 Ep: 733 | Ep_r: -355\n",
      "W_1 Ep: 734 | Ep_r: -361\n",
      "W_3 Ep: 735 | Ep_r: -416\n",
      "W_2 Ep: 736 | Ep_r: -454\n",
      "W_0 Ep: 737 | Ep_r: -422\n",
      "W_1 Ep: 738 | Ep_r: -456\n",
      "W_3 Ep: 739 | Ep_r: -437\n",
      "W_2 Ep: 740 | Ep_r: -469\n",
      "W_1 Ep: 741 | Ep_r: -449\n",
      "W_3 Ep: 742 | Ep_r: -473\n",
      "W_2 Ep: 743 | Ep_r: -483\n",
      "W_1 Ep: 744 | Ep_r: -476\n",
      "W_3 Ep: 745 | Ep_r: -442\n",
      "W_2 Ep: 746 | Ep_r: -425\n",
      "W_1 Ep: 747 | Ep_r: -382\n",
      "W_3 Ep: 748 | Ep_r: -426\n",
      "W_2 Ep: 749 | Ep_r: -463\n",
      "W_0 Ep: 750 | Ep_r: -485\n",
      "W_1 Ep: 751 | Ep_r: -476\n",
      "W_3 Ep: 752 | Ep_r: -455\n",
      "W_2 Ep: 753 | Ep_r: -450\n",
      "W_1 Ep: 754 | Ep_r: -432\n",
      "W_3 Ep: 755 | Ep_r: -441\n",
      "W_2 Ep: 756 | Ep_r: -463\n",
      "W_3 Ep: 757 | Ep_r: -444\n",
      "W_1 Ep: 758 | Ep_r: -455\n",
      "W_2 Ep: 759 | Ep_r: -498\n",
      "W_3 Ep: 760 | Ep_r: -502\n",
      "W_1 Ep: 761 | Ep_r: -531\n",
      "W_2 Ep: 762 | Ep_r: -531\n",
      "W_0 Ep: 763 | Ep_r: -558\n",
      "W_3 Ep: 764 | Ep_r: -516\n",
      "W_1 Ep: 765 | Ep_r: -532\n",
      "W_2 Ep: 766 | Ep_r: -504\n",
      "W_3 Ep: 767 | Ep_r: -494\n",
      "W_1 Ep: 768 | Ep_r: -471\n",
      "W_2 Ep: 769 | Ep_r: -451\n",
      "W_3 Ep: 770 | Ep_r: -447\n",
      "W_1 Ep: 771 | Ep_r: -416\n",
      "W_2 Ep: 772 | Ep_r: -445\n",
      "W_3 Ep: 773 | Ep_r: -481\n",
      "W_1 Ep: 774 | Ep_r: -499\n",
      "W_2 Ep: 775 | Ep_r: -503\n",
      "W_0 Ep: 776 | Ep_r: -466\n",
      "W_3 Ep: 777 | Ep_r: -464\n",
      "W_1 Ep: 778 | Ep_r: -470\n",
      "W_2 Ep: 779 | Ep_r: -478\n",
      "W_3 Ep: 780 | Ep_r: -497\n",
      "W_1 Ep: 781 | Ep_r: -500\n",
      "W_2 Ep: 782 | Ep_r: -516\n",
      "W_3 Ep: 783 | Ep_r: -491\n",
      "W_1 Ep: 784 | Ep_r: -469\n",
      "W_2 Ep: 785 | Ep_r: -458\n",
      "W_0 Ep: 786 | Ep_r: -466\n",
      "W_3 Ep: 787 | Ep_r: -434\n",
      "W_2 Ep: 788 | Ep_r: -418\n",
      "W_1 Ep: 789 | Ep_r: -444\n",
      "W_3 Ep: 790 | Ep_r: -440\n",
      "W_2 Ep: 791 | Ep_r: -450\n",
      "W_1 Ep: 792 | Ep_r: -485\n",
      "W_3 Ep: 793 | Ep_r: -463\n",
      "W_2 Ep: 794 | Ep_r: -470\n",
      "W_1 Ep: 795 | Ep_r: -479\n",
      "W_3 Ep: 796 | Ep_r: -485\n",
      "W_2 Ep: 797 | Ep_r: -450\n",
      "W_1 Ep: 798 | Ep_r: -405\n",
      "W_0 Ep: 799 | Ep_r: -380\n",
      "W_3 Ep: 800 | Ep_r: -369\n",
      "W_2 Ep: 801 | Ep_r: -332\n",
      "W_1 Ep: 802 | Ep_r: -366\n",
      "W_3 Ep: 803 | Ep_r: -342\n",
      "W_1W_2  Ep: 804 Ep:| Ep_r: -315 \n",
      "804 | Ep_r: -335\n",
      "W_3 Ep: 806 | Ep_r: -284\n",
      "W_1 Ep: 807 | Ep_r: -310\n",
      "W_2 Ep: 808 | Ep_r: -346\n",
      "W_3 Ep: 809 | Ep_r: -386\n",
      "W_1 Ep: 810 | Ep_r: -402\n",
      "W_2 Ep: 811 | Ep_r: -440\n",
      "W_0 Ep: 812 | Ep_r: -423\n",
      "W_3 Ep: 813 | Ep_r: -407\n",
      "W_1 Ep: 814 | Ep_r: -380\n",
      "W_2 Ep: 815 | Ep_r: -396\n",
      "W_3 Ep: 816 | Ep_r: -423\n",
      "W_1 Ep: 817 | Ep_r: -421\n",
      "W_2 Ep: 818 | Ep_r: -459\n",
      "W_3 Ep: 819 | Ep_r: -466\n",
      "W_2 Ep: 820 | Ep_r: -460\n",
      "W_1 Ep: 821 | Ep_r: -469\n",
      "W_3 Ep: 822 | Ep_r: -436\n",
      "W_2 Ep: 823 | Ep_r: -419\n",
      "W_1 Ep: 824 | Ep_r: -417\n",
      "W_0 Ep: 825 | Ep_r: -404\n",
      "W_3 Ep: 826 | Ep_r: -413\n",
      "W_2 Ep: 827 | Ep_r: -398\n",
      "W_1 Ep: 828 | Ep_r: -386\n",
      "W_3 Ep: 829 | Ep_r: -400\n",
      "W_2 Ep: 830 | Ep_r: -442\n",
      "W_1 Ep: 831 | Ep_r: -425\n",
      "W_3 Ep: 832 | Ep_r: -454\n",
      "W_2 Ep: 833 | Ep_r: -449\n",
      "W_1 Ep: 834 | Ep_r: -418\n",
      "W_3 Ep: 835 | Ep_r: -403\n",
      "W_2 Ep: 836 | Ep_r: -416\n",
      "W_1 Ep: 837 | Ep_r: -431\n",
      "W_0 Ep: 838 | Ep_r: -429\n",
      "W_3 Ep: 839 | Ep_r: -386\n",
      "W_2 Ep: 840 | Ep_r: -361\n",
      "W_1 Ep: 841 | Ep_r: -352\n",
      "W_3 Ep: 842 | Ep_r: -384\n",
      "W_2 Ep: 843 | Ep_r: -410\n",
      "W_1 Ep: 844 | Ep_r: -395\n",
      "W_3 Ep: 845 | Ep_r: -409\n",
      "W_2 Ep: 846 | Ep_r: -422\n",
      "W_1 Ep: 847 | Ep_r: -439\n",
      "W_3 Ep: 848 | Ep_r: -487\n",
      "W_2 Ep: 849 | Ep_r: -452\n",
      "W_1 Ep: 850 | Ep_r: -461\n",
      "W_0 Ep: 851 | Ep_r: -494\n",
      "W_2 Ep: 852 | Ep_r: -498\n",
      "W_3 Ep: 853 | Ep_r: -488\n",
      "W_1 Ep: 854 | Ep_r: -476\n",
      "W_2 Ep: 855 | Ep_r: -442\n",
      "W_3 Ep: 856 | Ep_r: -438\n",
      "W_1 Ep: 857 | Ep_r: -449\n",
      "W_2 Ep: 858 | Ep_r: -404\n",
      "W_3 Ep: 859 | Ep_r: -365\n",
      "W_1 Ep: 860 | Ep_r: -342\n",
      "W_2 Ep: 861 | Ep_r: -334\n",
      "W_3 Ep: 862 | Ep_r: -314\n",
      "W_0 Ep: W_1 Ep:863  863 | Ep_r: -369\n",
      "| Ep_r: -365\n",
      "W_2 Ep: 865 | Ep_r: -359\n",
      "W_3 Ep: 866 | Ep_r: -350\n",
      "W_1 Ep: 867 | Ep_r: -355\n",
      "W_2 Ep: 868 | Ep_r: -321\n",
      "W_3 Ep: 869 | Ep_r: -343\n",
      "W_1 Ep: 870 | Ep_r: -310\n",
      "W_2 Ep: 871 | Ep_r: -292\n",
      "W_3 Ep: 872 | Ep_r: -290\n",
      "W_1 Ep: 873 | Ep_r: -300\n",
      "W_0 Ep: 874 | Ep_r: -284\n",
      "W_2 Ep: 875 | Ep_r: -269\n",
      "W_3 Ep: 876 | Ep_r: -297\n",
      "W_1 Ep: 877 | Ep_r: -338\n",
      "W_3 Ep: 878 | Ep_r: -385\n",
      "W_2 Ep: 879 | Ep_r: -413\n",
      "W_1 Ep: 880 | Ep_r: -412\n",
      "W_3 Ep: 881 | Ep_r: -424\n",
      "W_1 Ep: 882 | Ep_r: -452\n",
      "W_2 Ep: 883 | Ep_r: -448\n",
      "W_3 Ep: 884 | Ep_r: -417\n",
      "W_1 Ep: 885 | Ep_r: -452\n",
      "W_2 Ep: 886 | Ep_r: -461\n",
      "W_0 Ep: 887 | Ep_r: -487\n",
      "W_3 Ep: 888 | Ep_r: -466\n",
      "W_1 Ep: 889 | Ep_r: -459\n",
      "W_2 Ep: 890 | Ep_r: -440\n",
      "W_1 Ep: 891 | Ep_r: -423\n",
      "W_3 Ep: 892 | Ep_r: -395\n",
      "W_2 Ep: 893 | Ep_r: -447\n",
      "W_1 Ep: 894 | Ep_r: -483\n",
      "W_3 Ep: 895 | Ep_r: -501\n",
      "W_2 Ep: 896 | Ep_r: -491\n",
      "W_1 Ep: 897W_2 | Ep_r: -535\n",
      " Ep: 897 | Ep_r: -575\n",
      "W_3 Ep: 899 | Ep_r: -586\n",
      "W_0 Ep: 900 | Ep_r: -568\n",
      "W_2 Ep: 901 | Ep_r: -579\n",
      "W_3 Ep: 902 | Ep_r: -522\n",
      "W_1 Ep: 903 | Ep_r: -552\n",
      "W_2 Ep: 904 | Ep_r: -511\n",
      "W_3 Ep: 905 | Ep_r: -460\n",
      "W_1 Ep: 906 | Ep_r: -468\n",
      "W_2 Ep: 907 | Ep_r: -462\n",
      "W_1 Ep: 908 | Ep_r: -497\n",
      "W_3 Ep: 909 | Ep_r: -516\n",
      "W_2 Ep: 910 | Ep_r: -505\n",
      "W_1 Ep: 911 | Ep_r: -537\n",
      "W_3 Ep: 912 | Ep_r: -510\n",
      "W_2 Ep: 913 | Ep_r: -539\n",
      "W_1 Ep: 914 | Ep_r: -539\n",
      "W_3 Ep: 915 | Ep_r: -499\n",
      "W_0 Ep: 916 | Ep_r: -541\n",
      "W_2 Ep: 917 | Ep_r: -553\n",
      "W_1 Ep: 918 | Ep_r: -538\n",
      "W_3 Ep: 919 | Ep_r: -498\n",
      "W_2 Ep: 920 | Ep_r: -461\n",
      "W_1 Ep: 921 | Ep_r: -511\n",
      "W_3 Ep: 922 | Ep_r: -500\n",
      "W_2 Ep: 923 | Ep_r: -476\n",
      "W_1 Ep: 924 | Ep_r: -442\n",
      "W_3 Ep: 925 | Ep_r: -465\n",
      "W_1 Ep: 926 | Ep_r: -473\n",
      "W_2 Ep: 927 | Ep_r: -465\n",
      "W_3 Ep: 928 | Ep_r: -433\n",
      "W_0 Ep: 929 | Ep_r: -430\n",
      "W_1 Ep: 930 | Ep_r: -462\n",
      "W_2 Ep: 931 | Ep_r: -442\n",
      "W_3 Ep: 932 | Ep_r: -412\n",
      "W_1 Ep: 933 | Ep_r: -438\n",
      "W_2 Ep: 934 | Ep_r: -449\n",
      "W_3 Ep: 935 | Ep_r: -472\n",
      "W_2 Ep: 936 | Ep_r: -511\n",
      "W_1 Ep: 937 | Ep_r: -487\n",
      "W_3 Ep: 938 | Ep_r: -480\n",
      "W_0 Ep: 939 | Ep_r: -473\n",
      "W_2 Ep: 940 | Ep_r: -466\n",
      "W_1 Ep: 941 | Ep_r: -420\n",
      "W_3 Ep: 942 | Ep_r: -404\n",
      "W_2 Ep: 943 | Ep_r: -377\n",
      "W_1 Ep: 944 | Ep_r: -339\n",
      "W_3 Ep: 945 | Ep_r: -305\n",
      "W_1 Ep: 946 | Ep_r: -315\n",
      "W_2 Ep: 947 | Ep_r: -324\n",
      "W_3 Ep: 948 | Ep_r: -364\n",
      "W_1 Ep: 949 | Ep_r: -396\n",
      "W_2 Ep: 950 | Ep_r: -429\n",
      "W_3 Ep: 951 | Ep_r: -400\n",
      "W_0 Ep: 952 | Ep_r: -441\n",
      "W_1 Ep: 953 | Ep_r: -397\n",
      "W_2 Ep: 954 | Ep_r: -385\n",
      "W_3 Ep: 955 | Ep_r: -386\n",
      "W_1 Ep: 956 | Ep_r: -375\n",
      "W_2 Ep: 957 | Ep_r: -364\n",
      "W_3 Ep: 958 | Ep_r: -355\n",
      "W_1 Ep: 959 | Ep_r: -386\n",
      "W_2 Ep: 960 | Ep_r: -403\n",
      "W_3 Ep: 961 | Ep_r: -376\n",
      "W_1 Ep: 962 | Ep_r: -379\n",
      "W_2 Ep: 963 | Ep_r: -381\n",
      "W_0 Ep: 964 | Ep_r: -356\n",
      "W_3 Ep: 965 | Ep_r: -400\n",
      "W_1 Ep: 966 | Ep_r: -400\n",
      "W_2 Ep: 967 | Ep_r: -386\n",
      "W_3 Ep: 968 | Ep_r: -387\n",
      "W_1 Ep: 969 | Ep_r: -404\n",
      "W_2 Ep: 970 | Ep_r: -404\n",
      "W_3 Ep: 971 | Ep_r: -424\n",
      "W_1 Ep: 972 | Ep_r: -408\n",
      "W_2 Ep: 973 | Ep_r: -425\n",
      "W_0 Ep: 974 | Ep_r: -411\n",
      "W_3 Ep: 975 | Ep_r: -410\n",
      "W_1 Ep: 976 | Ep_r: -396\n",
      "W_2 Ep: 977 | Ep_r: -396\n",
      "W_3 Ep: 978 | Ep_r: -384\n",
      "W_1 Ep: 979 | Ep_r: -386\n",
      "W_2 Ep: 980 | Ep_r: -388\n",
      "W_3 Ep: 981 | Ep_r: -376\n",
      "W_1 Ep: 982 | Ep_r: -378\n",
      "W_2 Ep: 983 | Ep_r: -355\n",
      "W_0 Ep: 984 | Ep_r: -387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_3 Ep: 985 | Ep_r: -362\n",
      "W_1 Ep: 986 | Ep_r: -408\n",
      "W_2 Ep: 987 | Ep_r: -408\n",
      "W_3 Ep: 988 | Ep_r: -402\n",
      "W_1 Ep: 989 | Ep_r: -402\n",
      "W_2 Ep: 990 | Ep_r: -388\n",
      "W_3 Ep: 991 | Ep_r: -390\n",
      "W_1 Ep: 992 | Ep_r: -364\n",
      "W_2 Ep: 993 | Ep_r: -390\n",
      "W_0 Ep: 994 | Ep_r: -391\n",
      "W_3 Ep: 995 | Ep_r: -420\n",
      "W_1 Ep: 996 | Ep_r: -474\n",
      "W_2 Ep: 997 | Ep_r: -511\n",
      "W_3 Ep: 998 | Ep_r: -530\n",
      "W_1 Ep: 999 | Ep_r: -544\n",
      "W_2 Ep: 1000 | Ep_r: -556\n",
      "W_3 Ep: 1001 | Ep_r: -540\n",
      "W_1 Ep: 1002 | Ep_r: -513\n",
      "W_2 Ep: 1003 | Ep_r: -502\n",
      "W_0 Ep: 1004 | Ep_r: -492\n",
      "W_3 Ep: 1005 | Ep_r: -511\n",
      "W_1 Ep: 1006 | Ep_r: -487\n",
      "W_2 Ep: 1007 | Ep_r: -478\n",
      "W_3 Ep: 1008 | Ep_r: -484\n",
      "W_1 Ep: 1009 | Ep_r: -486\n",
      "W_2 Ep: 1010 | Ep_r: -491\n",
      "W_1 Ep: 1011 | Ep_r: -534\n",
      "W_3 Ep: 1012 | Ep_r: -563\n",
      "W_2 Ep: 1013 | Ep_r: -588\n",
      "W_1 Ep: 1014 | Ep_r: -616\n",
      "W_3 Ep: 1015 | Ep_r: -625\n",
      "W_2 Ep: 1016 | Ep_r: -590\n",
      "W_0 Ep: 1017 | Ep_r: -584\n",
      "W_1 Ep: 1018 | Ep_r: -552\n",
      "W_3 Ep: 1019 | Ep_r: -567\n",
      "W_2 Ep: 1020 | Ep_r: -578\n",
      "W_1 Ep: 1021 | Ep_r: -594\n",
      "W_3 Ep: 1022 | Ep_r: -575\n",
      "W_2 Ep: 1023 | Ep_r: -558\n",
      "W_1 Ep: 1024 | Ep_r: -557\n",
      "W_3 Ep: 1025 | Ep_r: -528\n",
      "W_2 Ep: 1026 | Ep_r: -489\n",
      "W_1 Ep: 1027 | Ep_r: -454\n",
      "W_3 Ep: 1028 | Ep_r: -477\n",
      "W_2 Ep: 1029 | Ep_r: -471\n",
      "W_0 Ep: 1030 | Ep_r: -451\n",
      "W_1 Ep: 1031 | Ep_r: -461\n",
      "W_3 Ep: 1032 | Ep_r: -455\n",
      "W_2 Ep: 1033 | Ep_r: -453\n",
      "W_1 Ep: 1034 | Ep_r: -429\n",
      "W_3 Ep: 1035 | Ep_r: -452\n",
      "W_2 Ep: 1036 | Ep_r: -478\n",
      "W_1 Ep: 1037 | Ep_r: -484\n",
      "W_3 Ep: 1038 | Ep_r: -462\n",
      "W_2 Ep: 1039 | Ep_r: -475\n",
      "W_1 Ep: 1040 | Ep_r: -467\n",
      "W_3 Ep: 1041 | Ep_r: -460\n",
      "W_2 Ep: 1042 | Ep_r: -428\n",
      "W_1 Ep: 1043 | Ep_r: -425\n",
      "W_0 Ep: 1044 | Ep_r: -438\n",
      "W_3 Ep: 1045 | Ep_r: -408\n",
      "W_2 Ep: 1046 | Ep_r: -394\n",
      "W_1 Ep: 1047 | Ep_r: -381\n",
      "W_3 Ep: 1048 | Ep_r: -432\n",
      "W_2 Ep: 1049 | Ep_r: -454\n",
      "W_1 Ep: 1050 | Ep_r: -436\n",
      "W_3 Ep: 1051 | Ep_r: -432\n",
      "W_2 Ep: 1052 | Ep_r: -456\n",
      "W_1 Ep: 1053 | Ep_r: -424\n",
      "W_3 Ep: 1054 | Ep_r: -409\n",
      "W_2 Ep: 1055 | Ep_r: -368\n",
      "W_1 Ep: 1056 | Ep_r: -331\n",
      "W_0 Ep: 1057 | Ep_r: -338\n",
      "W_3 Ep: 1058 | Ep_r: -345\n",
      "W_2 Ep: 1059 | Ep_r: -374\n",
      "W_1 Ep: 1060 | Ep_r: -404\n",
      "W_3 Ep: 1061 | Ep_r: -424\n",
      "W_2 Ep: 1062 | Ep_r: -449\n",
      "W_1 Ep: 1063 | Ep_r: -458\n",
      "W_3 Ep: 1064 | Ep_r: -454\n",
      "W_2 Ep: 1065 | Ep_r: -465\n",
      "W_1 Ep: 1066 | Ep_r: -489\n",
      "W_3 Ep: 1067 | Ep_r: -522\n",
      "W_2 Ep: 1068 | Ep_r: -554\n",
      "W_1 Ep: 1069 | Ep_r: -581\n",
      "W_0 Ep: 1070 | Ep_r: -539\n",
      "W_3 Ep: 1071 | Ep_r: -499\n",
      "W_2 Ep: 1072 | Ep_r: -475\n",
      "W_1 Ep: 1073 | Ep_r: -508\n",
      "W_3 Ep: 1074 | Ep_r: -511\n",
      "W_2 Ep: 1075 | Ep_r: -534\n",
      "W_1 Ep: 1076 | Ep_r: -534\n",
      "W_3 Ep: 1077 | Ep_r: -521\n",
      "W_2 Ep: 1078 | Ep_r: -510\n",
      "W_1 Ep: 1079 | Ep_r: -473\n",
      "W_3 Ep: 1080 | Ep_r: -426\n",
      "W_2 Ep: 1081 | Ep_r: -452\n",
      "W_1 Ep: 1082 | Ep_r: -433\n",
      "W_0 Ep: 1083 | Ep_r: -444\n",
      "W_2 W_3Ep:  1084 | Ep_r: -445\n",
      "Ep: 1084 | Ep_r: -414\n",
      "W_1 Ep: 1086 | Ep_r: -400\n",
      "W_2 Ep: 1087 | Ep_r: -401\n",
      "W_1 Ep: 1088 | Ep_r: -387\n",
      "W_3 Ep: 1089 | Ep_r: -390\n",
      "W_1 Ep: 1090 | Ep_r: -405\n",
      "W_2 Ep: 1091 | Ep_r: -419\n",
      "W_3 Ep: 1092 | Ep_r: -391\n",
      "W_1 Ep: 1093 | Ep_r: -406\n",
      "W_2 Ep: 1094 | Ep_r: -440\n",
      "W_3 Ep: 1095 | Ep_r: -439\n",
      "W_0 Ep: 1096 | Ep_r: -422\n",
      "W_1 Ep: 1097 | Ep_r: -448\n",
      "W_2 Ep: 1098 | Ep_r: -431\n",
      "W_3 Ep: 1099 | Ep_r: -454\n",
      "W_2 Ep: 1100 | Ep_r: -423\n",
      "W_1 Ep: 1101 | Ep_r: -450\n",
      "W_3 Ep: 1102 | Ep_r: -433\n",
      "W_1 Ep: 1103 | Ep_r: -417\n",
      "W_2 Ep: 1104 | Ep_r: -402\n",
      "W_3 Ep: 1105 | Ep_r: -389\n",
      "W_1 Ep: 1106 | Ep_r: -390\n",
      "W_2 Ep: 1107 | Ep_r: -392\n",
      "W_3 Ep: 1108 | Ep_r: -367\n",
      "W_0 Ep: 1109 | Ep_r: -357\n",
      "W_1 Ep: 1110 | Ep_r: -394\n",
      "W_2 Ep: 1111 | Ep_r: -424\n",
      "W_3 Ep: 1112 | Ep_r: -467\n",
      "W_1 Ep: 1113 | Ep_r: -474\n",
      "W_2 Ep: 1114 | Ep_r: -466\n",
      "W_3 Ep: 1115 | Ep_r: -447\n",
      "W_1 Ep: 1116 | Ep_r: -443\n",
      "W_2 Ep: 1117 | Ep_r: -466\n",
      "W_3 Ep: 1118 | Ep_r: -466\n",
      "W_1 Ep: 1119 | Ep_r: -459\n",
      "W_2 Ep: 1120 | Ep_r: -467\n",
      "W_3 Ep: 1121 | Ep_r: -476\n",
      "W_0 Ep: 1122 | Ep_r: -474\n",
      "W_1 Ep: 1123 | Ep_r: -467\n",
      "W_2 Ep: 1124 | Ep_r: -475\n",
      "W_3 Ep: 1125 | Ep_r: -479\n",
      "W_1 Ep: 1126 | Ep_r: -475\n",
      "W_2 Ep: 1127 | Ep_r: -468\n",
      "W_3 Ep: 1128 | Ep_r: -462\n",
      "W_1 Ep: 1129 | Ep_r: -471\n",
      "W_3 Ep: 1130 | Ep_r: -464\n",
      "W_2 Ep: 1131 | Ep_r: -474\n",
      "W_1 Ep: 1132 | Ep_r: -483\n",
      "W_2 Ep: 1133 | Ep_r: -513\n",
      "W_3 Ep: 1134 | Ep_r: -489\n",
      "W_0 Ep: 1135 | Ep_r: -493\n",
      "W_1 Ep: 1136 | Ep_r: -484\n",
      "W_2 Ep: 1137 | Ep_r: -502\n",
      "W_3 Ep: 1138 | Ep_r: -526\n",
      "W_1 Ep: 1139 | Ep_r: -512\n",
      "W_2 Ep: 1140 | Ep_r: -504\n",
      "W_3 Ep: 1141 | Ep_r: -480\n",
      "W_1 Ep: 1142 | Ep_r: -459\n",
      "W_2 Ep: 1143 | Ep_r: -479\n",
      "W_3 Ep: 1144 | Ep_r: -511\n",
      "W_1 Ep: 1145 | Ep_r: -487\n",
      "W_2 Ep: 1146 | Ep_r: -492\n",
      "W_3 Ep: 1147 | Ep_r: -509\n",
      "W_0 Ep: 1148 | Ep_r: -501\n",
      "W_1 Ep: 1149 | Ep_r: -478\n",
      "W_2 Ep: 1150 | Ep_r: -471\n",
      "W_3 Ep: 1151 | Ep_r: -477\n",
      "W_1 Ep: 1152 | Ep_r: -512\n",
      "W_2 Ep: 1153 | Ep_r: -518\n",
      "W_3 Ep: 1154 | Ep_r: -521\n",
      "W_1 Ep: 1155 | Ep_r: -530\n",
      "W_2 Ep: 1156 | Ep_r: -478\n",
      "W_3 Ep: 1157 | Ep_r: -468\n",
      "W_1 Ep: 1158 | Ep_r: -504\n",
      "W_2 Ep: 1159 | Ep_r: -494\n",
      "W_0 Ep: 1160 | Ep_r: -484\n",
      "W_3 Ep: 1161 | Ep_r: -462\n",
      "W_2 Ep: 1162 | Ep_r: -468\n",
      "W_1 Ep: 1163 | Ep_r: -448\n",
      "W_3 Ep: 1164 | Ep_r: -417\n",
      "W_2 Ep: 1165 | Ep_r: -402\n",
      "W_1 Ep: 1166 | Ep_r: -464\n",
      "W_3 Ep: 1167 | Ep_r: -431\n",
      "W_2 Ep: 1168 | Ep_r: -402\n",
      "W_1 Ep: 1169 | Ep_r: -388\n",
      "W_3 Ep: 1170 | Ep_r: -376\n",
      "W_2 Ep: 1171 | Ep_r: -339\n",
      "W_1 Ep: 1172 | Ep_r: -318\n",
      "W_0 Ep: 1173 | Ep_r: -363\n",
      "W_3 Ep: 1174 | Ep_r: -354\n",
      "W_2 Ep: 1175 | Ep_r: -385\n",
      "W_1 Ep: 1176 | Ep_r: -433\n",
      "W_3 Ep: 1177 | Ep_r: -431\n",
      "W_1 Ep: 1178 | Ep_r: -454\n",
      "W_2 Ep: 1179 | Ep_r: -465\n",
      "W_3 Ep: 1180 | Ep_r: -418\n",
      "W_1 Ep: 1181 | Ep_r: -390\n",
      "W_2 Ep: 1182 | Ep_r: -410\n",
      "W_3 Ep: 1183 | Ep_r: -413\n",
      "W_1 Ep: 1184 | Ep_r: -386\n",
      "W_2 Ep: 1185 | Ep_r: -347\n",
      "W_0 Ep: 1186 | Ep_r: -326\n",
      "W_3 Ep: 1187 | Ep_r: -319\n",
      "W_1 Ep: 1188 | Ep_r: -342\n",
      "W_2 Ep: 1189 | Ep_r: -393\n",
      "W_3 Ep: 1190 | Ep_r: -420\n",
      "W_1 Ep: 1191 | Ep_r: -446\n",
      "W_2 Ep: 1192 | Ep_r: -471\n",
      "W_3 Ep: 1193 | Ep_r: -451\n",
      "W_1 Ep: 1194 | Ep_r: -420\n",
      "W_2 Ep: 1195 | Ep_r: -378\n",
      "W_3 Ep: 1196 | Ep_r: -354\n",
      "W_1 Ep: 1197 | Ep_r: -332\n",
      "W_2 Ep: 1198 | Ep_r: -312\n",
      "W_3 Ep: 1199 | Ep_r: -294\n",
      "W_0 Ep: 1200 | Ep_r: -278\n",
      "W_1 Ep: 1201 | Ep_r: -291\n",
      "W_2 Ep: 1202 | Ep_r: -262\n",
      "W_3 Ep: 1203 | Ep_r: -289\n",
      "W_1 Ep: 1204 | Ep_r: -341\n",
      "W_2 Ep: 1205 | Ep_r: -373\n",
      "W_3 Ep: 1206 | Ep_r: -362\n",
      "W_1 Ep: 1207 | Ep_r: -385\n",
      "W_2 Ep: 1208 | Ep_r: -375\n",
      "W_3 Ep: 1209 | Ep_r: -364\n",
      "W_1 Ep: 1210 | Ep_r: -354\n",
      "W_2 Ep: 1211 | Ep_r: -367\n",
      "W_3 Ep: 1212 | Ep_r: -370\n",
      "W_0 Ep: 1213 | Ep_r: -372\n",
      "W_1 Ep: 1214 | Ep_r: -362\n",
      "W_2 Ep: 1215 | Ep_r: -339\n",
      "W_3 Ep: 1216 | Ep_r: -373\n",
      "W_1 Ep: 1217 | Ep_r: -363\n",
      "W_2 Ep: 1218 | Ep_r: -380\n",
      "W_3 Ep: 1219 | Ep_r: -368\n",
      "W_1 Ep: 1220 | Ep_r: -396\n",
      "W_2 Ep: 1221 | Ep_r: -383\n",
      "W_3 Ep: 1222 | Ep_r: -386\n",
      "W_1 Ep: 1223 | Ep_r: -387\n",
      "W_2 Ep: 1224 | Ep_r: -405\n",
      "W_3 Ep: 1225 | Ep_r: -419\n",
      "W_0 Ep: 1226 | Ep_r: -446\n",
      "W_1 Ep: 1227 | Ep_r: -442\n",
      "W_2 Ep: 1228 | Ep_r: -411\n",
      "W_3 Ep: 1229 | Ep_r: -429\n",
      "W_1 Ep: 1230 | Ep_r: -429\n",
      "W_2 Ep: 1231 | Ep_r: -439\n",
      "W_3 Ep: 1232 | Ep_r: -450\n",
      "W_1 W_2Ep:  Ep: 1233 | Ep_r: -442\n",
      "1233 | Ep_r: -446\n",
      "W_3 Ep: 1235 | Ep_r: -454\n",
      "W_1 Ep: 1236 | Ep_r: -425\n",
      "W_2 Ep: 1237 | Ep_r: -441\n",
      "W_3 Ep: 1238 | Ep_r: -424\n",
      "W_0 Ep: 1239 | Ep_r: -436\n",
      "W_2 Ep: 1240 | Ep_r: -477\n",
      "W_1 Ep: 1241 | Ep_r: -473\n",
      "W_3 Ep: 1242 | Ep_r: -439\n",
      "W_2 Ep: 1243 | Ep_r: -436\n",
      "W_1 Ep: 1244 | Ep_r: -433\n",
      "W_3 Ep: 1245 | Ep_r: -456\n",
      "W_2 Ep: 1246 | Ep_r: -424\n",
      "W_1 Ep: 1247 | Ep_r: -408\n",
      "W_3 Ep: 1248 | Ep_r: -421\n",
      "W_2 Ep: 1249 | Ep_r: -419\n",
      "W_1 Ep: 1250 | Ep_r: -404\n",
      "W_0 Ep: 1251 | Ep_r: -404\n",
      "W_3 Ep: 1252 | Ep_r: -417\n",
      "W_1 W_2 Ep: 1253 | Ep_r: -417Ep: 1253 \n",
      "| Ep_r: -449\n",
      "W_3 Ep: 1255 | Ep_r: -417\n",
      "W_2 Ep: 1256 | Ep_r: -429\n",
      "W_1 Ep: 1257 | Ep_r: -427\n",
      "W_3 Ep: 1258 | Ep_r: -436\n",
      "W_2 Ep: 1259 | Ep_r: -459\n",
      "W_1 Ep: 1260 | Ep_r: -426\n",
      "W_3 Ep: 1261 | Ep_r: -411\n",
      "W_0 Ep: 1262 | Ep_r: -383\n",
      "W_2 Ep: 1263 | Ep_r: -386\n",
      "W_1 Ep: 1264 | Ep_r: -404\n",
      "W_3 Ep: 1265 | Ep_r: -417\n",
      "W_2 Ep: 1266 | Ep_r: -416\n",
      "W_1 Ep: 1267 | Ep_r: -375\n",
      "W_3 Ep: 1268 | Ep_r: -364\n",
      "W_2 Ep: 1269 | Ep_r: -354\n",
      "W_1 Ep: 1270 | Ep_r: -345\n",
      "W_3 Ep: 1271 | Ep_r: -351\n",
      "W_1W_2 Ep: 1272 | Ep_r: -375\n",
      " Ep: 1272 | Ep_r: -372\n",
      "W_0 Ep: 1274 | Ep_r: -351\n",
      "W_3 Ep: 1275 | Ep_r: -330\n",
      "W_2 Ep: 1276 | Ep_r: -376\n",
      "W_1 Ep: 1277 | Ep_r: -405\n",
      "W_3 Ep: 1278 | Ep_r: -425\n",
      "W_1 Ep: 1279 | Ep_r: -422\n",
      "W_2 Ep: 1280 | Ep_r: -422\n",
      "W_3 Ep: 1281 | Ep_r: -419\n",
      "W_2 Ep: 1282 | Ep_r: -434\n",
      "W_1 Ep: 1283 | Ep_r: -417\n",
      "W_3 Ep: 1284 | Ep_r: -415\n",
      "W_0 Ep: 1285 | Ep_r: -429\n",
      "W_2 Ep: 1286 | Ep_r: -426\n",
      "W_1 Ep: 1287 | Ep_r: -410\n",
      "W_3 Ep: 1288 | Ep_r: -410\n",
      "W_2 Ep: 1289 | Ep_r: -423\n",
      "W_1 Ep: 1290 | Ep_r: -421\n",
      "W_3 Ep: 1291 | Ep_r: -393\n",
      "W_2 Ep: 1292 | Ep_r: -380\n",
      "W_1 Ep: 1293 | Ep_r: -371\n",
      "W_3 Ep: 1294 | Ep_r: -374\n",
      "W_2 Ep: 1295 | Ep_r: -377\n",
      "W_1 Ep: 1296 | Ep_r: -379\n",
      "W_3 Ep: 1297 | Ep_r: -355\n",
      "W_2 Ep: 1298 | Ep_r: -376\n",
      "W_0 Ep: 1299 | Ep_r: -398\n",
      "W_1 Ep: 1300 | Ep_r: -425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_3 Ep: 1301 | Ep_r: -422\n",
      "W_2 Ep: 1302 | Ep_r: -421\n",
      "W_1 Ep: 1303 | Ep_r: -406\n",
      "W_3 Ep: 1304 | Ep_r: -392\n",
      "W_2 Ep: 1305 | Ep_r: -384\n",
      "W_1 Ep: 1306 | Ep_r: -373\n",
      "W_3 Ep: 1307 | Ep_r: -395\n",
      "W_2 Ep: 1308 | Ep_r: -442\n",
      "W_1 Ep: 1309 | Ep_r: -451\n",
      "W_3 Ep: 1310 | Ep_r: -433\n",
      "W_0 Ep: 1311 | Ep_r: -445\n",
      "W_2 Ep: 1312 | Ep_r: -468\n",
      "W_1 Ep: 1313 | Ep_r: -461\n",
      "W_3 Ep: 1314 | Ep_r: -471\n",
      "W_2 Ep: 1315 | Ep_r: -475\n",
      "W_1 Ep: 1316 | Ep_r: -481\n",
      "W_3 Ep: 1317 | Ep_r: -472\n",
      "W_2 Ep: 1318 | Ep_r: -477\n",
      "W_1 Ep: 1319 | Ep_r: -469\n",
      "W_3 Ep: 1320 | Ep_r: -463\n",
      "W_2 Ep: 1321 | Ep_r: -430\n",
      "W_1 Ep: 1322 | Ep_r: -400\n",
      "W_3 Ep: 1323 | Ep_r: -392\n",
      "W_2 Ep: 1324 | Ep_r: -422W_0 Ep:\n",
      " 1324 | Ep_r: -434\n",
      "W_1 Ep: 1326 | Ep_r: -446\n",
      "W_3 Ep: 1327 | Ep_r: -473\n",
      "W_2 Ep: 1328 | Ep_r: -495\n",
      "W_1 Ep: 1329 | Ep_r: -473\n",
      "W_3 Ep: 1330 | Ep_r: -466\n",
      "W_2 Ep: 1331 | Ep_r: -446\n",
      "W_1 Ep: 1332 | Ep_r: -472\n",
      "W_3 Ep: 1333 | Ep_r: -464\n",
      "W_2 Ep: 1334 | Ep_r: -446\n",
      "W_1 Ep: 1335 | Ep_r: -442\n",
      "W_3 Ep: 1336 | Ep_r: -411\n",
      "W_0 Ep: 1337 | Ep_r: -409\n",
      "W_2 Ep: 1338 | Ep_r: -381\n",
      "W_1 Ep: 1339 | Ep_r: -383\n",
      "W_3 Ep: 1340 | Ep_r: -358\n",
      "W_2 Ep: 1341 | Ep_r: -336\n",
      "W_1 Ep: 1342 | Ep_r: -329\n",
      "W_3 Ep: 1343 | Ep_r: -357\n",
      "W_2 Ep: 1344 | Ep_r: -393\n",
      "W_1 Ep: 1345 | Ep_r: -381\n",
      "W_3 Ep: 1346 | Ep_r: -357\n",
      "W_2 Ep: 1347 | Ep_r: -336\n",
      "W_1 Ep: 1348 | Ep_r: -328\n",
      "W_0 Ep: 1349 | Ep_r: -337\n",
      "W_2 Ep: 1350 | Ep_r: -354\n",
      "W_3 Ep: 1351 | Ep_r: -367\n",
      "W_1 Ep: 1352 | Ep_r: -358\n",
      "W_3 Ep: 1353 | Ep_r: -405\n",
      "W_2 Ep: 1354 | Ep_r: -443\n",
      "W_1 Ep: 1355 | Ep_r: -453\n",
      "W_3 Ep: W_21356  | Ep_r: -490\n",
      "Ep: 1356 | Ep_r: -469\n",
      "W_1 Ep: 1358 | Ep_r: -477\n",
      "W_2 Ep: 1359 | Ep_r: -497\n",
      "W_3 Ep: 1360 | Ep_r: -488\n",
      "W_1 Ep: 1361 | Ep_r: -494\n",
      "W_2 Ep: 1362 | Ep_r: -485\n",
      "W_0 Ep: 1363 | Ep_r: -464\n",
      "W_3 Ep: 1364 | Ep_r: -489\n",
      "W_1 Ep: 1365 | Ep_r: -455\n",
      "W_2 Ep: 1366 | Ep_r: -450\n",
      "W_3 Ep: 1367 | Ep_r: -489\n",
      "W_1 Ep: 1368 | Ep_r: -528\n",
      "W_2 Ep: 1369 | Ep_r: -529\n",
      "W_3 Ep: 1370 | Ep_r: -530\n",
      "W_1 Ep: 1371 | Ep_r: -518\n",
      "W_2 Ep: 1372 | Ep_r: -536\n",
      "W_3 Ep: 1373 | Ep_r: -538\n",
      "W_1 Ep: 1374 | Ep_r: -511\n",
      "W_2 Ep: 1375 | Ep_r: -487\n",
      "W_0 Ep: 1376 | Ep_r: -512\n",
      "W_3 Ep: 1377 | Ep_r: -501\n",
      "W_1 Ep: 1378 | Ep_r: -490\n",
      "W_2 Ep: 1379 | Ep_r: -495\n",
      "W_3 Ep: 1380 | Ep_r: -486\n",
      "W_1 Ep: 1381 | Ep_r: -521\n",
      "W_2 Ep: 1382 | Ep_r: -496\n",
      "W_3 Ep: 1383 | Ep_r: -500\n",
      "W_1 Ep: 1384 | Ep_r: -527\n",
      "W_2 Ep: 1385 | Ep_r: -544\n",
      "W_0 Ep: 1386 | Ep_r: -517\n",
      "W_3 Ep: 1387 | Ep_r: -506\n",
      "W_1 Ep: 1388 | Ep_r: -468\n",
      "W_2 Ep: 1389 | Ep_r: -448\n",
      "W_3 Ep: 1390 | Ep_r: -459\n",
      "W_1 Ep: 1391 | Ep_r: -467\n",
      "W_2 Ep: 1392 | Ep_r: -420\n",
      "W_3 Ep: 1393 | Ep_r: -430\n",
      "W_1 Ep: 1394 | Ep_r: -457\n",
      "W_2 Ep: 1395 | Ep_r: -439\n",
      "W_3 Ep: 1396 | Ep_r: -409\n",
      "W_1 Ep: 1397 | Ep_r: -451\n",
      "W_0 Ep: 1398 | Ep_r: -407\n",
      "W_2 Ep: 1399 | Ep_r: -407\n",
      "W_3 Ep: 1400 | Ep_r: -433\n",
      "W_1 Ep: 1401 | Ep_r: -485\n",
      "W_2 Ep: 1402 | Ep_r: -496\n",
      "W_3 Ep: 1403 | Ep_r: -446\n",
      "W_1 Ep: 1404 | Ep_r: -479\n",
      "W_2 Ep: 1405 | Ep_r: -486\n",
      "W_3 Ep: 1406 | Ep_r: -451\n",
      "W_1 Ep: 1407 | Ep_r: -419\n",
      "W_0 Ep: 1408 | Ep_r: -403\n",
      "W_2 Ep: 1409 | Ep_r: -418\n",
      "W_3 Ep: 1410 | Ep_r: -430\n",
      "W_1 Ep: 1411 | Ep_r: -461\n",
      "W_3 Ep: 1412 | Ep_r: -428\n",
      "W_2 Ep: 1413 | Ep_r: -399\n",
      "W_1 Ep: 1414 | Ep_r: -373\n",
      "W_3 Ep: 1415 | Ep_r: -403\n",
      "W_2 Ep: 1416 | Ep_r: -429\n",
      "W_1 Ep: 1417 | Ep_r: -452\n",
      "W_2 Ep: 1418 | Ep_r: -421\n",
      "W_3 Ep: 1419 | Ep_r: -419\n",
      "W_0 Ep: 1420 | Ep_r: -428\n",
      "W_1 Ep: 1421 | Ep_r: -412\n",
      "W_3W_2 Ep: 1422  | Ep_r: -453\n",
      "Ep: 1422 | Ep_r: -442\n",
      "W_1 Ep: 1424 | Ep_r: -409\n",
      "W_2 Ep: 1425 | Ep_r: -421\n",
      "W_3 Ep: 1426 | Ep_r: -406\n",
      "W_1 Ep: 1427 | Ep_r: -393\n",
      "W_3 Ep: 1428 | Ep_r: -413\n",
      "W_2 Ep: 1429 | Ep_r: -386\n",
      "W_1 Ep: 1430 | Ep_r: -418\n",
      "W_2 Ep: 1431 | Ep_r: -430\n",
      "W_3 Ep: 1432 | Ep_r: -428\n",
      "W_0 Ep: 1433 | Ep_r: -412\n",
      "W_1 Ep: 1434 | Ep_r: -426\n",
      "W_2 Ep: 1435 | Ep_r: -396\n",
      "W_3 Ep: 1436 | Ep_r: -416\n",
      "W_1 Ep: 1437 | Ep_r: -401\n",
      "W_2 Ep: 1438 | Ep_r: -388\n",
      "W_3 Ep: 1439 | Ep_r: -404\n",
      "W_1 Ep: 1440 | Ep_r: -404\n",
      "W_2 W_3Ep:  Ep:1441  1441| Ep_r: -403 \n",
      "| Ep_r: -404\n",
      "W_1 Ep: 1443 | Ep_r: -376\n",
      "W_0 Ep: 1444 | Ep_r: -365\n",
      "W_2 Ep: 1445 | Ep_r: -355\n",
      "W_3 Ep: 1446 | Ep_r: -375\n",
      "W_1 Ep: 1447 | Ep_r: -378\n",
      "W_2 Ep: 1448 | Ep_r: -368\n",
      "W_3 Ep: 1449 | Ep_r: -402\n",
      "W_1 Ep: 1450 | Ep_r: -417\n",
      "W_3 Ep: 1451 W_2| Ep_r: -415 Ep: 1451 | Ep_r: -413\n",
      "\n",
      "W_1 Ep: 1453 | Ep_r: -442\n",
      "W_3 Ep: 1454 | Ep_r: -411\n",
      "W_2 Ep: 1455 | Ep_r: -412\n",
      "W_0 Ep: 1456 | Ep_r: -451\n",
      "W_1 Ep: 1457 | Ep_r: -433\n",
      "W_2 Ep: 1458 | Ep_r: -429\n",
      "W_3 Ep: 1459 | Ep_r: -413\n",
      "W_1 Ep: 1460 | Ep_r: -399\n",
      "W_3 Ep: 1461 | Ep_r: -413\n",
      "W_2 Ep: 1462 | Ep_r: -372\n",
      "W_1 Ep: 1463 | Ep_r: -362\n",
      "W_3 Ep: 1464 | Ep_r: -366\n",
      "W_2 Ep: 1465 | Ep_r: -386\n",
      "W_1 Ep: 1466 | Ep_r: -415\n",
      "W_0 Ep: 1467 | Ep_r: -387\n",
      "W_3 Ep: 1468 | Ep_r: -388\n",
      "W_2 Ep: 1469 | Ep_r: -350\n",
      "W_1 Ep: 1470 | Ep_r: -355\n",
      "W_3 Ep: 1471 | Ep_r: -360\n",
      "W_2 Ep: 1472 | Ep_r: -378\n",
      "W_1 Ep: 1473 | Ep_r: -394\n",
      "W_3 Ep: 1474 | Ep_r: -424\n",
      "W_2 Ep: 1475 | Ep_r: -423\n",
      "W_1 Ep: 1476 | Ep_r: -408\n",
      "W_0 Ep: 1477 | Ep_r: -407\n",
      "W_3 Ep: 1478 | Ep_r: -394\n",
      "W_2 Ep: 1479 | Ep_r: -381\n",
      "W_1 Ep: 1480 | Ep_r: -386\n",
      "W_3 Ep: 1481 | Ep_r: -374\n",
      "W_2 Ep: 1482 | Ep_r: -376\n",
      "W_1 Ep: 1483 | Ep_r: -365\n",
      "W_3 Ep: 1484 | Ep_r: -396\n",
      "W_2 Ep: 1485 | Ep_r: -416\n",
      "W_1 Ep: 1486 | Ep_r: -401\n",
      "W_0 Ep: 1487 | Ep_r: -403\n",
      "W_3 Ep: 1488 | Ep_r: -435\n",
      "W_2 Ep: 1489 | Ep_r: -406\n",
      "W_1 Ep: 1490 | Ep_r: -417\n",
      "W_3 Ep: 1491 | Ep_r: -403\n",
      "W_2 Ep: 1492 | Ep_r: -363\n",
      "W_1 Ep: 1493 | Ep_r: -404\n",
      "W_3 Ep: 1494 | Ep_r: -377\n",
      "W_2 Ep: 1495 | Ep_r: -340\n",
      "W_1 Ep: 1496 | Ep_r: -333\n",
      "W_3 Ep: 1497 | Ep_r: -300\n",
      "W_2 Ep: 1498 | Ep_r: -296\n",
      "W_0 Ep: 1499 | Ep_r: -280\n",
      "W_1 Ep: 1500 | Ep_r: -265\n",
      "W_3 Ep: 1501 | Ep_r: -265\n",
      "W_2 Ep: 1502 | Ep_r: -239\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEKCAYAAADenhiQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3b391977b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "SESS = tf.Session()\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    OPT_A = tf.train.RMSPropOptimizer(LR_A, name='RMSPropA')\n",
    "    OPT_C = tf.train.RMSPropOptimizer(LR_C, name='RMSPropC')\n",
    "    GLOBAL_AC = ACNet(GLOBAL_NET_SCOPE)  # we only need its params\n",
    "    workers = []\n",
    "    # Create worker\n",
    "    for i in range(N_WORKERS):\n",
    "        i_name = 'W_%i' % i   # worker name\n",
    "        workers.append(Worker(i_name, GLOBAL_AC))\n",
    "\n",
    "COORD = tf.train.Coordinator()\n",
    "SESS.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    if os.path.exists(LOG_DIR):\n",
    "        shutil.rmtree(LOG_DIR)\n",
    "    tf.summary.FileWriter(LOG_DIR, SESS.graph)\n",
    "\n",
    "worker_threads = []\n",
    "for worker in workers:\n",
    "    job = lambda: worker.work()\n",
    "    t = threading.Thread(target=job)\n",
    "    t.start()\n",
    "    worker_threads.append(t)\n",
    "COORD.join(worker_threads)\n",
    "\n",
    "plt.plot(np.arange(len(GLOBAL_RUNNING_R)), GLOBAL_RUNNING_R)\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('Total moving reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(GAME)\n",
    "\n",
    "N_S = env.observation_space.shape[0]\n",
    "N_A = env.action_space.shape[0]\n",
    "A_BOUND = [env.action_space.low, env.action_space.high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#sess.close()\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "GLOBAL_AC = ACNet(GLOBAL_NET_SCOPE)\n",
    "\n",
    "OPT_A = tf.train.RMSPropOptimizer(LR_A, name='RMSPropA')\n",
    "OPT_C = tf.train.RMSPropOptimizer(LR_C, name='RMSPropC')\n",
    "workers = []\n",
    "for i in range(N_WORKERS):\n",
    "    print(i)\n",
    "    i_name = 'W_%i' % i   # worker name\n",
    "    workers.append(ACNet(i_name, GLOBAL_AC))\n",
    "    \n",
    "tf.summary.FileWriter(\"/home/adrian/Schreibtisch/Uni/Data-Innovation-Lab/tensorflowlogs\", sess.graph).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.vstack(buffer_s)\n",
    "actions = np.vstack(buffer_a)\n",
    "rewards = np.vstack(buffer_r)\n",
    "\n",
    "mu, sigma = SESS.run([testA3C.mu, testA3C.sigma], {testA3C.s: states})\n",
    "normal_dist = tf.distributions.Normal(mu, sigma)\n",
    "log_prob = normal_dist.log_prob(actions)\n",
    "values = SESS.run(testA3C.v, {testA3C.s: states})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_multiplyer = [1]\n",
    "for i in range(1,ROLLOUT-1):\n",
    "    gamma_multiplyer = np.append(gamma_multiplyer, GAMMA ** i)\n",
    "gamma_multiplyer = gamma_multiplyer.reshape([9,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_s_ = SESS.run(testA3C.v, {testA3C.s: s_[np.newaxis, :]})[0, 0]\n",
    "buffer_v_target = []\n",
    "for r in rewards[::-1]:    # reverse buffer r\n",
    "    v_s_ = r + GAMMA * v_s_\n",
    "    buffer_v_target.append(v_s_)\n",
    "buffer_v_target.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {\n",
    "    testA3C.s: states,\n",
    "    testA3C.a_his: actions,\n",
    "    testA3C.v_target: buffer_v_target,\n",
    "}\n",
    "testA3C.update_global(feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACNet(object):\n",
    "    def __init__(self, scope, globalAC=None):\n",
    "\n",
    "        if scope == GLOBAL_NET_SCOPE:   # get global network\n",
    "            with tf.variable_scope(scope):\n",
    "                self.s = tf.placeholder(tf.float32, [None, N_S], 'S')\n",
    "                self.a_params, self.c_params = self._build_net(scope)[-2:]\n",
    "        else:   # local net, calculate losses\n",
    "            with tf.variable_scope(scope):\n",
    "                \n",
    "                self.s = tf.placeholder(tf.float32, [None, N_S], 'S')\n",
    "                self.a_his = tf.placeholder(tf.float32, [None, N_A], 'A')\n",
    "                self.v_target = tf.placeholder(tf.float32, [None, 1], 'Vtarget')\n",
    "                self.reward = tf.placeholder(tf.float32, [None,1], 'Reward')\n",
    "                self.batch_size = tf.placeholder(tf.float32, [1,1], 'BatchSize')\n",
    "                                                \n",
    "                mu, sigma, self.v, self.a_params, self.c_params = self._build_net(scope)\n",
    "\n",
    "                td = tf.subtract(self.v_target, self.v, name='TD_error')\n",
    "                with tf.name_scope('c_loss'):\n",
    "                    self.c_loss = tf.reduce_mean(tf.square(td))\n",
    "\n",
    "                with tf.name_scope('wrap_a_out'):\n",
    "                    self.mu =  sigma + 1e-4\n",
    "                    self.sigma = mu * A_BOUND[1]\n",
    "                \n",
    "                self.policy_loss = tf.make_template(\n",
    "                        name_=(scope + '/a_loss'),\n",
    "                        func_=self.compute_loss\n",
    "                )\n",
    "                \n",
    "                with tf.name_scope('prepare_Loss'):\n",
    "                    discount_filter = tf.reshape(GAMMA ** tf.range(float(ROLLOUT)), [-1, 1, 1], name = 'Discount_factors')\n",
    "                    discountedReward = tf.multiply(discount_filter, self.reward, name = 'Discounted_Reward')\n",
    "        \n",
    "                \"\"\"with tf.name_scope('a_loss'):\n",
    "                    log_prob = normal_dist.log_prob(self.a_his)\n",
    "                    exp_v = log_prob * td\n",
    "                    entropy = normal_dist.entropy()  # encourage exploration\n",
    "                    self.exp_v = ENTROPY_BETA * entropy + exp_v\n",
    "                    self.a_loss = tf.reduce_mean(-self.exp_v)\n",
    "                \"\"\"\n",
    "                normal_dist = tf.distributions.Normal(mu, sigma)\n",
    "                \n",
    "                with tf.name_scope('a_loss'):\n",
    "                    #_ , action_loss, _ = self.policy_loss(mu, sigma, self.a_his, td, self.reward, self.v)\n",
    "                    loss_all = self.compute_loss_and_update(self.s, self.a_his, self.reward, self.v, self.mu, self.sigma)\n",
    "                with tf.name_scope('choose_a'):  # use local params to choose action\n",
    "                    self.A = tf.clip_by_value(tf.squeeze(normal_dist.sample(1), axis=0), A_BOUND[0], A_BOUND[1])\n",
    "                with tf.name_scope('local_grad'):\n",
    "                    self.a_grads = tf.gradients(loss_all, self.a_params)\n",
    "                    self.c_grads = tf.gradients(loss_all, self.c_params)\n",
    "\n",
    "            with tf.name_scope('sync'):\n",
    "                with tf.name_scope('pull'):\n",
    "                    self.pull_a_params_op = [l_p.assign(g_p) for l_p, g_p in zip(self.a_params, globalAC.a_params)]\n",
    "                    self.pull_c_params_op = [l_p.assign(g_p) for l_p, g_p in zip(self.c_params, globalAC.c_params)]\n",
    "                with tf.name_scope('push'):\n",
    "                    self.update_a_op = OPT_A.apply_gradients(zip(self.a_grads, globalAC.a_params))\n",
    "                    self.update_c_op = OPT_C.apply_gradients(zip(self.c_grads, globalAC.c_params))\n",
    "\n",
    "    def _build_net(self, scope):\n",
    "        w_init = tf.random_normal_initializer(0., .1)\n",
    "        with tf.variable_scope('actor'):\n",
    "            l_a = tf.layers.dense(self.s, 200, tf.nn.relu6, kernel_initializer=w_init, name='la')\n",
    "            mu = tf.layers.dense(l_a, N_A, tf.nn.tanh, kernel_initializer=w_init, name='mu')\n",
    "            sigma = tf.layers.dense(l_a, N_A, tf.nn.softplus, kernel_initializer=w_init, name='sigma')\n",
    "        with tf.variable_scope('critic'):\n",
    "            l_c = tf.layers.dense(self.s, 100, tf.nn.relu6, kernel_initializer=w_init, name='lc')\n",
    "            v = tf.layers.dense(l_c, 1, kernel_initializer=w_init, name='v')  # state value\n",
    "        a_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')\n",
    "        c_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')\n",
    "        return mu, sigma, v, a_params, c_params\n",
    "\n",
    "    def update_global(self, feed_dict):  # run by a local\n",
    "        \n",
    "        \n",
    "        SESS.run([self.update_a_op, self.update_c_op], feed_dict)  # local grads applies to global net\n",
    "\n",
    "    def pull_global(self):  # run by a local\n",
    "        SESS.run([self.pull_a_params_op, self.pull_c_params_op])\n",
    "\n",
    "    def choose_action(self, s):  # run by a local\n",
    "        self.current_state = s\n",
    "        s = s[np.newaxis, :]\n",
    "        return SESS.run(self.A, {self.s: s})[0]\n",
    "    \n",
    "    def compute_loss(self, mu, sigma, action, td, reward, value):\n",
    "        normal_dist = tf.distributions.Normal(mu, sigma)\n",
    "        log_prob = normal_dist.log_prob(action)\n",
    "        a = tf.reduce_sum(log_prob)\n",
    "        exp_v = log_prob * td\n",
    "        entropy = normal_dist.entropy()  # encourage exploration\n",
    "        exp_v = ENTROPY_BETA * entropy + exp_v\n",
    "        a_loss = tf.reduce_mean(-exp_v)\n",
    "        return exp_v, a_loss, normal_dist\n",
    "    \n",
    "    def compute_loss_and_update(self, states, actions, rewards, mu, sigma, values):\n",
    "        #mu, sigma = SESS.run([self.mu, self.sigma], {self.s: states})\n",
    "        normal_dist = tf.distributions.Normal(mu, sigma)\n",
    "        log_prob = normal_dist.log_prob(actions)\n",
    "        #values = SESS.run(self.v, {self.s: states})\n",
    "        v_start = tf.gather(values, 0)\n",
    "        v_end = tf.gather(values, ROLLOUT-1)\n",
    "        cost_per_instance = -1 * v_start + GAMMA ** ROLLOUT * v_end + tf.reduce_sum(gamma_multiplyer * rewards) - tf.reduce_sum(gamma_multiplyer * (TAU * log_prob))\n",
    "        loss = 0.5 * cost_per_instance ** 2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(object):\n",
    "    def __init__(self, name, globalAC):\n",
    "        self.env = gym.make(GAME).unwrapped\n",
    "        self.name = name\n",
    "        self.AC = ACNet(name, globalAC)\n",
    "\n",
    "    def work(self):\n",
    "        global GLOBAL_RUNNING_R, GLOBAL_EP\n",
    "        total_step = 1\n",
    "        buffer_s, buffer_a, buffer_r = [], [], []\n",
    "        while not COORD.should_stop() and GLOBAL_EP < MAX_GLOBAL_EP:\n",
    "            s = self.env.reset()\n",
    "            ep_r = 0\n",
    "            for ep_t in range(MAX_EP_STEP):\n",
    "                if self.name == 'W_0':\n",
    "                    self.env.render()\n",
    "                # self.agent.act(states=state, deterministic=deterministic)\n",
    "                a = self.AC.choose_action(s)\n",
    "                # state, terminal, step_reward = self.environment.execute(actions=action)\n",
    "                s_, r, done, info = self.env.step(a)\n",
    "                \n",
    "                done = True if ep_t == MAX_EP_STEP - 1 else False\n",
    "                \n",
    "                self.AC.observe(terminal=terminal, reward=reward)\n",
    "                ep_r += r\n",
    "                buffer_s.append(s)\n",
    "                buffer_a.append(a)\n",
    "                buffer_r.append((r+8)/8)    # normalize\n",
    "\n",
    "                if total_step % UPDATE_GLOBAL_ITER == 0 or done:   # update global and assign to local net\n",
    "                    if done:\n",
    "                        v_s_ = 0   # terminal\n",
    "                    else:\n",
    "                        v_s_ = SESS.run(self.AC.v, {self.AC.s: s_[np.newaxis, :]})[0, 0]\n",
    "                    buffer_v_target = []\n",
    "                    for r in buffer_r[::-1]:    # reverse buffer r\n",
    "                        v_s_ = r + GAMMA * v_s_\n",
    "                        buffer_v_target.append(v_s_)\n",
    "                    buffer_v_target.reverse()\n",
    "\n",
    "                    buffer_s, buffer_a, buffer_v_target = np.vstack(buffer_s), np.vstack(buffer_a), np.vstack(buffer_v_target)\n",
    "                    \n",
    "                    \n",
    "                    buffer_r = np.vstack(buffer_r)\n",
    "                    \n",
    "                    batchSize = np.vstack([UPDATE_GLOBAL_ITER])\n",
    "                    \n",
    "                    feed_dict = {\n",
    "                        self.AC.s: buffer_s,\n",
    "                        self.AC.a_his: buffer_a,\n",
    "                        self.AC.v_target: buffer_v_target,\n",
    "                        self.AC.reward: buffer_r,\n",
    "                        self.AC.batch_size: batchSize\n",
    "                    }\n",
    "                    self.AC.update_global(feed_dict)\n",
    "                    buffer_s, buffer_a, buffer_r = [], [], []\n",
    "                    self.AC.pull_global()\n",
    "\n",
    "                s = s_\n",
    "                total_step += 1\n",
    "                if done:\n",
    "                    if len(GLOBAL_RUNNING_R) == 0:  # record running episode reward\n",
    "                        GLOBAL_RUNNING_R.append(ep_r)\n",
    "                    else:\n",
    "                        GLOBAL_RUNNING_R.append(0.9 * GLOBAL_RUNNING_R[-1] + 0.1 * ep_r)\n",
    "                    print(\n",
    "                        self.name,\n",
    "                        \"Ep:\", GLOBAL_EP,\n",
    "                        \"| Ep_r: %i\" % GLOBAL_RUNNING_R[-1],\n",
    "                          )\n",
    "                    GLOBAL_EP += 1\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n",
      "    status, run_metadata)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [9,1] vs. [10,1]\n",
      "\t [[Node: W_1/a_loss/mul_2 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_1/a_loss/mul_2/x, _arg_W_1/Reward_0_2)]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-156-16c5b2a055bb>\", line 23, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-5-88b3bc0277f1>\", line 51, in work\n",
      "    self.AC.update_global(feed_dict)\n",
      "  File \"<ipython-input-155-867c9efb6775>\", line 78, in update_global\n",
      "    SESS.run([self.update_a_op, self.update_c_op], feed_dict)  # local grads applies to global net\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 889, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [9,1] vs. [10,1]\n",
      "\t [[Node: W_1/a_loss/mul_2 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_1/a_loss/mul_2/x, _arg_W_1/Reward_0_2)]]\n",
      "\n",
      "Caused by op 'W_1/a_loss/mul_2', defined at:\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-156-16c5b2a055bb>\", line 11, in <module>\n",
      "    workers.append(Worker(i_name, GLOBAL_AC))\n",
      "  File \"<ipython-input-5-88b3bc0277f1>\", line 5, in __init__\n",
      "    self.AC = ACNet(name, globalAC)\n",
      "  File \"<ipython-input-155-867c9efb6775>\", line 47, in __init__\n",
      "    loss_all = self.compute_loss_and_update(self.s, self.a_his, self.reward, self.v, self.mu, self.sigma)\n",
      "  File \"<ipython-input-155-867c9efb6775>\", line 104, in compute_loss_and_update\n",
      "    cost_per_instance = -1 * v_start + GAMMA ** ROLLOUT * v_end + tf.reduce_sum(gamma_multiplyer * rewards) - tf.reduce_sum(gamma_multiplyer * (TAU * log_prob))\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 910, in r_binary_op_wrapper\n",
      "    return func(x, y, name=name)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 1117, in _mul_dispatch\n",
      "    return gen_math_ops._mul(x, y, name=name)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2726, in _mul\n",
      "    \"Mul\", x=x, y=y, name=name)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "InvalidArgumentError (see above for traceback): Incompatible shapes: [9,1] vs. [10,1]\n",
      "\t [[Node: W_1/a_loss/mul_2 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_1/a_loss/mul_2/x, _arg_W_1/Reward_0_2)]]\n",
      "\n",
      "\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n",
      "    status, run_metadata)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [9,1] vs. [10,1]\n",
      "\t [[Node: W_3/a_loss/mul_2 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_3/a_loss/mul_2/x, _arg_W_3/Reward_0_2)]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-156-16c5b2a055bb>\", line 23, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-5-88b3bc0277f1>\", line 51, in work\n",
      "    self.AC.update_global(feed_dict)\n",
      "  File \"<ipython-input-155-867c9efb6775>\", line 78, in update_global\n",
      "    SESS.run([self.update_a_op, self.update_c_op], feed_dict)  # local grads applies to global net\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 889, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [9,1] vs. [10,1]\n",
      "\t [[Node: W_3/a_loss/mul_2 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_3/a_loss/mul_2/x, _arg_W_3/Reward_0_2)]]\n",
      "\n",
      "Caused by op 'W_3/a_loss/mul_2', defined at:\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-156-16c5b2a055bb>\", line 11, in <module>\n",
      "    workers.append(Worker(i_name, GLOBAL_AC))\n",
      "  File \"<ipython-input-5-88b3bc0277f1>\", line 5, in __init__\n",
      "    self.AC = ACNet(name, globalAC)\n",
      "  File \"<ipython-input-155-867c9efb6775>\", line 47, in __init__\n",
      "    loss_all = self.compute_loss_and_update(self.s, self.a_his, self.reward, self.v, self.mu, self.sigma)\n",
      "  File \"<ipython-input-155-867c9efb6775>\", line 104, in compute_loss_and_update\n",
      "    cost_per_instance = -1 * v_start + GAMMA ** ROLLOUT * v_end + tf.reduce_sum(gamma_multiplyer * rewards) - tf.reduce_sum(gamma_multiplyer * (TAU * log_prob))\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 910, in r_binary_op_wrapper\n",
      "    return func(x, y, name=name)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 1117, in _mul_dispatch\n",
      "    return gen_math_ops._mul(x, y, name=name)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2726, in _mul\n",
      "    \"Mul\", x=x, y=y, name=name)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "InvalidArgumentError (see above for traceback): Incompatible shapes: [9,1] vs. [10,1]\n",
      "\t [[Node: W_3/a_loss/mul_2 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_3/a_loss/mul_2/x, _arg_W_3/Reward_0_2)]]\n",
      "\n",
      "\n",
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n",
      "    status, run_metadata)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [9,1] vs. [10,1]\n",
      "\t [[Node: W_2/a_loss/mul_2 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_2/a_loss/mul_2/x, _arg_W_2/Reward_0_2)]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-156-16c5b2a055bb>\", line 23, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-5-88b3bc0277f1>\", line 51, in work\n",
      "    self.AC.update_global(feed_dict)\n",
      "  File \"<ipython-input-155-867c9efb6775>\", line 78, in update_global\n",
      "    SESS.run([self.update_a_op, self.update_c_op], feed_dict)  # local grads applies to global net\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 889, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [9,1] vs. [10,1]\n",
      "\t [[Node: W_2/a_loss/mul_2 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_2/a_loss/mul_2/x, _arg_W_2/Reward_0_2)]]\n",
      "\n",
      "Caused by op 'W_2/a_loss/mul_2', defined at:\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-156-16c5b2a055bb>\", line 11, in <module>\n",
      "    workers.append(Worker(i_name, GLOBAL_AC))\n",
      "  File \"<ipython-input-5-88b3bc0277f1>\", line 5, in __init__\n",
      "    self.AC = ACNet(name, globalAC)\n",
      "  File \"<ipython-input-155-867c9efb6775>\", line 47, in __init__\n",
      "    loss_all = self.compute_loss_and_update(self.s, self.a_his, self.reward, self.v, self.mu, self.sigma)\n",
      "  File \"<ipython-input-155-867c9efb6775>\", line 104, in compute_loss_and_update\n",
      "    cost_per_instance = -1 * v_start + GAMMA ** ROLLOUT * v_end + tf.reduce_sum(gamma_multiplyer * rewards) - tf.reduce_sum(gamma_multiplyer * (TAU * log_prob))\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 910, in r_binary_op_wrapper\n",
      "    return func(x, y, name=name)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 1117, in _mul_dispatch\n",
      "    return gen_math_ops._mul(x, y, name=name)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2726, in _mul\n",
      "    \"Mul\", x=x, y=y, name=name)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "InvalidArgumentError (see above for traceback): Incompatible shapes: [9,1] vs. [10,1]\n",
      "\t [[Node: W_2/a_loss/mul_2 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_2/a_loss/mul_2/x, _arg_W_2/Reward_0_2)]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n",
      "    status, run_metadata)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [9,1] vs. [10,1]\n",
      "\t [[Node: W_0/a_loss/mul_2 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_0/a_loss/mul_2/x, _arg_W_0/Reward_0_2)]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-156-16c5b2a055bb>\", line 23, in <lambda>\n",
      "    job = lambda: worker.work()\n",
      "  File \"<ipython-input-5-88b3bc0277f1>\", line 51, in work\n",
      "    self.AC.update_global(feed_dict)\n",
      "  File \"<ipython-input-155-867c9efb6775>\", line 78, in update_global\n",
      "    SESS.run([self.update_a_op, self.update_c_op], feed_dict)  # local grads applies to global net\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 889, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [9,1] vs. [10,1]\n",
      "\t [[Node: W_0/a_loss/mul_2 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_0/a_loss/mul_2/x, _arg_W_0/Reward_0_2)]]\n",
      "\n",
      "Caused by op 'W_0/a_loss/mul_2', defined at:\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-156-16c5b2a055bb>\", line 11, in <module>\n",
      "    workers.append(Worker(i_name, GLOBAL_AC))\n",
      "  File \"<ipython-input-5-88b3bc0277f1>\", line 5, in __init__\n",
      "    self.AC = ACNet(name, globalAC)\n",
      "  File \"<ipython-input-155-867c9efb6775>\", line 47, in __init__\n",
      "    loss_all = self.compute_loss_and_update(self.s, self.a_his, self.reward, self.v, self.mu, self.sigma)\n",
      "  File \"<ipython-input-155-867c9efb6775>\", line 104, in compute_loss_and_update\n",
      "    cost_per_instance = -1 * v_start + GAMMA ** ROLLOUT * v_end + tf.reduce_sum(gamma_multiplyer * rewards) - tf.reduce_sum(gamma_multiplyer * (TAU * log_prob))\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 910, in r_binary_op_wrapper\n",
      "    return func(x, y, name=name)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 1117, in _mul_dispatch\n",
      "    return gen_math_ops._mul(x, y, name=name)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2726, in _mul\n",
      "    \"Mul\", x=x, y=y, name=name)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "InvalidArgumentError (see above for traceback): Incompatible shapes: [9,1] vs. [10,1]\n",
      "\t [[Node: W_0/a_loss/mul_2 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](W_0/a_loss/mul_2/x, _arg_W_0/Reward_0_2)]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6430ae71d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SESS = tf.Session()\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    OPT_A = tf.train.RMSPropOptimizer(LR_A, name='RMSPropA')\n",
    "    OPT_C = tf.train.RMSPropOptimizer(LR_C, name='RMSPropC')\n",
    "    GLOBAL_AC = ACNet(GLOBAL_NET_SCOPE)  # we only need its params\n",
    "    workers = []\n",
    "    # Create worker\n",
    "    for i in range(N_WORKERS):\n",
    "        i_name = 'W_%i' % i   # worker name\n",
    "        workers.append(Worker(i_name, GLOBAL_AC))\n",
    "\n",
    "COORD = tf.train.Coordinator()\n",
    "SESS.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    if os.path.exists(LOG_DIR):\n",
    "        shutil.rmtree(LOG_DIR)\n",
    "    tf.summary.FileWriter(LOG_DIR, SESS.graph)\n",
    "\n",
    "worker_threads = []\n",
    "for worker in workers:\n",
    "    job = lambda: worker.work()\n",
    "    t = threading.Thread(target=job)\n",
    "    t.start()\n",
    "    worker_threads.append(t)\n",
    "COORD.join(worker_threads)\n",
    "\n",
    "plt.plot(np.arange(len(GLOBAL_RUNNING_R)), GLOBAL_RUNNING_R)\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('Total moving reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
