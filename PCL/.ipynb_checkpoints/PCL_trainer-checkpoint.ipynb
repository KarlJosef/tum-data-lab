{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *** Unified PCL Trainer for pendulum ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_wrapper\n",
    "import env_spec\n",
    "import objective_PCL\n",
    "import policy\n",
    "import baseline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gym_wrapper' from '/home/adrian/PycharmProjects/PCL/gym_wrapper.py'>"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(policy)\n",
    "imp.reload(baseline)\n",
    "imp.reload(env_spec)\n",
    "imp.reload(gym_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_str = 'Pendulum-v0'\n",
    "#env_str = 'HalfCheetah-v1'\n",
    "env_gym = gym_wrapper.GymWrapper(env_str,\n",
    "                             distinct= 1,\n",
    "                             count= 1)\n",
    "\n",
    "env_spec_gym =  env_spec.EnvSpec(env_gym.get_one())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup required algorithm parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = '/home/adrian/Schreibtisch/Uni/Data-Innovation-Lab/tensorflowlogs'\n",
    "\n",
    "batch_size_episodes = 20\n",
    "replay_batch_size_episodes = None #--> replay batch size; defaults to batch_size\n",
    "\n",
    "max_steps_episodes = 100\n",
    "total_number_of_steps = 1000\n",
    "critic_weight = 1.0\n",
    "policy_weight = 1.0\n",
    "learning_rate = 0.01\n",
    "# Clip_adv (Advantage) is required in objective class\n",
    "clip_adv = 0.0 # --> Clip advantages at this value --> Leave as 0 to not clip at all\n",
    "# Clip norm is required in objective class to clip the gradients\n",
    "clip_norm = 5.0\n",
    "# Entropy regularization paramter tau\n",
    "tau = 0.1 # --> If using decaying tau, this is the final value\n",
    "tau_decy = None # --> decay tau by this much every 100 steps\n",
    "tau_start = 0.1 # --> start tau at this value\n",
    "gamma = 1.0 # --> Discount factor\n",
    "rollout = 10 # --> Rollout length for PCL objective\n",
    "# If we use unified episodes we need to ensure our batch_size_episodes is 1\n",
    "unify_episodes = False #--> Make sure replay buffer holds entire episodes, even across distinct sampling steps\n",
    "batch_by_steps = False #--> ensure each training batch has batch_size * max_step steps'\n",
    "\n",
    "# Neural network settings now\n",
    "input_prev_actions = True #--> Required for unified PCL since Q(a.s) is modeled\n",
    "recurrent = True #--> Indicate that we are going to use a recurrent policy (Q function approximator)\n",
    "input_time_step = False #--> Indicator if the current time step scould also be an input to the model\n",
    "internal_dim = 254 #--> Internal RNN dimension \n",
    "fixed_std = False #--> fix the std in Gaussian distributions\n",
    "# If fixed we obtain the following std\n",
    "# log_std = tf.get_variable('std%d' % i, [1, sampling_dim // 2])\n",
    "\n",
    "# Settings related to value function if considered seperatly\n",
    "value_hidden_layers = 0 #--> number of hidden layers in value estimate\n",
    "\n",
    "# Settings related to the replay buffer\n",
    "replay_buffer_size = 5000 \n",
    "replay_buffer_alpha = 0.5 #--> replay buffer alpha param\n",
    "replay_buffer_freq = 0 #--> replay buffer frequency (only supports -1/0/1)'\n",
    "eviction = 'rand' #--> How to evict from replay buffer: rand/rank/fifo\n",
    "prioritize_by = 'rewards' #--> Prioritize replay buffer by \"rewards\" or \"step\"\n",
    "\n",
    "# The following are only required for Trust-PCL\n",
    "eps_lambda = 0.0 #--> start tau at this value\n",
    "update_eps_lambda = False #--> Update lambda automatically based on last 100 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup PCL-Objective object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Recurrent Policy - In the unified approach the output is also used as foundation to compute the values\n",
    "* Try recurrent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine unified baseline:\n",
    "* In case of a seperate network one needs to clarify the function of the \"input_policy_state\" paramter\n",
    "* If false the \"input\" looks like the following \"obs, action, time_step\"\n",
    "* If true the \"input\" looks like \"internal_policy_states, time_step\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "objective_pendulum = objective_PCL.Objective(learning_rate = learning_rate,\n",
    "                                     clip_norm = clip_norm,\n",
    "                                     policy_weight = policy_weight,\n",
    "                                     critic_weight = critic_weight,\n",
    "                                     tau = tau, \n",
    "                                     gamma = gamma,\n",
    "                                     rollout = rollout,\n",
    "                                     eps_lambda = eps_lambda,\n",
    "                                     clip_adv = clip_adv)\n",
    "\n",
    "policy_unified_pendulum = policy.Policy(env_spec_gym,\n",
    "                                       internal_dim,\n",
    "                                       recurrent = recurrent,\n",
    "                                       input_prev_actions = input_prev_actions)\n",
    "\n",
    "\n",
    "baseline_unified_pendulum = baseline.UnifiedBaseline(env_spec_gym,\n",
    "                                                    internal_dim,\n",
    "                                                    input_prev_actions=input_prev_actions,\n",
    "                                                    input_time_step=input_time_step,\n",
    "                                                    input_policy_state=recurrent,  # may want to change this\n",
    "                                                    n_hidden_layers=value_hidden_layers,\n",
    "                                                    hidden_dim=internal_dim,\n",
    "                                                    tau=tau,\n",
    "                                                    eps_lambda = eps_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the unified case we do not need a value function optimizer \"get_value_opt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it es time to setup the tensorflow graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with setup of placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary placeholder\n",
    "\n",
    "avg_episode_reward = tf.placeholder(\n",
    "        tf.float32, [], 'avg_episode_reward')\n",
    "\n",
    "# sampling placeholders\n",
    "\n",
    "internal_state = tf.placeholder(tf.float32,\n",
    "                                    [None, policy_unified_pendulum.rnn_state_dim],\n",
    "                                    'internal_state')\n",
    "\n",
    "# One episode of observations (Time_Steps, Observation dimension)\n",
    "single_observation = []\n",
    "for i, (obs_dim, obs_type) in enumerate(zip(env_spec_gym.obs_dims, env_spec_gym.obs_types)):\n",
    "    if env_spec_gym.is_discrete(obs_type):\n",
    "        single_observation.append(\n",
    "            tf.placeholder(tf.int32, [None], 'obs%d' % i))\n",
    "    elif env_spec_gym.is_box(obs_type):\n",
    "        single_observation.append(\n",
    "            tf.placeholder(tf.float32, [None, obs_dim], 'obs%d' % i))\n",
    "    else:\n",
    "        assert False\n",
    "        \n",
    "# One episode of actions (Time_steps, action dimension)        \n",
    "single_action = []\n",
    "for i, (action_dim, action_type) in enumerate(zip(env_spec_gym.act_dims, env_spec_gym.act_types)):\n",
    "    if env_spec_gym.is_discrete(action_type):\n",
    "        single_action.append(\n",
    "            tf.placeholder(tf.int32, [None], 'act%d' % i))\n",
    "    elif env_spec_gym.is_box(action_type):\n",
    "        single_action.append(\n",
    "            tf.placeholder(tf.float32, [None, action_dim], 'act%d' % i))\n",
    "    else:\n",
    "        assert False\n",
    "        \n",
    "# training placeholders\n",
    "\n",
    "# Observations batch size many episodes of time length [batch size, time length, observation dim]\n",
    "observations = []\n",
    "for i, (obs_dim, obs_type) in enumerate(zip(env_spec_gym.obs_dims, env_spec_gym.obs_types)):\n",
    "    if env_spec_gym.is_discrete(obs_type):\n",
    "        observations.append(\n",
    "            tf.placeholder(tf.int32, [None, None], 'all_obs%d' % i))\n",
    "    else:\n",
    "        observations.append(\n",
    "            tf.placeholder(tf.float32, [None, None, obs_dim], 'all_obs%d' % i))\n",
    "        \n",
    "# Actions batch size many episodes of time length [batch size, time length, action dim]        \n",
    "actions = []\n",
    "for i, (action_dim, action_type) in enumerate(zip(env_spec_gym.act_dims, env_spec_gym.act_types)):\n",
    "    if env_spec_gym.is_discrete(action_type):\n",
    "        actions.append(\n",
    "            tf.placeholder(tf.int32, [None, None], 'all_act%d' % i))\n",
    "    if env_spec_gym.is_box(action_type):\n",
    "        actions.append(\n",
    "            tf.placeholder(tf.float32, [None, None, action_dim],\n",
    "                       'all_act%d' % i))\n",
    "        \n",
    "# Rewards of Batch Size many episodes of time length [batch size, time length]\n",
    "rewards = tf.placeholder(tf.float32, [None, None], 'rewards')\n",
    "# Indicator if episode has terminated \n",
    "terminated = tf.placeholder(tf.float32, [None], 'terminated')\n",
    "# Batch Size many episodes of time length indicators if episode has ended\n",
    "pads = tf.placeholder(tf.float32, [None, None], 'pads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7af6b34828>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7af6b34828>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "tf.summary.scalar('avg_episode_reward', avg_episode_reward)\n",
    "\n",
    "with tf.variable_scope('model', reuse=None):\n",
    "    # policy network\n",
    "    with tf.variable_scope('policy_net'):\n",
    "        # ,entropies, self_kls)\n",
    "        (policy_internal_states, logits, log_probs) = \\\n",
    "                policy_unified_pendulum.multi_step(observations,\n",
    "                                                internal_state,\n",
    "                                                actions)\n",
    "        out_log_probs = sum(log_probs)\n",
    "\n",
    "\"\"\"\n",
    "       # value network\n",
    "    with tf.variable_scope('value_net'):\n",
    "        (values,\n",
    "         regression_input,\n",
    "         regression_weight) = baseline_unified_pendulum.get_values(\n",
    "            observations, actions,\n",
    "            policy_internal_states, logits)\n",
    "\n",
    "    \n",
    "      # evaluate objective\n",
    "    (loss, raw_loss, regression_target,\n",
    "     gradient_ops, summary) = objective_pendulum.get(\n",
    "      rewards, pads,\n",
    "      values[:-1, :],\n",
    "      values[-1, :] * (1 - terminated),\n",
    "      log_probs)\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "tf.summary.FileWriter(\"/home/adrian/Schreibtisch/Uni/Data-Innovation-Lab/tensorflowlogs\", sess.graph).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7af99e32e8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7af99e32e8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7ad6f38278>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f7ad6f38278>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    " # we re-use variables for the sampling operations\n",
    "with tf.variable_scope('model', reuse=True):\n",
    "    scope = 'policy_net'\n",
    "    with tf.variable_scope(scope):\n",
    "        next_internal_state, sampled_actions = \\\n",
    "            policy_unified_pendulum.sample_step(single_observation,\n",
    "                                internal_state,\n",
    "                                single_action)\n",
    "        greedy_next_internal_state, greedy_sampled_actions = \\\n",
    "            policy_unified_pendulum.sample_step(single_observation,\n",
    "                               internal_state,\n",
    "                               single_action,\n",
    "                                greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original sample episodes code from the paper code 'pcl_rl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_internal_state():\n",
    "    return np.zeros(policy_unified_pendulum.rnn_state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_obs = env_spec_gym.initial_obs(1)\n",
    "last_act = env_spec_gym.initial_act(len(env_gym))\n",
    "last_pad = np.zeros(len(env_gym))\n",
    "\n",
    "start_episode = np.array([True] * len(env_gym))\n",
    "step_count = np.array([0] * len(env_gym))\n",
    "episode_running_rewards = np.zeros(len(env_gym))\n",
    "episode_running_lengths = np.zeros(len(env_gym))\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "total_rewards = []\n",
    "\n",
    "cutoff_agent = 1\n",
    "max_step = 1000\n",
    "\n",
    "unify_episodes = False\n",
    "\n",
    "all_obs_global = []\n",
    "all_act_global = []\n",
    "all_pad_global = []\n",
    "rewards_global = []\n",
    "\n",
    "internal_state_global = np.array([initial_internal_state()] * len(env_gym))\n",
    "\n",
    "start_id_global = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "(initial_state_, observations_, actions_, rewards_, terminated_, pads_) = sample_episodes_pcl(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 254)\n",
      "(1, 201, 5, 3)\n",
      "(1, 201, 5, 1)\n",
      "(200, 5)\n",
      "(5,)\n",
      "(200, 5)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(initial_state_))\n",
    "print(np.shape(observations_))\n",
    "print(np.shape(actions_))\n",
    "print(np.shape(rewards_))\n",
    "print(np.shape(terminated_))\n",
    "print(np.shape(pads_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = np.shape(initial_state_)[0]\n",
    "initial_actions = [act[0] for act in actions_]\n",
    "all_actions = [tf.concat([act[1:], act[0:1]], 0)\n",
    "                   for act in actions_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given those observations we can feed them now to the model to obtain the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "ConcatOp : Dimensions of inputs should match: shape[0] = [5,127] vs. shape[1] = [1,127]\n\t [[Node: model/policy_net/scan/while/policy_net/output_projection_wrapper/output_projection_wrapper/lstm_cell/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/policy_net/scan/while/add_1, model/policy_net/scan/while/policy_net/output_projection_wrapper/output_projection_wrapper/lstm_cell/Slice_1, model/policy_net/scan/while/concat/axis)]]\n\nCaused by op 'model/policy_net/scan/while/policy_net/output_projection_wrapper/output_projection_wrapper/lstm_cell/concat', defined at:\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-881-63b9fa6ec5ff>\", line 9, in <module>\n    actions)\n  File \"/home/adrian/PycharmProjects/PCL/policy.py\", line 307, in multi_step\n    batch_size, initial_state, initial_actions))\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/functional_ops.py\", line 584, in scan\n    back_prop=back_prop, swap_memory=swap_memory)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2816, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2640, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2590, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/functional_ops.py\", line 574, in compute\n    a_out = fn(packed_a, packed_elems)\n  File \"/home/adrian/PycharmProjects/PCL/policy.py\", line 275, in single_step\n    obs, prev_internal_state, prev_actions)\n  File \"/home/adrian/PycharmProjects/PCL/policy.py\", line 104, in core\n    output, next_state = cell(cell_input, prev_internal_state)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 575, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell.py\", line 230, in call\n    output, res_state = self._cell(inputs, state)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 575, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 611, in call\n    lstm_matrix = self._linear1([inputs, m_prev])\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1189, in __call__\n    res = math_ops.matmul(array_ops.concat(args, 1), self._weights)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1099, in concat\n    return gen_array_ops._concat_v2(values=values, axis=axis, name=name)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 706, in _concat_v2\n    \"ConcatV2\", values=values, axis=axis, name=name)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): ConcatOp : Dimensions of inputs should match: shape[0] = [5,127] vs. shape[1] = [1,127]\n\t [[Node: model/policy_net/scan/while/policy_net/output_projection_wrapper/output_projection_wrapper/lstm_cell/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/policy_net/scan/while/add_1, model/policy_net/scan/while/policy_net/output_projection_wrapper/output_projection_wrapper/lstm_cell/Slice_1, model/policy_net/scan/while/concat/axis)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: ConcatOp : Dimensions of inputs should match: shape[0] = [5,127] vs. shape[1] = [1,127]\n\t [[Node: model/policy_net/scan/while/policy_net/output_projection_wrapper/output_projection_wrapper/lstm_cell/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/policy_net/scan/while/add_1, model/policy_net/scan/while/policy_net/output_projection_wrapper/output_projection_wrapper/lstm_cell/Slice_1, model/policy_net/scan/while/concat/axis)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-888-d7844894c8f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs_place\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0minternals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: ConcatOp : Dimensions of inputs should match: shape[0] = [5,127] vs. shape[1] = [1,127]\n\t [[Node: model/policy_net/scan/while/policy_net/output_projection_wrapper/output_projection_wrapper/lstm_cell/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/policy_net/scan/while/add_1, model/policy_net/scan/while/policy_net/output_projection_wrapper/output_projection_wrapper/lstm_cell/Slice_1, model/policy_net/scan/while/concat/axis)]]\n\nCaused by op 'model/policy_net/scan/while/policy_net/output_projection_wrapper/output_projection_wrapper/lstm_cell/concat', defined at:\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-881-63b9fa6ec5ff>\", line 9, in <module>\n    actions)\n  File \"/home/adrian/PycharmProjects/PCL/policy.py\", line 307, in multi_step\n    batch_size, initial_state, initial_actions))\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/functional_ops.py\", line 584, in scan\n    back_prop=back_prop, swap_memory=swap_memory)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2816, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2640, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2590, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/functional_ops.py\", line 574, in compute\n    a_out = fn(packed_a, packed_elems)\n  File \"/home/adrian/PycharmProjects/PCL/policy.py\", line 275, in single_step\n    obs, prev_internal_state, prev_actions)\n  File \"/home/adrian/PycharmProjects/PCL/policy.py\", line 104, in core\n    output, next_state = cell(cell_input, prev_internal_state)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 575, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell.py\", line 230, in call\n    output, res_state = self._cell(inputs, state)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 575, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 611, in call\n    lstm_matrix = self._linear1([inputs, m_prev])\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1189, in __call__\n    res = math_ops.matmul(array_ops.concat(args, 1), self._weights)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1099, in concat\n    return gen_array_ops._concat_v2(values=values, axis=axis, name=name)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 706, in _concat_v2\n    \"ConcatV2\", values=values, axis=axis, name=name)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/adrian/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): ConcatOp : Dimensions of inputs should match: shape[0] = [5,127] vs. shape[1] = [1,127]\n\t [[Node: model/policy_net/scan/while/policy_net/output_projection_wrapper/output_projection_wrapper/lstm_cell/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/policy_net/scan/while/add_1, model/policy_net/scan/while/policy_net/output_projection_wrapper/output_projection_wrapper/lstm_cell/Slice_1, model/policy_net/scan/while/concat/axis)]]\n"
     ]
    }
   ],
   "source": [
    "outputs = [policy_internal_states, logits, log_probs]\n",
    "\n",
    "feed_dict = {internal_state: internal_state_}\n",
    "for action_place, action in zip(actions, actions_):\n",
    "    feed_dict[action_place] = action\n",
    "for obs_place, obs in zip(observations, observations_):\n",
    "    feed_dict[obs_place] = obs\n",
    "\n",
    "internals, logits, log_probs = sess.run(outputs, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_batched_episodes_pcl(initial_state_, observations_, actions_, rewards_, terminated_, pads_):\n",
    "    \"\"\"Convert time-major batch of episodes to batch-major list of episodes.\"\"\"\n",
    "\n",
    "    rewards_ = np.array(rewards_)\n",
    "    pads_ = np.array(pads_)\n",
    "    observations_ = [np.array(obs) for obs in observations_]\n",
    "    actions_ = [np.array(act) for act in actions_]\n",
    "\n",
    "    total_rewards_ = np.sum(rewards_ * (1 - pads_), axis=0)\n",
    "    total_length_ = np.sum(1 - pads_, axis=0).astype('int32')\n",
    "\n",
    "    episodes_ = []\n",
    "    num_episodes_ = rewards_.shape[1]\n",
    "    for i in range(num_episodes_):\n",
    "        length = total_length_[i]\n",
    "        ep_initial = initial_state_[i]\n",
    "        ep_obs = [obs[:length, i, ...] for obs in observations_]\n",
    "        ep_act = [act[:length + 1, i, ...] for act in actions_]\n",
    "        ep_rewards = rewards_[:length, i]\n",
    "        \n",
    "        episodes_.append(\n",
    "          [ep_initial, ep_obs, ep_act, ep_rewards, terminated_[i]])\n",
    "    \n",
    "    return episodes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episodes_pcl(sess):\n",
    "    \"\"\"Sample steps from the environment until we have enough for a batch.\"\"\"\n",
    "\n",
    "    # define global variables\n",
    "    global max_step, env_gym, start_id, total_rewards, episode_running_rewards, episode_running_lengths\n",
    "    global episode_running_rewards, episode_running_lengths, start_episode, cutoff_agent, step_count\n",
    "    global episode_rewards, episode_lengths\n",
    "    \n",
    "    # check if last batch ended with episode that was not terminated\n",
    "    #if unify_episodes:\n",
    "    #    all_new_ep = self.start_episode[0]\n",
    "\n",
    "    # sample episodes until we either have enough episodes or enough steps\n",
    "    episodes = []\n",
    "    total_steps = 0\n",
    "    while total_steps < max_step * len(env_gym):\n",
    "        (initial_state_,\n",
    "        observations, actions, rewards,\n",
    "        pads) = _sample_episodes_pcl(sess)\n",
    "\n",
    "        observations = zip(*observations)\n",
    "        actions = zip(*actions)\n",
    "\n",
    "        terminated = np.array(env_gym.dones)\n",
    "\n",
    "        total_rewards = np.sum(np.array(rewards[start_id_global:]) *\n",
    "                                  (1 - np.array(pads[start_id_global:])), axis=0)\n",
    "        episode_running_rewards *= 1 - start_episode\n",
    "        episode_running_lengths *= 1 - start_episode\n",
    "        episode_running_rewards += total_rewards\n",
    "        episode_running_lengths += np.sum(1 - np.array(pads[start_id_global:]), axis=0)\n",
    "\n",
    "        episodes.extend(convert_from_batched_episodes_pcl(\n",
    "          initial_state_, observations, actions, rewards,\n",
    "          terminated, pads))\n",
    "        total_steps += np.sum(1 - np.array(pads))\n",
    "\n",
    "        # set next starting episodes\n",
    "        start_episode = np.logical_or(terminated,\n",
    "                                         step_count >= cutoff_agent)\n",
    "        episode_rewards = episode_running_rewards[start_episode].tolist()\n",
    "        episode_rewards.extend(episode_rewards)\n",
    "        episode_lengths.extend(episode_running_lengths[start_episode].tolist())\n",
    "        # ToDo: Check why 100\n",
    "        episode_rewards = episode_rewards[-100:]\n",
    "        episode_lengths = episode_lengths[-100:]\n",
    "\n",
    "        \"\"\"\n",
    "        if (self.save_trajectories_file is not None and\n",
    "          (self.best_batch_rewards is None or\n",
    "           np.mean(self.total_rewards) > self.best_batch_rewards)):\n",
    "        self.best_batch_rewards = np.mean(self.total_rewards)\n",
    "        my_episodes = self.convert_from_batched_episodes(\n",
    "          initial_state, observations, actions, rewards,\n",
    "          terminated, pads)\n",
    "        with gfile.GFile(self.save_trajectories_file, 'w') as f:\n",
    "            pickle.dump(my_episodes, f)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if not self.batch_by_steps:\n",
    "        return (initial_state,\n",
    "                observations, actions, rewards,\n",
    "                terminated, pads)        \n",
    "        \"\"\"\n",
    "        \n",
    "    return convert_to_batched_episodes(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_episodes_pcl(sess, greedy=False):\n",
    "    \"\"\"Sample episodes from environment using model.\"\"\"\n",
    "    \n",
    "    # Define global variables\n",
    "    global start_episode, env_gym, step_count, internal_state, last_obs, last_act, last_pad, env_spec_gym, start_id\n",
    "    global internal_state_global, all_obs_global, all_act_global, all_pad_global, rewards_global, last_act\n",
    "    global last_obs, max_step\n",
    "    \n",
    "    # reset environments as necessary\n",
    "    obs_after_reset = env_gym.reset_if(start_episode)\n",
    "\n",
    "    for i, obs in enumerate(obs_after_reset):\n",
    "        if obs is not None:\n",
    "            step_count[i] = 0\n",
    "            internal_state_global[i] = initial_internal_state()\n",
    "            for j in range(len(env_spec_gym.obs_dims)):\n",
    "                last_obs[j][i] = obs[j]\n",
    "            for j in range(len(env_spec_gym.act_dims)):\n",
    "                last_act[j][i] = -1\n",
    "            last_pad[i] = 0\n",
    "\n",
    "    # maintain episode as a single unit if the last sampling\n",
    "    # batch ended before the episode was terminated\n",
    "    \"\"\"\n",
    "    if unify_episodes:\n",
    "        assert len(obs_after_reset) == 1\n",
    "        new_ep = obs_after_reset[0] is not None\n",
    "    else:\n",
    "        new_ep = True\n",
    "    \"\"\"\n",
    "    new_ep = True\n",
    "    \n",
    "    # ToDo: Make start_id global\n",
    "    start_id_global = 0 if new_ep else len(all_obs_global[:])\n",
    "\n",
    "    initial_state = internal_state_global\n",
    "    all_obs = [] if new_ep else all_obs_global[:]\n",
    "    all_act = ([last_act] if new_ep else all_act_global[:])\n",
    "    all_pad = [] if new_ep else all_pad_global[:]\n",
    "    rewards = [] if new_ep else rewards_global[:]\n",
    "\n",
    "    # start stepping in the environments\n",
    "    step = 0\n",
    "    while not env.all_done():\n",
    "        step_count += 1 - np.array(env.dones)\n",
    "\n",
    "        next_internal_state, sampled_actions = sample_step_pcl(\n",
    "          sess, last_obs, internal_state_global, last_act,\n",
    "          greedy=greedy)\n",
    "\n",
    "        env_actions = env_spec_gym.convert_actions_to_env(sampled_actions)\n",
    "        next_obs, reward, next_dones, _ = env_gym.step(env_actions)\n",
    "\n",
    "        all_obs.append(last_obs)\n",
    "        all_act.append(sampled_actions)\n",
    "        all_pad.append(last_pad)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        internal_state_global = next_internal_state\n",
    "        last_obs = next_obs\n",
    "        last_act = sampled_actions\n",
    "        last_pad = np.array(next_dones).astype('float32')\n",
    "\n",
    "        step += 1\n",
    "        if max_step and step >= max_step:\n",
    "            break\n",
    "\n",
    "    all_obs_global = all_obs[:]\n",
    "    all_act_global = all_act[:]\n",
    "    all_pad_global = all_pad[:]\n",
    "    rewards_global = rewards[:]\n",
    "\n",
    "    # append final observation\n",
    "    all_obs_global.append(last_obs)\n",
    "\n",
    "    return initial_state, all_obs, all_act, rewards, all_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_step_pcl(sess, single_observation_, internal_state_, single_action_, greedy=False):\n",
    "    \"\"\"Sample batch of steps from policy.\"\"\"\n",
    "    global greedy_next_internal_state, greedy_sampled_actions, next_internal_state, sampled_actions\n",
    "    global internal_state, single_action, single_observation\n",
    "    \n",
    "    if greedy:\n",
    "        outputs = [greedy_next_internal_state, greedy_sampled_actions]\n",
    "    else:\n",
    "        outputs = [next_internal_state, sampled_actions]\n",
    "\n",
    "    feed_dict = {internal_state: internal_state_}\n",
    "    for action_place, action in zip(single_action, single_action_):\n",
    "        feed_dict[action_place] = action\n",
    "    for obs_place, obs in zip(single_observation, single_observation_):\n",
    "        feed_dict[obs_place] = obs\n",
    "\n",
    "    return sess.run(outputs, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_batched_episodes_pcl(episodes_, max_length=None):\n",
    "    \"\"\"Convert batch-major list of episodes to time-major batch of episodes.\"\"\"\n",
    "    lengths = [len(ep[-2]) for ep in episodes_]\n",
    "    max_length = max_length or max(lengths)\n",
    "\n",
    "    new_episodes = []\n",
    "    for ep, length in zip(episodes_, lengths):\n",
    "        initial_, observations_, actions_, rewards_, terminated_ = ep\n",
    "        observations_ = [np.resize(obs, [max_length + 1] + list(obs.shape)[1:])\n",
    "                      for obs in observations_]\n",
    "        actions = [np.resize(act, [max_length + 1] + list(act.shape)[1:])\n",
    "                     for act in actions_]\n",
    "        pads_ = np.array([0] * length + [1] * (max_length - length))\n",
    "        rewards_ = np.resize(rewards_, [max_length]) * (1 - pads_)\n",
    "        new_episodes.append([initial_, observations_, actions_, rewards_,\n",
    "                           terminated_, pads_])\n",
    "\n",
    "    (initial_, observations_, actions_, rewards_,\n",
    "     terminated_, pads_) = zip(*new_episodes)\n",
    "    observations_ = [np.swapaxes(obs, 0, 1)\n",
    "                    for obs in zip(*observations_)]\n",
    "    actions_ = [np.swapaxes(act, 0, 1)\n",
    "               for act in zip(*actions_)]\n",
    "    rewards_ = np.transpose(rewards_)\n",
    "    pads_ = np.transpose(pads_)\n",
    "\n",
    "    return (initial_, observations_, actions_, rewards_, terminated_, pads_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
