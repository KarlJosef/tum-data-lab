{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import controller\n",
    "import model\n",
    "import policy\n",
    "import baseline\n",
    "from objective import PCL\n",
    "import optimizers\n",
    "import replay_buffer\n",
    "import expert_paths\n",
    "#import gym_wrapper\n",
    "import env_spec\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(env_str):\n",
    "  return gym.make(env_str)\n",
    "\n",
    "\n",
    "class GymWrapper(object):\n",
    "\n",
    "  def __init__(self, env_str, distinct=1, count=1, seeds=None):\n",
    "    self.distinct = distinct\n",
    "    self.count = count\n",
    "    self.total = self.distinct * self.count\n",
    "    self.seeds = seeds or [random.randint(0, 1e12)\n",
    "                           for _ in range(self.distinct)]\n",
    "\n",
    "    self.envs = []\n",
    "    for seed in self.seeds:\n",
    "      for _ in range(self.count):\n",
    "        env = get_env(env_str)\n",
    "        env.seed(seed)\n",
    "        if hasattr(env, 'last'):\n",
    "          env.last = 100  # for algorithmic envs\n",
    "        self.envs.append(env)\n",
    "\n",
    "    self.dones = [True] * self.total\n",
    "    self.num_episodes_played = 0\n",
    "\n",
    "    one_env = self.get_one()\n",
    "    self.use_action_list = hasattr(one_env.action_space, 'spaces')\n",
    "    self.env_spec = env_spec.EnvSpec(self.get_one())\n",
    "\n",
    "  def get_seeds(self):\n",
    "    return self.seeds\n",
    "\n",
    "  def reset(self):\n",
    "    self.dones = [False] * self.total\n",
    "    self.num_episodes_played += len(self.envs)\n",
    "\n",
    "    # reset seeds to be synchronized\n",
    "    self.seeds = [random.randint(0, 1e12) for _ in range(self.distinct)]\n",
    "    counter = 0\n",
    "    for seed in self.seeds:\n",
    "      for _ in range(self.count):\n",
    "        self.envs[counter].seed(seed)\n",
    "        counter += 1\n",
    "\n",
    "    return [self.env_spec.convert_obs_to_list(env.reset())\n",
    "            for env in self.envs]\n",
    "\n",
    "  def reset_if(self, predicate=None):\n",
    "    if predicate is None:\n",
    "      predicate = self.dones\n",
    "    if self.count != 1:\n",
    "      assert np.all(predicate)\n",
    "      return self.reset()\n",
    "    self.num_episodes_played += sum(predicate)\n",
    "    output = [self.env_spec.convert_obs_to_list(env.reset())\n",
    "              if pred else None\n",
    "              for env, pred in zip(self.envs, predicate)]\n",
    "    for i, pred in enumerate(predicate):\n",
    "      if pred:\n",
    "        self.dones[i] = False\n",
    "    return output\n",
    "\n",
    "  def all_done(self):\n",
    "    return all(self.dones)\n",
    "\n",
    "  def step(self, actions):\n",
    "\n",
    "    def env_step(action):\n",
    "      action = self.env_spec.convert_action_to_gym(action)\n",
    "      obs, reward, done, tt = env.step(action)\n",
    "      obs = self.env_spec.convert_obs_to_list(obs)\n",
    "      return obs, reward, done, tt\n",
    "\n",
    "    actions = zip(*actions)\n",
    "    outputs = [env_step(action)\n",
    "               if not done else (self.env_spec.initial_obs(None), 0, True, None)\n",
    "               for action, env, done in zip(actions, self.envs, self.dones)]\n",
    "    for i, (_, _, done, _) in enumerate(outputs):\n",
    "      self.dones[i] = self.dones[i] or done\n",
    "    obs, reward, done, tt = zip(*outputs)\n",
    "    obs = [list(oo) for oo in zip(*obs)]\n",
    "    return [obs, reward, done, tt]\n",
    "\n",
    "  def get_one(self):\n",
    "    return random.choice(self.envs)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1 # FLAGS.batch_size\n",
    "replay_batch_size = 25 # FLAGS.replay_batch_size\n",
    "num_samples = 1 # FLAGS.num_samples # number of samples from each random seed initialization\n",
    "env_str = 'HalfCheetah-v1' # FLAGS.env\n",
    "env = GymWrapper(env_str, distinct=1 // 1, count=1)\n",
    "env_spec = env_spec.EnvSpec(env.get_one())\n",
    "max_step = 100 # FLAGS.max_step\n",
    "cutoff_agent = 1000 # FLAGS.cutoff_agent\n",
    "num_steps = 100000 # FLAGS.num_steps\n",
    "validation_frequency = 50 # FLAGS.validation_frequency\n",
    "target_network_lag = 0.99 # FLAGS.target_network_lag\n",
    "sample_from = 'target' # FLAGS.sample_from\n",
    "critic_weight = 0.0 # FLAGS.critic_weight\n",
    "objective = 'pcl' # FLAGS.objective\n",
    "trust_region_p = False # FLAGS.trust_region_p\n",
    "value_opt = 'grad' # FLAGS.value_opt\n",
    "max_divergence = 0.001 # FLAGS.max_divergence\n",
    "learning_rate = 0.002 # FLAGS.learning_rate\n",
    "clip_norm = 40 # FLAGS.clip_norm\n",
    "clip_adv = 1.0 # FLAGS.clip_adv\n",
    "tau = 0.0 # FLAGS.tau\n",
    "tau_decay = None # FLAGS.tau_decay # decay tau by this much every 100 steps\n",
    "tau_start = 0.1 # FLAGS.tau_start\n",
    "eps_lambda = 0.0 # FLAGS.eps_lambda # relative entropy regularizer\n",
    "update_eps_lambda = True # FLAGS.update_eps_lambda\n",
    "gamma = 0.995 # FLAGS.gamma\n",
    "rollout = 10 # FLAGS.rollout\n",
    "fixed_std = True # FLAGS.fixed_std # fix the std in Gaussian distributions\n",
    "input_prev_actions = True # FLAGS.input_prev_actions # input previous actions to policy network\n",
    "recurrent = False # FLAGS.recurrent\n",
    "input_time_step = False # FLAGS.input_time_step # input time step into value calucations\n",
    "use_online_batch = False # FLAGS.use_online_batch\n",
    "batch_by_steps = True # FLAGS.batch_by_steps\n",
    "unify_episodes = True # FLAGS.unify_episodes\n",
    "replay_buffer_size = 20000 # FLAGS.replay_buffer_size\n",
    "replay_buffer_alpha = 0.1 # FLAGS.replay_buffer_alpha\n",
    "replay_buffer_freq = 1 # FLAGS.replay_buffer_freq\n",
    "eviction = 'fifo' # FLAGS.eviction\n",
    "prioritize_by = 'step' # FLAGS.prioritize_by\n",
    "num_expert_paths = 0 # FLAGS.num_expert_paths\n",
    "internal_dim = 64 # FLAGS.internal_dim\n",
    "value_hidden_layers = 2 # FLAGS.value_hidden_layers\n",
    "tf_seed = 42 # FLAGS.tf_seed # random seed for tensorflow\n",
    "save_trajectories_dir = None # FLAGS.save_trajectories_dir # directory to save trajectories to, if desired\n",
    "load_trajectories_dir = None # FLAGS.load_trajectories_dir # file to load expert trajectories from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer2(object):\n",
    "    \n",
    "    def __init__(self, batch_size = 1, # FLAGS.batch_size\n",
    "                replay_batch_size = 25, # FLAGS.replay_batch_size\n",
    "                num_samples = 1, # FLAGS.num_samples # number of samples from each random seed initialization\n",
    "                env_str = 'HalfCheetah-v1', # FLAGS.env\n",
    "                max_step = 100, # FLAGS.max_step\n",
    "                cutoff_agent = 1000, # FLAGS.cutoff_agent\n",
    "                num_steps = 100000, # FLAGS.num_steps\n",
    "                validation_frequency = 50, # FLAGS.validation_frequency\n",
    "                target_network_lag = 0.99, # FLAGS.target_network_lag\n",
    "                sample_from = 'target', # FLAGS.sample_from\n",
    "                critic_weight = 0.0, # FLAGS.critic_weight\n",
    "                objective = 'pcl', # FLAGS.objective\n",
    "                trust_region_p = False, # FLAGS.trust_region_p\n",
    "                value_opt = 'grad', # FLAGS.value_opt\n",
    "                max_divergence = 0.001, # FLAGS.max_divergence\n",
    "                learning_rate = 0.002, # FLAGS.learning_rate\n",
    "                clip_norm = 40, # FLAGS.clip_norm\n",
    "                clip_adv = 1.0, # FLAGS.clip_adv\n",
    "                tau = 0.0, # FLAGS.tau\n",
    "                tau_decay = None, # FLAGS.tau_decay # decay tau by this much every 100 steps\n",
    "                tau_start = 0.1, # FLAGS.tau_start\n",
    "                eps_lambda = 0.0, # FLAGS.eps_lambda # relative entropy regularizer\n",
    "                update_eps_lambda = True, # FLAGS.update_eps_lambda\n",
    "                gamma = 0.995, # FLAGS.gamma\n",
    "                rollout = 10, # FLAGS.rollout\n",
    "                fixed_std = True, # FLAGS.fixed_std # fix the std in Gaussian distributions\n",
    "                input_prev_actions = True, # FLAGS.input_prev_actions # input previous actions to policy network\n",
    "                recurrent = False, # FLAGS.recurrent\n",
    "                input_time_step = False, # FLAGS.input_time_step # input time step into value calucations\n",
    "                use_online_batch = False, # FLAGS.use_online_batch\n",
    "                batch_by_steps = True, # FLAGS.batch_by_steps\n",
    "                unify_episodes = True, # FLAGS.unify_episodes\n",
    "                replay_buffer_size = 20000, # FLAGS.replay_buffer_size\n",
    "                replay_buffer_alpha = 0.1, # FLAGS.replay_buffer_alpha\n",
    "                replay_buffer_freq = 1, # FLAGS.replay_buffer_freq\n",
    "                eviction = 'fifo', # FLAGS.eviction\n",
    "                prioritize_by = 'step', # FLAGS.prioritize_by\n",
    "                num_expert_paths = 0, # FLAGS.num_expert_paths\n",
    "                internal_dim = 64, # FLAGS.internal_dim\n",
    "                value_hidden_layers = 2, # FLAGS.value_hidden_layers\n",
    "                tf_seed = 42, # FLAGS.tf_seed # random seed for tensorflow\n",
    "                save_trajectories_dir = None, # FLAGS.save_trajectories_dir # directory to save trajectories to, if desired\n",
    "                load_trajectories_dir = None # FLAGS.load_trajectories_dir # file to load expert trajectories from):\n",
    "                ):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.replay_batch_size = replay_batch_size\n",
    "        if self.replay_batch_size is None:\n",
    "          self.replay_batch_size = self.batch_size\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        self.env_str = env_str\n",
    "        self.env = GymWrapper(self.env_str,\n",
    "                              distinct=batch_size // self.num_samples,\n",
    "                              count=self.num_samples)\n",
    "        self.env_spec = env_spec.EnvSpec(self.env.get_one())\n",
    "\n",
    "        self.max_step = max_step\n",
    "        self.cutoff_agent = cutoff_agent\n",
    "        self.num_steps = num_steps\n",
    "        self.validation_frequency = validation_frequency\n",
    "\n",
    "        self.target_network_lag = target_network_lag\n",
    "        self.sample_from = sample_from\n",
    "        assert self.sample_from in ['online', 'target']\n",
    "\n",
    "        self.critic_weight = critic_weight\n",
    "        self.objective = objective\n",
    "        self.trust_region_p = False\n",
    "        self.value_opt = value_opt\n",
    "        assert not self.trust_region_p or self.objective in ['pcl', 'trpo']\n",
    "        assert self.objective != 'trpo' or self.trust_region_p\n",
    "        assert self.value_opt is None or self.critic_weight == 0.0\n",
    "        self.max_divergence = max_divergence\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.clip_norm = clip_norm\n",
    "        self.clip_adv = clip_adv\n",
    "        self.tau = tau\n",
    "        self.tau_decay = tau_decay\n",
    "        self.tau_start = tau_start\n",
    "        self.eps_lambda = eps_lambda\n",
    "        self.update_eps_lambda = update_eps_lambda\n",
    "        self.gamma = gamma\n",
    "        self.rollout = rollout\n",
    "        self.fixed_std = fixed_std\n",
    "        self.input_prev_actions = input_prev_actions\n",
    "        self.recurrent = recurrent\n",
    "        assert not self.trust_region_p or not self.recurrent\n",
    "        self.input_time_step = input_time_step\n",
    "        assert not self.input_time_step or (self.cutoff_agent <= self.max_step)\n",
    "\n",
    "        self.use_online_batch = use_online_batch\n",
    "        self.batch_by_steps = batch_by_steps\n",
    "        self.unify_episodes = unify_episodes\n",
    "        if self.unify_episodes:\n",
    "          assert self.batch_size == 1\n",
    "\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.replay_buffer_alpha = replay_buffer_alpha\n",
    "        self.replay_buffer_freq = replay_buffer_freq\n",
    "        assert self.replay_buffer_freq in [-1, 0, 1]\n",
    "        self.eviction = eviction\n",
    "        self.prioritize_by = prioritize_by\n",
    "        assert self.prioritize_by in ['rewards', 'step']\n",
    "        self.num_expert_paths = num_expert_paths\n",
    "\n",
    "        self.internal_dim = internal_dim\n",
    "        self.value_hidden_layers = value_hidden_layers\n",
    "        self.tf_seed = tf_seed\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        #self.save_trajectories_dir = (\n",
    "        #    save_trajectories_dir or save_dir)\n",
    "        #self.save_trajectories_file = (\n",
    "        #    os.path.join(\n",
    "        #        self.save_trajectories_dir, self.env_str.replace('-', '_'))\n",
    "        #    if self.save_trajectories_dir else None)\n",
    "        #self.load_trajectories_file = load_trajectories_file\n",
    "        \n",
    "    def get_objective(self):\n",
    "        tau = self.tau\n",
    "        if self.tau_decay is not None:\n",
    "            assert self.tau_start >= self.tau\n",
    "            tau = tf.maximum(\n",
    "                  tf.train.exponential_decay(\n",
    "                  self.tau_start, self.global_step, 100, self.tau_decay),\n",
    "                  self.tau)\n",
    "\n",
    "        if self.objective in ['pcl', 'a3c', 'trpo', 'upcl']:\n",
    "            cls = (objective.PCL if self.objective in ['pcl', 'upcl'] else\n",
    "                 objective.TRPO if self.objective == 'trpo' else\n",
    "                 objective.ActorCritic)\n",
    "        policy_weight = 1.0\n",
    "\n",
    "        return cls(self.learning_rate,\n",
    "                 clip_norm=self.clip_norm,\n",
    "                 policy_weight=policy_weight,\n",
    "                 critic_weight=self.critic_weight,\n",
    "                 tau=tau, gamma=self.gamma, rollout=self.rollout,\n",
    "                 eps_lambda=self.eps_lambda, clip_adv=self.clip_adv)\n",
    "\n",
    "\n",
    "    def get_policy(self):\n",
    "        if self.recurrent:\n",
    "            cls = policy.Policy\n",
    "        else:\n",
    "            cls = policy.MLPPolicy\n",
    "        return cls(self.env_spec, self.internal_dim,\n",
    "                   fixed_std=self.fixed_std,\n",
    "                   recurrent=self.recurrent,\n",
    "                   input_prev_actions=self.input_prev_actions)\n",
    "\n",
    "    def get_baseline(self):\n",
    "        cls = (baseline.UnifiedBaseline if self.objective == 'upcl' else\n",
    "               baseline.Baseline)\n",
    "        return cls(self.env_spec, self.internal_dim,\n",
    "                   input_prev_actions=self.input_prev_actions,\n",
    "                   input_time_step=self.input_time_step,\n",
    "                   input_policy_state=self.recurrent,  # may want to change this\n",
    "                   n_hidden_layers=self.value_hidden_layers,\n",
    "                   hidden_dim=self.internal_dim,\n",
    "                   tau=self.tau)\n",
    "\n",
    "    def get_trust_region_p_opt(self):\n",
    "          return None\n",
    "\n",
    "    def get_value_opt(self):\n",
    "        if self.value_opt == 'grad':\n",
    "            return optimizers.GradOptimization(\n",
    "              learning_rate=self.learning_rate, max_iter=5, mix_frac=0.05)\n",
    "        elif self.value_opt == 'lbfgs':\n",
    "            return optimizers.LbfgsOptimization(max_iter=25, mix_frac=0.1)\n",
    "        elif self.value_opt == 'best_fit':\n",
    "            return optimizers.BestFitOptimization(mix_frac=1.0)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def get_model(self):\n",
    "        cls = model.Model\n",
    "        return cls(self.env_spec, self.global_step,\n",
    "                   target_network_lag=self.target_network_lag,\n",
    "                   sample_from=self.sample_from,\n",
    "                   get_policy=self.get_policy,\n",
    "                   get_baseline=self.get_baseline,\n",
    "                   get_objective=self.get_objective,\n",
    "                   get_trust_region_p_opt=self.get_trust_region_p_opt,\n",
    "                   get_value_opt=self.get_value_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Trainer2' object has no attribute 'global_step'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-946bf3c57ad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_PCL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-7f2b7d21fdf4>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         return cls(self.env_spec, self.global_step,\n\u001b[0m\u001b[1;32m    183\u001b[0m                    \u001b[0mtarget_network_lag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_network_lag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                    \u001b[0msample_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_from\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Trainer2' object has no attribute 'global_step'"
     ]
    }
   ],
   "source": [
    "model_PCL = trainer.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GymWrapper('HalfCheetah-v1',\n",
    "                 distinct=1 // 1,\n",
    "                 count=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get objective of PCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_weight = 1.0\n",
    "objective_PCL = PCL(learning_rate,\n",
    "                    clip_norm=clip_norm,\n",
    "                    policy_weight=policy_weight,\n",
    "                    critic_weight=critic_weight,\n",
    "                    tau=tau, gamma=gamma, rollout=rollout,\n",
    "                    eps_lambda=eps_lambda, clip_adv=clip_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Policy setup of PCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = policy.MLPPolicy\n",
    "policy_PCL = cls(env_spec, internal_dim,\n",
    "                 fixed_std=fixed_std,\n",
    "                 recurrent=recurrent,\n",
    "                 input_prev_actions=input_prev_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get baseline/value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = (baseline.UnifiedBaseline if objective == 'upcl' else baseline.Baseline)\n",
    "baseline_PCL = cls(env_spec, internal_dim,\n",
    "                   input_prev_actions=input_prev_actions,\n",
    "                   input_time_step=input_time_step,\n",
    "                   input_policy_state=recurrent,  # may want to change this\n",
    "                   n_hidden_layers=value_hidden_layers,\n",
    "                   hidden_dim=internal_dim,\n",
    "                   tau=tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get value function optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if value_opt == 'grad':\n",
    "  optimizer_PCL =  optimizers.GradOptimization(learning_rate=learning_rate, max_iter=5, mix_frac=0.05)\n",
    "elif value_opt == 'lbfgs':\n",
    "  optimizer_PCL = optimizers.LbfgsOptimization(max_iter=25, mix_frac=0.1)\n",
    "elif value_opt == 'best_fit':\n",
    "  optimizer_PCL = optimizers.BestFitOptimization(mix_frac=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get model object that is responsible to set up all required optimization\n",
    "ops, including gradient ops, trust region ops, and value optimizers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'MLPPolicy' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-42e9a46ecc2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                \u001b[0mget_objective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobjective_PCL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                \u001b[0mget_trust_region_p_opt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                get_value_opt=optimizer_PCL)\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/PCL/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_spec, global_step, target_network_lag, sample_from, get_policy, get_baseline, get_objective, get_trust_region_p_opt, get_value_opt)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_from\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'MLPPolicy' object is not callable"
     ]
    }
   ],
   "source": [
    "global_step = tf.train.get_or_create_global_step()\n",
    "cls = model.Model\n",
    "model_PCL = cls(env_spec, global_step,\n",
    "               target_network_lag=target_network_lag,\n",
    "               sample_from=sample_from,\n",
    "               get_policy= policy_PCL,\n",
    "               get_baseline=baseline_PCL,\n",
    "               get_objective=objective_PCL,\n",
    "               get_trust_region_p_opt=None,\n",
    "               get_value_opt=optimizer_PCL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'global_step:0' shape=() dtype=int64_ref>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
