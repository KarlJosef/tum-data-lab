{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width:900px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "           }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "            }\n",
       "    h4{\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "             }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width:900px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "     \ttext-align: justify;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "    .prompt{\n",
       "            display: None;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 22pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "          }\n",
       "    \n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                TeX: {\n",
       "                    extensions: [\"AMSmath.js\"]\n",
       "                },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [['$','$']],\n",
       "                    displayMath: [['$$','$$']],\n",
       "                    processEscapes: true\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML  #For a more pleasing rendering...\n",
    "HTML(open(\"styles/custom.css\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Actor Critic Methods - A3C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font> **A3C** is short for **A**synchronous **A**dvantage **A**ctor-**C**ritic and was first described by the Deep Mind Team in 2016. The algorithm tries to utilize the benefits of Q-Learning and Policy Gradient methods. Let's start by unrolling the name: </font>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font> **Asynchronous**: Unlike DQN, where a single agent represented by a single neural network interacts with a single environment, A3C utilizes multiple incarnations of the above in order to learn more efficiently. In A3C there is a global network, and multiple worker agents which each have their own set of network parameters. Each of these agents interacts with it’s own copy of the environment at the same time as the other agents are interacting with their environments. The reason this works better than having a single agent, is that the experience of each agent is independent of the experience of the others. In this way the overall experience available for training becomes more diverse. (We also don't need an experience buffer anymore) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font> **Advantage**: The update rule we used with Policy Gradients: the discounted returns from a set of experiences, were used in order to tell the agent which of its actions were “good” and which were “bad.” The insight of using advantage estimates rather than just discounted returns is to allow the agent to determine not just how good its actions were, but how much better they turned out to be than expected. Intuitively, this allows the algorithm to focus on where the network’s predictions were lacking. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font> **Actor-Critic**: Our network will estimate both a value function V(s) (how good a certain state is to be in) and a policy $π(s)$ (a set of action probability outputs). These will each be separate fully-connected layers sitting at the top of the network. Critically, the agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font> Currently there are two state-of-the-art implementations of this algorithm. One is pureply based on computation on the cpu and one utilizes an gpu on top. We will look at two simple approaches to both types and link more complex projects for further reading. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3C - CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>he main\n",
    "reason for using CPU other than GPU, is the inherently sequential nature of RL in general,  and\n",
    "A3C in particular.  In RL, the training data are generated while learning, which means the training\n",
    "and inference batches are small and GPU is mostly idle during the training, waiting for new data to\n",
    "arrive.  Since A3C does not utilize any replay memory, it is completely sequential and therefore a\n",
    "CPU implementation is as fast as a naive GPU implementation.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This method is based on this [paper](https://arxiv.org/pdf/1602.01783.pdf). Good implementations can be found [here](https://github.com/openai/universe-starter-agent) or [here](https://github.com/ppwwyyxx/tensorpack/tree/master/examples/A3C-Gym)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/a3c_cpu.png\" width=450/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "import scipy.signal\n",
    "import gym\n",
    "import os\n",
    "import threading\n",
    "import multiprocessing\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font> The algorithm is very sensitive to learning parameters, just play around to find the ones that fit your system the best. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clipping ratio for gradients\n",
    "CLIP_NORM = 40.0\n",
    "# Cell units\n",
    "CELL_UNITS = 16\n",
    "#Size of mini batches to run training on\n",
    "MINI_BATCH = 40\n",
    "REWARD_FACTOR = 0.001\n",
    "\n",
    "# Gym environment\n",
    "ENV_NAME = 'CartPole-v0' # Discrete (4, 2)\n",
    "STATE_DIM = 4\n",
    "ACTION_DIM = 2\n",
    "\n",
    "# Learning rate\n",
    "LEARNING_RATE = 0.0005\n",
    "# Discount rate for advantage estimation and reward discounting\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Used to initialize weights for policy and value output layers\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer\n",
    "\n",
    "class AC_Network():\n",
    "    def __init__(self, s_size, a_size, scope, trainer):\n",
    "        with tf.variable_scope(scope):\n",
    "            # Input\n",
    "            self.inputs = tf.placeholder(shape=[None, s_size], dtype=tf.float32)\n",
    "\n",
    "            # Recurrent network for temporal dependencies\n",
    "            lstm_cell = tf.contrib.rnn.BasicLSTMCell(CELL_UNITS, state_is_tuple=True)\n",
    "            c_init = np.zeros((1, lstm_cell.state_size.c), np.float32)\n",
    "            h_init = np.zeros((1, lstm_cell.state_size.h), np.float32)\n",
    "            self.state_init = [c_init, h_init]\n",
    "            c_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.c])\n",
    "            h_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.h])\n",
    "            self.state_in = [c_in, h_in]\n",
    "            rnn_in = tf.expand_dims(self.inputs, [0])\n",
    "            state_in = tf.contrib.rnn.LSTMStateTuple(c_in, h_in)\n",
    "            lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n",
    "                lstm_cell, rnn_in,\n",
    "                initial_state=state_in,\n",
    "                time_major=False)\n",
    "            lstm_c, lstm_h = lstm_state\n",
    "            self.state_out = (lstm_c[:1, :], lstm_h[:1, :])\n",
    "            rnn_out = tf.reshape(lstm_outputs, [-1, CELL_UNITS])\n",
    "\n",
    "            # Output layers for policy and value estimations\n",
    "            self.policy = slim.fully_connected(rnn_out, a_size,\n",
    "                                               activation_fn=tf.nn.softmax,\n",
    "                                               weights_initializer=normalized_columns_initializer(0.01),\n",
    "                                               biases_initializer=None)\n",
    "            self.value = slim.fully_connected(rnn_out, 1,\n",
    "                                              activation_fn=None,\n",
    "                                              weights_initializer=normalized_columns_initializer(1.0),\n",
    "                                              biases_initializer=None)\n",
    "\n",
    "            # Only the worker network need ops for loss functions and gradient updating.\n",
    "            if scope != 'global':\n",
    "                self.actions = tf.placeholder(shape=[None, a_size], dtype=tf.float32)\n",
    "                self.target_v = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "                self.advantages = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "\n",
    "                self.responsible_outputs = tf.reduce_sum(self.policy * self.actions, [1])\n",
    "\n",
    "                # Value loss function\n",
    "                self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value, [-1])))\n",
    "\n",
    "                # Softmax policy loss function\n",
    "                self.policy_loss = -tf.reduce_sum(tf.log(tf.maximum(self.responsible_outputs, 1e-12)) * self.advantages)\n",
    "\n",
    "                # Softmax entropy function\n",
    "                self.entropy = - tf.reduce_sum(self.policy * tf.log(tf.maximum(self.policy, 1e-12)))\n",
    "\n",
    "                self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.01\n",
    "\n",
    "                # Get gradients from local network using local losses\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "                self.gradients = tf.gradients(self.loss, local_vars)\n",
    "                self.var_norms = tf.global_norm(local_vars)\n",
    "                grads, self.grad_norms = tf.clip_by_global_norm(self.gradients, 40.0)\n",
    "\n",
    "                # Apply local gradients to global network\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.apply_grads = trainer.apply_gradients(zip(grads, global_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copies one set of variables to another.\n",
    "# Used to set worker network parameters to those of global network.\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "# Weighted random selection returns n_picks random indexes.\n",
    "# the chance to pick the index i is give by the weight weights[i].\n",
    "def weighted_pick(weights,n_picks):\n",
    "    t = np.cumsum(weights)\n",
    "    s = np.sum(weights)\n",
    "    return np.searchsorted(t,np.random.rand(n_picks)*s)\n",
    "\n",
    "# Discounting function used to calculate discounted returns.\n",
    "def discounting(x, gamma):\n",
    "    return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "\n",
    "# Normalization of inputs and outputs\n",
    "def norm(x, upper, lower=0.):\n",
    "    return (x-lower)/max((upper-lower), 1e-12)\n",
    "\n",
    "class Worker():\n",
    "    def __init__(self, name, s_size, a_size, trainer, global_episodes, env_name, seed):\n",
    "        self.name = \"worker_\" + str(name)\n",
    "        self.number = name\n",
    "        self.trainer = trainer\n",
    "        self.global_episodes = global_episodes\n",
    "        self.increment = self.global_episodes.assign_add(1)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_values = []\n",
    "\n",
    "        self.a_size = a_size\n",
    "\n",
    "        # Create the local copy of the network and the tensorflow op to copy global parameters to local network\n",
    "        self.local_AC = AC_Network(s_size, a_size, self.name, trainer)\n",
    "        self.update_local_ops = update_target_graph('global', self.name)\n",
    "\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env.seed(seed)\n",
    "\n",
    "    def get_env(self):\n",
    "        return self.env\n",
    "\n",
    "    def train(self, rollout, sess, gamma, r):\n",
    "        rollout = np.array(rollout)\n",
    "        states = rollout[:, 0]\n",
    "        actions = rollout[:, 1]\n",
    "        rewards = rollout[:, 2]\n",
    "        values = rollout[:, 5]\n",
    "\n",
    "        # Here we take the rewards and values from the rollout, and use them to\n",
    "        # generate the advantage and discounted returns.\n",
    "        rewards_list = np.asarray(rewards.tolist()+[r])*REWARD_FACTOR\n",
    "        discounted_rewards = discounting(rewards_list, gamma)[:-1]\n",
    "\n",
    "        # Advantage estimation\n",
    "        # JS, P Moritz, S Levine, M Jordan, P Abbeel,\n",
    "        # \"High-dimensional continuous control using generalized advantage estimation.\"\n",
    "        # arXiv preprint arXiv:1506.02438 (2015).\n",
    "        values_list = np.asarray(values.tolist()+[r])*REWARD_FACTOR\n",
    "        advantages = rewards + gamma * values_list[1:] - values_list[:-1]\n",
    "        discounted_advantages = discounting(advantages, gamma)\n",
    "\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        # sess.run(self.local_AC.reset_state_op)\n",
    "        rnn_state = self.local_AC.state_init\n",
    "        feed_dict = {self.local_AC.target_v: discounted_rewards,\n",
    "                     self.local_AC.inputs: np.vstack(states),\n",
    "                     self.local_AC.actions: np.vstack(actions),\n",
    "                     self.local_AC.advantages: discounted_advantages,\n",
    "                     self.local_AC.state_in[0]: rnn_state[0],\n",
    "                     self.local_AC.state_in[1]: rnn_state[1]}\n",
    "        v_l, p_l, e_l, g_n, v_n, _ = sess.run([self.local_AC.value_loss,\n",
    "                                               self.local_AC.policy_loss,\n",
    "                                               self.local_AC.entropy,\n",
    "                                               self.local_AC.grad_norms,\n",
    "                                               self.local_AC.var_norms,\n",
    "                                               self.local_AC.apply_grads],\n",
    "                                              feed_dict=feed_dict)\n",
    "        return v_l / len(rollout), p_l / len(rollout), e_l / len(rollout), g_n, v_n\n",
    "\n",
    "    def work(self, gamma, sess, coord):\n",
    "        episode_count = sess.run(self.global_episodes)\n",
    "        total_steps = 0\n",
    "        print(\"Starting worker \" + str(self.number))\n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            while not coord.should_stop():\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_mini_buffer = []\n",
    "                episode_values = []\n",
    "                episode_states = []\n",
    "                episode_reward = 0\n",
    "                episode_step_count = 0\n",
    "\n",
    "                # Restart environment\n",
    "                terminal = False\n",
    "                s = self.env.reset()\n",
    "\n",
    "                rnn_state = self.local_AC.state_init\n",
    "\n",
    "                # Run an episode\n",
    "                while not terminal:\n",
    "                    episode_states.append(s)\n",
    "\n",
    "\n",
    "                    # Get preferred action distribution\n",
    "                    a_dist, v, rnn_state = sess.run([self.local_AC.policy, self.local_AC.value, self.local_AC.state_out],\n",
    "                                         feed_dict={self.local_AC.inputs: [s],\n",
    "                                                    self.local_AC.state_in[0]: rnn_state[0],\n",
    "                                                    self.local_AC.state_in[1]: rnn_state[1]})\n",
    "\n",
    "                    a0 = weighted_pick(a_dist[0], 1) # Use stochastic distribution sampling\n",
    "                    a = np.zeros(self.a_size)\n",
    "                    a[a0] = 1\n",
    "\n",
    "                    s2, r, terminal, info = self.env.step(np.argmax(a))\n",
    "\n",
    "                    episode_reward += r\n",
    "\n",
    "                    episode_buffer.append([s, a, r, s2, terminal, v[0, 0]])\n",
    "                    episode_mini_buffer.append([s, a, r, s2, terminal, v[0, 0]])\n",
    "\n",
    "                    episode_values.append(v[0, 0])\n",
    "\n",
    "                    # Train on mini batches from episode\n",
    "                    if len(episode_mini_buffer) == MINI_BATCH:\n",
    "                        v1 = sess.run([self.local_AC.value],\n",
    "                                      feed_dict={self.local_AC.inputs: [s],\n",
    "                                                    self.local_AC.state_in[0]: rnn_state[0],\n",
    "                                                    self.local_AC.state_in[1]: rnn_state[1]})\n",
    "                        v_l, p_l, e_l, g_n, v_n = self.train(episode_mini_buffer, sess, gamma, v1[0][0])\n",
    "                        episode_mini_buffer = []\n",
    "\n",
    "                    # Set previous state for next step\n",
    "                    s = s2\n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "\n",
    "                self.episode_rewards.append(episode_reward)\n",
    "                self.episode_lengths.append(episode_step_count)\n",
    "                self.episode_mean_values.append(np.mean(episode_values))\n",
    "\n",
    "                print(\"Reward: \" + str(episode_reward), \" | Episode\", episode_count)\n",
    "                sess.run(self.increment) # Next global episode\n",
    "\n",
    "                episode_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    global master_network\n",
    "    global global_episodes\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        RANDOM_SEED = 1234\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "        global_episodes = tf.Variable(0, dtype=tf.int32, name='global_episodes', trainable=False)\n",
    "        trainer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "        master_network = AC_Network(STATE_DIM, ACTION_DIM, 'global', None)  # Generate global network\n",
    "        num_workers = multiprocessing.cpu_count()  # Set workers to number of available CPU threads\n",
    "        \n",
    "\n",
    "        workers = []\n",
    "        # Create worker classes\n",
    "        for i in range(num_workers):\n",
    "            workers.append(Worker(i, STATE_DIM, ACTION_DIM, trainer, global_episodes,\n",
    "                                  ENV_NAME, RANDOM_SEED))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        coord = tf.train.Coordinator()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # This is where the asynchronous magic happens.\n",
    "        # Start the \"work\" process for each worker in a separate thread.\n",
    "        worker_threads = []\n",
    "        for worker in workers:\n",
    "            worker_work = lambda: worker.work(GAMMA, sess, coord)\n",
    "            t = threading.Thread(target=(worker_work))\n",
    "            t.start()\n",
    "            worker_threads.append(t)\n",
    "            \n",
    "        coord.join(worker_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-21 17:07:22,888] Making new env: CartPole-v0\n",
      "[2017-09-21 17:07:23,297] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting worker 1\n",
      "Starting worker 0\n",
      "('Reward: 26.0', ' | Episode', 0)\n",
      "('Reward: 15.0', ' | Episode', 1)\n",
      "('Reward: 30.0', ' | Episode', 0)\n",
      "('Reward: 15.0', ' | Episode', 2)\n",
      "('Reward: 10.0', ' | Episode', 3)\n",
      "('Reward: 18.0', ' | Episode', 1)\n",
      "('Reward: 11.0', ' | Episode', 4)\n",
      "('Reward: 36.0', ' | Episode', 2)\n",
      "('Reward: 16.0', ' | Episode', 3)\n",
      "('Reward: 26.0', ' | Episode', 4)\n",
      "('Reward: 58.0', ' | Episode', 5)\n",
      "('Reward: 40.0', ' | Episode', 6)\n",
      "('Reward: 17.0', ' | Episode', 7)\n",
      "('Reward: 41.0', ' | Episode', 5)\n",
      "('Reward: 13.0', ' | Episode', 8)\n",
      "('Reward: 16.0', ' | Episode', 6)\n",
      "('Reward: 14.0', ' | Episode', 9)\n",
      "('Reward: 35.0', ' | Episode', 7)\n",
      "('Reward: 12.0', ' | Episode', 8)\n",
      "('Reward: 41.0', ' | Episode', 10)\n",
      "('Reward: 25.0', ' | Episode', 9)\n",
      "('Reward: 38.0', ' | Episode', 11)\n",
      "('Reward: 30.0', ' | Episode', 10)\n",
      "('Reward: 19.0', ' | Episode', 12)\n",
      "('Reward: 14.0', ' | Episode', 11)\n",
      "('Reward: 33.0', ' | Episode', 13)\n",
      "('Reward: 23.0', ' | Episode', 12)\n",
      "('Reward: 17.0', ' | Episode', 14)\n",
      "('Reward: 25.0', ' | Episode', 13)\n",
      "('Reward: 9.0', ' | Episode', 15)\n",
      "('Reward: 12.0', ' | Episode', 14)\n",
      "('Reward: 18.0', ' | Episode', 16)\n",
      "('Reward: 16.0', ' | Episode', 15)\n",
      "('Reward: 13.0', ' | Episode', 17)\n",
      "('Reward: 16.0', ' | Episode', 18)\n",
      "('Reward: 9.0', ' | Episode', 19)\n",
      "('Reward: 20.0', ' | Episode', 20)\n",
      "('Reward: 22.0', ' | Episode', 21)\n",
      "('Reward: 76.0', ' | Episode', 16)\n",
      "('Reward: 16.0', ' | Episode', 22)\n",
      "('Reward: 22.0', ' | Episode', 17)\n",
      "('Reward: 22.0', ' | Episode', 23)\n",
      "('Reward: 12.0', ' | Episode', 18)\n",
      "('Reward: 17.0', ' | Episode', 24)\n",
      "('Reward: 12.0', ' | Episode', 19)\n",
      "('Reward: 11.0', ' | Episode', 20)\n",
      "('Reward: 24.0', ' | Episode', 25)\n",
      "('Reward: 18.0', ' | Episode', 21)\n",
      "('Reward: 33.0', ' | Episode', 22)\n",
      "('Reward: 54.0', ' | Episode', 26)\n",
      "('Reward: 18.0', ' | Episode', 23)\n",
      "('Reward: 18.0', ' | Episode', 27)\n",
      "('Reward: 11.0', ' | Episode', 24)\n",
      "('Reward: 42.0', ' | Episode', 28)\n",
      "('Reward: 35.0', ' | Episode', 25)\n",
      "('Reward: 17.0', ' | Episode', 29)\n",
      "('Reward: 29.0', ' | Episode', 26)\n",
      "('Reward: 13.0', ' | Episode', 30)\n",
      "('Reward: 19.0', ' | Episode', 27)\n",
      "('Reward: 26.0', ' | Episode', 31)\n",
      "('Reward: 18.0', ' | Episode', 28)\n",
      "('Reward: 14.0', ' | Episode', 32)\n",
      "('Reward: 13.0', ' | Episode', 29)\n",
      "('Reward: 17.0', ' | Episode', 33)\n",
      "('Reward: 17.0', ' | Episode', 30)\n",
      "('Reward: 16.0', ' | Episode', 34)\n",
      "('Reward: 24.0', ' | Episode', 31)\n",
      "('Reward: 23.0', ' | Episode', 32)('Reward: 37.0', ' | Episode', 35)\n",
      "\n",
      "('Reward: 17.0', ' | Episode', 36)\n",
      "('Reward: 30.0', ' | Episode', 33)\n",
      "('Reward: 20.0', ' | Episode', 37)\n",
      "('Reward: 10.0', ' | Episode', 34)\n",
      "('Reward: 44.0', ' | Episode', 38)\n",
      "('Reward: 11.0', ' | Episode', 39)\n",
      "('Reward: 56.0', ' | Episode', 35)\n",
      "('Reward: 15.0', ' | Episode', 40)\n",
      "('Reward: 14.0', ' | Episode', 41)\n",
      "('Reward: 22.0', ' | Episode', 36)\n",
      "('Reward: 14.0', ' | Episode', 42)\n",
      "('Reward: 14.0', ' | Episode', 37)\n",
      "('Reward: 19.0', ' | Episode', 43)\n",
      "('Reward: 25.0', ' | Episode', 38)\n",
      "('Reward: 10.0', ' | Episode', 39)\n",
      "('Reward: 35.0', ' | Episode', 44)\n",
      "('Reward: 30.0', ' | Episode', 45)\n",
      " ('Reward: 32.0', ' | Episode', 40)\n",
      "('Reward: 14.0', ' | Episode', 41)\n",
      "('Reward: 24.0', ' | Episode', 46)\n",
      "('Reward: 17.0', ' | Episode', 47)\n",
      "('Reward: 33.0', ' | Episode', 42)\n",
      "('Reward: 18.0', ' | Episode', 48)\n",
      "('Reward: 22.0', ' | Episode', 49)\n",
      "('Reward: 11.0', ' | Episode', 50)\n",
      "('Reward: 43.0', ' | Episode', 43)\n",
      "('Reward: 17.0', ' | Episode', 51)\n",
      "('Reward: 25.0', ' | Episode', 44)\n",
      "('Reward: 21.0', ' | Episode', 52)\n",
      "('Reward: 25.0', ' | Episode', 45)\n",
      "('Reward: 23.0', ' | Episode', 53)\n",
      "('Reward: 9.0', ' | Episode', 54)\n",
      "('Reward: 19.0', ' | Episode', 55)\n",
      "('Reward: 37.0', ' | Episode', 46)\n",
      "('Reward: 10.0', ' | Episode', 47)\n",
      "('Reward: 21.0', ' | Episode', 56)\n",
      "('Reward: 23.0', ' | Episode', 48)\n",
      "('Reward: 27.0', ' | Episode', 57)\n",
      "('Reward: 13.0', ' | Episode', 49)\n",
      "('Reward: 18.0', ' | Episode', 58)\n",
      "('Reward: 14.0', ' | Episode', 50)\n",
      "('Reward: 20.0', ' | Episode', 59)\n",
      "('Reward: 21.0', ' | Episode', 51)\n",
      "('Reward: 17.0', ' | Episode', 60)\n",
      "('Reward: 10.0', ' | Episode', 52)\n",
      "('Reward: 12.0', ' | Episode', 61)\n",
      "('Reward: 20.0', ' | Episode', 53)\n",
      "('Reward: 17.0', ' | Episode', 62)\n",
      "('Reward: 24.0', ' | Episode', 54)\n",
      "('Reward: 27.0', ' | Episode', 63)\n",
      "('Reward: 23.0', ' | Episode', 64)('Reward: 28.0', ' | Episode', 55)\n",
      "\n",
      "('Reward: 19.0', ' | Episode', 56)\n",
      "('Reward: 33.0', ' | Episode', 65)\n",
      "('Reward: 28.0', ' | Episode', 57)\n",
      "('Reward: 21.0', ' | Episode', 66)\n",
      "('Reward: 16.0', ' | Episode', 67)\n",
      "('Reward: 9.0', ' | Episode', 68)\n",
      "('Reward: 36.0', ' | Episode', 58)\n",
      "('Reward: 28.0', ' | Episode', 59)\n",
      "('Reward: 30.0', ' | Episode', 69)\n",
      "('Reward: 11.0', ' | Episode', 60)\n",
      "('Reward: 16.0', ' | Episode', 70)\n",
      "('Reward: 13.0', ' | Episode', 61)\n",
      "('Reward: 13.0', ' | Episode', 71)\n",
      "('Reward: 15.0', ' | Episode', 72)\n",
      "('Reward: 9.0', ' | Episode', 73)\n",
      "('Reward: 29.0', ' | Episode', 62)\n",
      "('Reward: 12.0', ' | Episode', 74)\n",
      "('Reward: 17.0', ' | Episode', 63)\n",
      "('Reward: 21.0', ' | Episode', 75)\n",
      "('Reward: 14.0', ' | Episode', 64)\n",
      "('Reward: 37.0', ' | Episode', 76)\n",
      "('Reward: 25.0', ' | Episode', 65)\n",
      "('Reward: 10.0', ' | Episode', 66)\n",
      "('Reward: 21.0', ' | Episode', 77)\n",
      "('Reward: 14.0', ' | Episode', 67)\n",
      "('Reward: 15.0', ' | Episode', 78)\n",
      "('Reward: 13.0', ' | Episode', 68)\n",
      "('Reward: 11.0', ' | Episode', 69)\n",
      "('Reward: 25.0', ' | Episode', 79)\n",
      "('Reward: 12.0', ' | Episode', 70)\n",
      "('Reward: 19.0', ' | Episode', 80)\n",
      "('Reward: 27.0', ' | Episode', 81)\n",
      "('Reward: 47.0', ' | Episode', 71)\n",
      " ('Reward: 23.0', ' | Episode', 82)\n",
      "('Reward: 16.0', ' | Episode', 83)\n",
      "('Reward: 27.0', ' | Episode', 72)\n",
      "('Reward: 18.0', ' | Episode', 84)\n",
      "('Reward: 13.0', ' | Episode', 73)\n",
      "('Reward: 11.0', ' | Episode', 74)\n",
      "('Reward: 28.0', ' | Episode', 85)\n",
      "('Reward: 13.0', ' | Episode', 86)\n",
      "('Reward: 12.0', ' | Episode', 87)\n",
      "('Reward: 19.0', ' | Episode', 75)\n",
      "('Reward: 14.0', ' | Episode', 88)\n",
      "('Reward: 30.0', ' | Episode', 76)\n",
      "('Reward: 13.0', ' | Episode', 89)\n",
      "('Reward: 14.0', ' | Episode', 90)\n",
      "('Reward: 25.0', ' | Episode', 77)\n",
      "('Reward: 16.0', ' | Episode', 91)\n",
      "('Reward: 53.0', ' | Episode', 92)\n",
      "('Reward: 19.0', ' | Episode', 93)\n",
      "('Reward: 91.0', ' | Episode', 78)\n",
      "('Reward: 15.0', ' | Episode', 94)\n",
      "('Reward: 12.0', ' | Episode', 79)\n",
      "('Reward: 17.0', ' | Episode', 95)\n",
      "('Reward: 21.0', ' | Episode', 80)\n",
      "('Reward: 17.0', ' | Episode', 96)\n",
      "('Reward: 21.0', ' | Episode', 81)\n",
      "('Reward: 13.0', ' | Episode', 97)\n",
      "('Reward: 21.0', ' | Episode', 82)\n",
      "('Reward: 21.0', ' | Episode', 98)\n",
      "('Reward: 26.0', ' | Episode', 83)\n",
      "('Reward: 10.0', ' | Episode', 84)\n",
      "('Reward: 27.0', ' | Episode', 99)\n",
      "('Reward: 30.0', ' | Episode', 85)\n",
      "('Reward: 20.0', ' | Episode', 86)\n",
      "('Reward: 11.0', ' | Episode', 87)\n",
      "('Reward: 49.0', ' | Episode', 100)\n",
      "('Reward: 12.0', ' | Episode', 101)\n",
      "('Reward: 24.0', ' | Episode', 88)\n",
      "('Reward: 20.0', ' | Episode', 89)\n",
      "('Reward: 32.0', ' | Episode', 102)\n",
      "('Reward: 24.0', ' | Episode', 90)\n",
      "('Reward: 15.0', ' | Episode', 103)\n",
      "('Reward: 20.0', ' | Episode', 91)\n",
      "('Reward: 25.0', ' | Episode', 104)\n",
      "('Reward: 18.0', ' | Episode', 92)\n",
      "('Reward: 18.0', ' | Episode', 105)\n",
      "('Reward: 15.0', ' | Episode', 106)\n",
      "('Reward: 31.0', ' | Episode', 93)\n",
      "('Reward: 16.0', ' | Episode', 94)\n",
      "('Reward: 19.0', ' | Episode', 95)\n",
      "('Reward: 46.0', ' | Episode', 107)\n",
      "('Reward: 13.0', ' | Episode', 96)\n",
      "('Reward: 13.0', ' | Episode', 108)\n",
      "('Reward: 16.0', ' | Episode', 97)\n",
      "('Reward: 24.0', ' | Episode', 109)('Reward: 23.0', ' | Episode', 98)\n",
      "\n",
      "('Reward: 11.0', ' | Episode', 99)\n",
      "('Reward: 20.0', ' | Episode', 110)\n",
      "('Reward: 43.0', ' | Episode', 100)\n",
      "('Reward: 13.0', ' | Episode', 101)\n",
      "('Reward: 46.0', ' | Episode', 111)\n",
      "('Reward: 27.0', ' | Episode', 102)\n",
      "('Reward: 38.0', ' | Episode', 112)\n",
      "('Reward: 17.0', ' | Episode', 103)\n",
      "('Reward: 11.0', ' | Episode', 104)\n",
      "('Reward: 15.0', ' | Episode', 113)\n",
      "('Reward: 21.0', ' | Episode', 114)\n",
      "('Reward: 36.0', ' | Episode', 105)\n",
      "('Reward: 19.0', ' | Episode', 115)\n",
      "('Reward: 26.0', ' | Episode', 106)\n",
      "('Reward: 21.0', ' | Episode', 116)\n",
      "('Reward: 42.0', ' | Episode', 107)\n",
      "('Reward: 25.0', ' | Episode', 108)\n",
      "('Reward: 45.0', ' | Episode', 117)\n",
      "('Reward: 29.0', ' | Episode', 109)\n",
      "('Reward: 12.0', ' | Episode', 110)\n",
      "('Reward: 45.0', ' | Episode', 118)\n",
      "('Reward: 11.0', ' | Episode', 111)\n",
      "('Reward: 25.0', ' | Episode', 112)\n",
      "('Reward: 15.0', ' | Episode', 113)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Reward: 12.0', ' | Episode', 114)\n",
      "('Reward: 91.0', ' | Episode', 119)\n",
      "('Reward: 19.0', ' | Episode', 120)\n",
      "('Reward: 54.0', ' | Episode', 115)\n",
      "('Reward: 30.0', ' | Episode', 121)\n",
      "('Reward: 13.0', ' | Episode', 116)\n",
      "('Reward: 18.0', ' | Episode', 122)\n",
      "('Reward: 13.0', ' | Episode', 123)\n",
      "('Reward: 50.0', ' | Episode', 117)\n",
      "('Reward: 32.0', ' | Episode', 124)\n",
      "('Reward: 35.0', ' | Episode', 118)\n",
      "('Reward: 34.0', ' | Episode', 125)\n",
      "('Reward: 32.0', ' | Episode', 119)\n",
      "('Reward: 23.0', ' | Episode', 126)\n",
      "('Reward: 13.0', ' | Episode', 120)\n",
      "('Reward: 16.0', ' | Episode', 127)\n",
      "('Reward: 17.0', ' | Episode', 121)\n",
      "('Reward: 9.0', ' | Episode', 122)\n",
      "('Reward: 33.0', ' | Episode', 128)\n",
      "('Reward: 36.0', ' | Episode', 123)\n",
      "('Reward: 15.0', ' | Episode', 129)\n",
      "('Reward: 14.0', ' | Episode', 124)\n",
      "('Reward: 24.0', ' | Episode', 130)\n",
      "('Reward: 15.0', ' | Episode', 125)\n",
      "('Reward: 11.0', ' | Episode', 131)\n",
      "('Reward: 22.0', ' | Episode', 132)\n",
      "('Reward: 21.0', ' | Episode', 133)\n",
      "('Reward: 31.0', ' | Episode', 134)\n",
      "('Reward: 17.0', ' | Episode', 135)\n",
      "('Reward: 29.0', ' | Episode', 136)\n",
      "('Reward: 136.0', ' | Episode', 126)\n",
      "('Reward: 14.0', ' | Episode', 137)\n",
      "('Reward: 20.0', ' | Episode', 127)\n",
      "('Reward: 14.0', ' | Episode', 138)\n",
      "('Reward: 19.0', ' | Episode', 128)\n",
      "('Reward: 9.0', ' | Episode', 129)\n",
      "('Reward: 20.0', ' | Episode', 139)\n",
      "('Reward: 15.0', ' | Episode', 130)\n",
      "('Reward: 19.0', ' | Episode', 140)\n",
      "('Reward: 15.0', ' | Episode', 131)\n",
      "('Reward: 10.0', ' | Episode', 132)\n",
      "('Reward: 22.0', ' | Episode', 141)\n",
      "('Reward: 27.0', ' | Episode', 142)\n",
      "('Reward: 46.0', ' | Episode', 133)\n",
      "('Reward: 15.0', ' | Episode', 143)\n",
      "('Reward: 15.0', ' | Episode', 134)\n",
      "('Reward: 15.0', ' | Episode', 144)\n",
      "('Reward: 12.0', ' | Episode', 145)\n",
      "('Reward: 29.0', ' | Episode', 146)\n",
      "('Reward: 63.0', ' | Episode', 135)\n",
      "('Reward: 12.0', ' | Episode', 147)\n",
      "('Reward: 9.0', ' | Episode', 136)\n",
      "('Reward: 11.0', ' | Episode', 148)\n",
      "('Reward: 27.0', ' | Episode', 137)\n",
      "('Reward: 34.0', ' | Episode', 149)\n",
      "('Reward: 17.0', ' | Episode', 138)\n",
      "('Reward: 11.0', ' | Episode', 150)\n",
      "('Reward: 14.0', ' | Episode', 151)\n",
      "('Reward: 20.0', ' | Episode', 139)\n",
      "('Reward: 23.0', ' | Episode', 152)\n",
      "('Reward: 33.0', ' | Episode', 140)\n",
      "('Reward: 14.0', ' | Episode', 153)\n",
      "('Reward: 15.0', ' | Episode', 154)\n",
      "('Reward: 16.0', ' | Episode', 141)\n",
      "('Reward: 30.0', ' | Episode', 142)\n",
      "('Reward: 39.0', ' | Episode', 155)\n",
      "('Reward: 18.0', ' | Episode', 143)\n",
      "('Reward: 15.0', ' | Episode', 156)\n",
      "('Reward: 12.0', ' | Episode', 157)\n",
      "('Reward: 8.0', ' | Episode', 158)\n",
      "('Reward: 24.0', ' | Episode', 144)\n",
      "('Reward: 21.0', ' | Episode', 145)\n",
      "('Reward: 23.0', ' | Episode', 159)\n",
      "('Reward: 31.0', ' | Episode', 160)\n",
      "('Reward: 15.0', ' | Episode', 161)\n",
      "('Reward: 57.0', ' | Episode', 146)\n",
      "('Reward: 29.0', ' | Episode', 162)\n",
      "('Reward: 33.0', ' | Episode', 147)('Reward: 12.0', ' | Episode', 163)\n",
      "\n",
      "('Reward: 13.0', ' | Episode', 148)\n",
      "('Reward: 24.0', ' | Episode', 164)\n",
      "('Reward: 20.0', ' | Episode', 149)\n",
      "('Reward: 22.0', ' | Episode', 165)\n",
      "('Reward: 16.0', ' | Episode', 150)\n",
      "('Reward: 11.0', ' | Episode', 151)\n",
      "('Reward: 15.0', ' | Episode', 166)\n",
      "('Reward: 12.0', ' | Episode', 167)\n",
      "('Reward: 27.0', ' | Episode', 152)\n",
      "('Reward: 15.0', ' | Episode', 168)\n",
      "('Reward: 13.0', ' | Episode', 169)\n",
      "('Reward: 20.0', ' | Episode', 153)\n",
      "('Reward: 9.0', ' | Episode', 170)\n",
      "('Reward: 13.0', ' | Episode', 154)\n",
      "('Reward: 15.0', ' | Episode', 171)('Reward: 13.0', ' | Episode', 155)\n",
      "\n",
      "('Reward: 12.0', ' | Episode', 156)\n",
      "('Reward: 15.0', ' | Episode', 172)\n",
      "('Reward: 21.0', ' | Episode', 157)\n",
      "('Reward: 20.0', ' | Episode', 158)\n",
      "('Reward: 39.0', ' | Episode', 173)\n",
      "('Reward: 18.0', ' | Episode', 159)\n",
      "('Reward: 23.0', ' | Episode', 174)\n",
      "('Reward: 35.0', ' | Episode', 160)\n",
      "('Reward: 11.0', ' | Episode', 161)\n",
      "('Reward: 31.0', ' | Episode', 175)\n",
      "('Reward: 17.0', ' | Episode', 162)\n",
      "('Reward: 19.0', ' | Episode', 176)\n",
      "('Reward: 16.0', ' | Episode', 177)('Reward: 14.0', ' | Episode', 163)\n",
      "\n",
      "('Reward: 11.0', ' | Episode', 164)\n",
      "('Reward: 31.0', ' | Episode', 165)\n",
      "('Reward: 35.0', ' | Episode', 178)\n",
      "('Reward: 34.0', ' | Episode', 166)\n",
      "('Reward: 33.0', ' | Episode', 179)\n",
      "('Reward: 12.0', ' | Episode', 167)\n",
      "('Reward: 18.0', ' | Episode', 180)\n",
      "('Reward: 21.0', ' | Episode', 168)\n",
      "('Reward: 22.0', ' | Episode', 169)\n",
      "('Reward: 24.0', ' | Episode', 170)\n",
      "('Reward: 12.0', ' | Episode', 171)\n",
      "('Reward: 21.0', ' | Episode', 172)\n",
      "('Reward: 85.0', ' | Episode', 181)\n",
      "('Reward: 14.0', ' | Episode', 173)\n",
      "('Reward: 25.0', ' | Episode', 182)\n",
      "('Reward: 12.0', ' | Episode', 183)\n",
      "('Reward: 27.0', ' | Episode', 184)\n",
      "('Reward: 34.0', ' | Episode', 185)\n",
      "('Reward: 11.0', ' | Episode', 186)\n",
      "('Reward: 104.0', ' | Episode', 174)\n",
      "('Reward: 20.0', ' | Episode', 187)\n",
      "('Reward: 25.0', ' | Episode', 175)\n",
      "('Reward: 10.0', ' | Episode', 188)\n",
      "('Reward: 32.0', ' | Episode', 189)\n",
      "('Reward: 19.0', ' | Episode', 190)\n",
      "('Reward: 74.0', ' | Episode', 176)\n",
      "('Reward: 28.0', ' | Episode', 191)\n",
      "('Reward: 19.0', ' | Episode', 177)\n",
      "('Reward: 19.0', ' | Episode', 178)\n",
      "('Reward: 26.0', ' | Episode', 192)\n",
      "('Reward: 11.0', ' | Episode', 179)\n",
      "('Reward: 22.0', ' | Episode', 193)\n",
      "('Reward: 17.0', ' | Episode', 180)\n",
      "('Reward: 18.0', ' | Episode', 181)\n",
      "('Reward: 15.0', ' | Episode', 182)('Reward: 36.0', ' | Episode', 194)\n",
      "\n",
      "('Reward: 28.0', ' | Episode', 195)\n",
      "('Reward: 18.0', ' | Episode', 196)\n",
      "('Reward: 54.0', ' | Episode', 183)\n",
      "('Reward: 16.0', ' | Episode', 184)\n",
      "('Reward: 36.0', ' | Episode', 197)\n",
      "('Reward: 21.0', ' | Episode', 185)\n",
      "('Reward: 20.0', ' | Episode', 186)\n",
      "('Reward: 28.0', ' | Episode', 198)\n",
      "('Reward: 12.0', ' | Episode', 199)\n",
      "('Reward: 23.0', ' | Episode', 200)\n",
      "('Reward: 19.0', ' | Episode', 201)\n",
      "('Reward: 56.0', ' | Episode', 187)\n",
      "('Reward: 9.0', ' | Episode', 188)\n",
      "('Reward: 20.0', ' | Episode', 202)\n",
      "('Reward: 27.0', ' | Episode', 189)\n",
      "('Reward: 10.0', ' | Episode', 190)\n",
      "('Reward: 40.0', ' | Episode', 203)\n",
      "('Reward: 36.0', ' | Episode', 191)\n",
      "('Reward: 22.0', ' | Episode', 192)\n",
      "('Reward: 40.0', ' | Episode', 204)\n",
      "('Reward: 20.0', ' | Episode', 193)\n",
      "('Reward: 14.0', ' | Episode', 194)\n",
      "('Reward: 34.0', ' | Episode', 205)\n",
      "('Reward: 16.0', ' | Episode', 195)\n",
      "('Reward: 13.0', ' | Episode', 206)\n",
      "('Reward: 21.0', ' | Episode', 196)\n",
      "('Reward: 23.0', ' | Episode', 207)\n",
      "('Reward: 21.0', ' | Episode', 197)\n",
      "('Reward: 12.0', ' | Episode', 198)\n",
      "('Reward: 30.0', ' | Episode', 208)\n",
      "('Reward: 26.0', ' | Episode', 199)\n",
      "('Reward: 23.0', ' | Episode', 209)\n",
      "('Reward: 19.0', ' | Episode', 200)\n",
      "('Reward: 16.0', ' | Episode', 210)\n",
      "('Reward: 22.0', ' | Episode', 211)\n",
      "('Reward: 38.0', ' | Episode', 201)\n",
      "('Reward: 19.0', ' | Episode', 212)\n",
      "('Reward: 19.0', ' | Episode', 202)\n",
      "('Reward: 24.0', ' | Episode', 213)\n",
      "('Reward: 27.0', ' | Episode', 203)\n",
      "('Reward: 15.0', ' | Episode', 214)\n",
      "('Reward: 14.0', ' | Episode', 215)\n",
      "('Reward: 29.0', ' | Episode', 216)\n",
      "('Reward: 44.0', ' | Episode', 204)\n",
      "('Reward: 13.0', ' | Episode', 205)\n",
      "('Reward: 30.0', ' | Episode', 217)\n",
      "('Reward: 17.0', ' | Episode', 218)\n",
      "('Reward: 34.0', ' | Episode', 206)\n",
      "('Reward: 18.0', ' | Episode', 207)\n",
      "('Reward: 24.0', ' | Episode', 219)\n",
      "('Reward: 18.0', ' | Episode', 208)\n",
      "('Reward: 16.0', ' | Episode', 220)\n",
      "('Reward: 16.0', ' | Episode', 209)\n",
      "('Reward: 18.0', ' | Episode', 210)('Reward: 29.0', ' | Episode', 221)\n",
      "\n",
      "('Reward: 14.0', ' | Episode', 222)\n",
      "('Reward: 34.0', ' | Episode', 211)\n",
      "('Reward: 17.0', ' | Episode', 212)\n",
      "('Reward: 14.0', ' | Episode', 213)\n",
      "('Reward: 76.0', ' | Episode', 223)\n",
      "('Reward: 31.0', ' | Episode', 214)\n",
      "('Reward: 11.0', ' | Episode', 224)\n",
      "('Reward: 15.0', ' | Episode', 215)\n",
      "('Reward: 26.0', ' | Episode', 225)\n",
      "('Reward: 17.0', ' | Episode', 216)\n",
      "('Reward: 14.0', ' | Episode', 217)\n",
      "('Reward: 21.0', ' | Episode', 218)\n",
      "('Reward: 49.0', ' | Episode', 226)\n",
      "('Reward: 15.0', ' | Episode', 219)\n",
      "('Reward: 11.0', ' | Episode', 227)\n",
      "('Reward: 16.0', ' | Episode', 228)\n",
      "('Reward: 31.0', ' | Episode', 220)\n",
      "('Reward: 15.0', ' | Episode', 229)\n",
      "('Reward: 12.0', ' | Episode', 221)\n",
      "('Reward: 41.0', ' | Episode', 222)\n",
      "('Reward: 43.0', ' | Episode', 230)\n",
      "('Reward: 11.0', ' | Episode', 223)\n",
      "('Reward: 11.0', ' | Episode', 231)\n",
      "('Reward: 18.0', ' | Episode', 224)\n",
      "('Reward: 18.0', ' | Episode', 225)\n",
      "('Reward: 22.0', ' | Episode', 232)\n",
      "('Reward: 14.0', ' | Episode', 226)\n",
      "('Reward: 13.0', ' | Episode', 233)\n",
      "('Reward: 18.0', ' | Episode', 227)\n",
      "('Reward: 11.0', ' | Episode', 234)\n",
      "('Reward: 12.0', ' | Episode', 235)\n",
      "('Reward: 15.0', ' | Episode', 236)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Reward: 48.0', ' | Episode', 228)\n",
      "('Reward: 19.0', ' | Episode', 229)\n",
      "('Reward: 10.0', ' | Episode', 230)\n",
      "('Reward: 24.0', ' | Episode', 231)\n",
      "('Reward: 19.0', ' | Episode', 232)\n",
      "('Reward: 86.0', ' | Episode', 237)\n",
      "('Reward: 25.0', ' | Episode', 238)\n",
      "('Reward: 54.0', ' | Episode', 233)\n",
      "('Reward: 11.0', ' | Episode', 234)\n",
      "('Reward: 36.0', ' | Episode', 239)\n",
      "('Reward: 18.0', ' | Episode', 235)\n",
      "('Reward: 14.0', ' | Episode', 236)\n",
      "('Reward: 47.0', ' | Episode', 240)\n",
      "('Reward: 21.0', ' | Episode', 237)\n",
      "('Reward: 12.0', ' | Episode', 238)\n",
      "('Reward: 33.0', ' | Episode', 241)\n",
      "('Reward: 27.0', ' | Episode', 239)\n",
      "('Reward: 13.0', ' | Episode', 240)\n",
      "('Reward: 59.0', ' | Episode', 242)\n",
      "('Reward: 30.0', ' | Episode', 241)\n",
      "('Reward: 20.0', ' | Episode', 243)\n",
      "('Reward: 12.0', ' | Episode', 242)\n",
      "('Reward: 11.0', ' | Episode', 243)\n",
      "('Reward: 14.0', ' | Episode', 244)\n",
      "('Reward: 9.0', ' | Episode', 245)\n",
      "('Reward: 36.0', ' | Episode', 244)\n",
      "('Reward: 17.0', ' | Episode', 245)\n",
      "('Reward: 48.0', ' | Episode', 246)\n",
      "('Reward: 38.0', ' | Episode', 246)\n",
      "('Reward: 31.0', ' | Episode', 247)\n",
      "('Reward: 46.0', ' | Episode', 247)\n",
      "('Reward: 29.0', ' | Episode', 248)\n",
      "('Reward: 17.0', ' | Episode', 248)\n",
      "('Reward: 17.0', ' | Episode', 249)\n",
      "('Reward: 16.0', ' | Episode', 249)\n",
      "('Reward: 20.0', ' | Episode', 250)\n",
      "('Reward: 43.0', ' | Episode', 250)\n",
      "('Reward: 22.0', ' | Episode', 251)\n",
      "('Reward: 17.0', ' | Episode', 251)\n",
      "('Reward: 20.0', ' | Episode', 252)\n",
      "('Reward: 42.0', ' | Episode', 252)\n",
      "('Reward: 50.0', ' | Episode', 253)\n",
      "('Reward: 13.0', ' | Episode', 253)\n",
      "('Reward: 14.0', ' | Episode', 254)\n",
      "('Reward: 24.0', ' | Episode', 254)\n",
      "('Reward: 15.0', ' | Episode', 255)\n",
      "('Reward: 11.0', ' | Episode', 255)\n",
      "('Reward: 16.0', ' | Episode', 256)\n",
      "('Reward: 10.0', ' | Episode', 257)('Reward: 33.0', ' | Episode', 256)\n",
      "\n",
      "('Reward: 19.0', ' | Episode', 258)\n",
      "('Reward: 26.0', ' | Episode', 257)\n",
      "('Reward: 14.0', ' | Episode', 258)\n",
      "('Reward: 16.0', ' | Episode', 259)\n",
      "('Reward: 51.0', ' | Episode', 259)\n",
      "('Reward: 31.0', ' | Episode', 260)\n",
      "('Reward: 16.0', ' | Episode', 260)\n",
      "('Reward: 10.0', ' | Episode', 261)\n",
      "('Reward: 40.0', ' | Episode', 261)\n",
      "('Reward: 33.0', ' | Episode', 262)\n",
      "('Reward: 26.0', ' | Episode', 262)\n",
      "('Reward: 29.0', ' | Episode', 263)\n",
      "('Reward: 45.0', ' | Episode', 263)\n",
      "('Reward: 30.0', ' | Episode', 264)\n",
      "('Reward: 15.0', ' | Episode', 265)\n",
      "('Reward: 10.0', ' | Episode', 266)\n",
      "('Reward: 53.0', ' | Episode', 264)\n",
      "('Reward: 22.0', ' | Episode', 267)\n",
      "('Reward: 10.0', ' | Episode', 265)\n",
      "('Reward: 10.0', ' | Episode', 268)\n",
      "('Reward: 32.0', ' | Episode', 266)\n",
      "('Reward: 29.0', ' | Episode', 269)\n",
      "('Reward: 23.0', ' | Episode', 267)\n",
      "('Reward: 14.0', ' | Episode', 270)\n",
      "('Reward: 39.0', ' | Episode', 271)\n",
      "('Reward: 12.0', ' | Episode', 272)\n",
      "('Reward: 59.0', ' | Episode', 268)\n",
      "('Reward: 12.0', ' | Episode', 273)\n",
      "('Reward: 39.0', ' | Episode', 269)\n",
      "('Reward: 27.0', ' | Episode', 274)\n",
      "('Reward: 16.0', ' | Episode', 270)\n",
      "('Reward: 26.0', ' | Episode', 275)\n",
      "('Reward: 21.0', ' | Episode', 271)\n",
      "('Reward: 14.0', ' | Episode', 276)\n",
      "('Reward: 39.0', ' | Episode', 272)\n",
      "('Reward: 33.0', ' | Episode', 277)\n",
      "('Reward: 33.0', ' | Episode', 273)\n",
      "('Reward: 28.0', ' | Episode', 278)\n",
      "('Reward: 22.0', ' | Episode', 274)\n",
      "('Reward: 27.0', ' | Episode', 279)\n",
      "('Reward: 15.0', ' | Episode', 275)\n",
      "('Reward: 12.0', ' | Episode', 280)\n",
      "('Reward: 16.0', ' | Episode', 281)\n",
      "('Reward: 19.0', ' | Episode', 276)\n",
      "('Reward: 11.0', ' | Episode', 282)\n",
      "('Reward: 17.0', ' | Episode', 277)\n",
      "('Reward: 47.0', ' | Episode', 283)\n",
      "('Reward: 16.0', ' | Episode', 284)\n",
      "('Reward: 21.0', ' | Episode', 285)\n",
      "('Reward: 85.0', ' | Episode', 278)\n",
      "('Reward: 14.0', ' | Episode', 286)\n",
      "('Reward: 26.0', ' | Episode', 279)\n",
      "('Reward: 15.0', ' | Episode', 280)\n",
      "('Reward: 32.0', ' | Episode', 287)\n",
      "('Reward: 13.0', ' | Episode', 281)\n",
      "('Reward: 22.0', ' | Episode', 282)\n",
      "('Reward: 26.0', ' | Episode', 288)\n",
      "('Reward: 13.0', ' | Episode', 283)\n",
      "('Reward: 11.0', ' | Episode', 289)\n",
      "('Reward: 19.0', ' | Episode', 290)\n",
      "('Reward: 24.0', ' | Episode', 284)\n",
      "('Reward: 12.0', ' | Episode', 291)\n",
      "('Reward: 26.0', ' | Episode', 285)\n",
      "('Reward: 13.0', ' | Episode', 286)\n",
      "('Reward: 28.0', ' | Episode', 292)\n",
      "('Reward: 18.0', ' | Episode', 287)\n",
      "('Reward: 15.0', ' | Episode', 288)\n",
      "('Reward: 35.0', ' | Episode', 293)\n",
      "('Reward: 23.0', ' | Episode', 289)\n",
      "('Reward: 24.0', ' | Episode', 294)\n",
      "('Reward: 22.0', ' | Episode', 295)\n",
      "('Reward: 36.0', ' | Episode', 290)\n",
      "('Reward: 14.0', ' | Episode', 291)\n",
      "('Reward: 19.0', ' | Episode', 292)\n",
      "('Reward: 13.0', ' | Episode', 293)\n",
      "('Reward: 59.0', ' | Episode', 296)\n",
      "('Reward: 12.0', ' | Episode', 297)\n",
      "('Reward: 18.0', ' | Episode', 294)\n",
      "('Reward: 14.0', ' | Episode', 298)\n",
      "('Reward: 14.0', ' | Episode', 299)\n",
      "('Reward: 51.0', ' | Episode', 295)\n",
      "('Reward: 28.0', ' | Episode', 300)\n",
      "('Reward: 10.0', ' | Episode', 296)\n",
      "('Reward: 15.0', ' | Episode', 297)\n",
      "('Reward: 11.0', ' | Episode', 298)\n",
      "('Reward: 17.0', ' | Episode', 299)\n",
      "('Reward: 65.0', ' | Episode', 301)\n",
      " ('Reward: 16.0', ' | Episode', 300)\n",
      "('Reward: 15.0', ' | Episode', 301)\n",
      "('Reward: 23.0', ' | Episode', 302)\n",
      "('Reward: 23.0', ' | Episode', 302)\n",
      "('Reward: 17.0', ' | Episode', 303)\n",
      "('Reward: 16.0', ' | Episode', 303)\n",
      "('Reward: 10.0', ' | Episode', 304)\n",
      "('Reward: 16.0', ' | Episode', 304)\n",
      "('Reward: 30.0', ' | Episode', 305)\n",
      "('Reward: 14.0', ' | Episode', 306)\n",
      "('Reward: 39.0', ' | Episode', 305)\n",
      "('Reward: 16.0', ' | Episode', 307)\n",
      "('Reward: 20.0', ' | Episode', 306)\n",
      "('Reward: 24.0', ' | Episode', 308)\n",
      "('Reward: 16.0', ' | Episode', 307)\n",
      "('Reward: 20.0', ' | Episode', 308)\n",
      "('Reward: 67.0', ' | Episode', 309)\n",
      "('Reward: 57.0', ' | Episode', 309)\n",
      "('Reward: 14.0', ' | Episode', 310)\n",
      "('Reward: 27.0', ' | Episode', 311)\n",
      "('Reward: 40.0', ' | Episode', 310)\n",
      "('Reward: 19.0', ' | Episode', 312)\n",
      "('Reward: 19.0', ' | Episode', 311)\n",
      "('Reward: 20.0', ' | Episode', 313)\n",
      "('Reward: 13.0', ' | Episode', 312)\n",
      "('Reward: 19.0', ' | Episode', 314)\n",
      "('Reward: 20.0', ' | Episode', 315)\n",
      "('Reward: 12.0', ' | Episode', 316)\n",
      "('Reward: 21.0', ' | Episode', 317)\n",
      "('Reward: 65.0', ' | Episode', 313)\n",
      "('Reward: 16.0', ' | Episode', 314)\n",
      "('Reward: 20.0', ' | Episode', 318)\n",
      "('Reward: 9.0', ' | Episode', 319)\n",
      "('Reward: 15.0', ' | Episode', 315)\n",
      "('Reward: 61.0', ' | Episode', 316)\n",
      "('Reward: 24.0', ' | Episode', 317)\n",
      "('Reward: 107.0', ' | Episode', 320)\n",
      "('Reward: 16.0', ' | Episode', 318)\n",
      "('Reward: 17.0', ' | Episode', 321)\n",
      "('Reward: 28.0', ' | Episode', 319)\n",
      "('Reward: 22.0', ' | Episode', 320)\n",
      "('Reward: 49.0', ' | Episode', 322)\n",
      "('Reward: 12.0', ' | Episode', 321)\n",
      "('Reward: 10.0', ' | Episode', 323)\n",
      "('Reward: 15.0', ' | Episode', 322)\n",
      "('Reward: 12.0', ' | Episode', 324)\n",
      "('Reward: 13.0', ' | Episode', 325)\n",
      "('Reward: 21.0', ' | Episode', 323)\n",
      "('Reward: 11.0', ' | Episode', 326)\n",
      "('Reward: 18.0', ' | Episode', 327)\n",
      "('Reward: 33.0', ' | Episode', 324)\n",
      "('Reward: 29.0', ' | Episode', 328)\n",
      "('Reward: 34.0', ' | Episode', 325)\n",
      "('Reward: 25.0', ' | Episode', 329)\n",
      "('Reward: 11.0', ' | Episode', 330)\n",
      "('Reward: 22.0', ' | Episode', 326)\n",
      "('Reward: 20.0', ' | Episode', 327)\n",
      "('Reward: 39.0', ' | Episode', 331)\n",
      "('Reward: 21.0', ' | Episode', 328)\n",
      "('Reward: 16.0', ' | Episode', 332)\n",
      "('Reward: 28.0', ' | Episode', 329)\n",
      "('Reward: 26.0', ' | Episode', 333)\n",
      "('Reward: 16.0', ' | Episode', 334)\n",
      "('Reward: 13.0', ' | Episode', 335)\n",
      "('Reward: 26.0', ' | Episode', 330)\n",
      "('Reward: 13.0', ' | Episode', 331)\n",
      "('Reward: 21.0', ' | Episode', 336)\n",
      "('Reward: 12.0', ' | Episode', 332)\n",
      "('Reward: 27.0', ' | Episode', 337)\n",
      "('Reward: 48.0', ' | Episode', 338)\n",
      "('Reward: 71.0', ' | Episode', 333)\n",
      "('Reward: 13.0', ' | Episode', 339)\n",
      "('Reward: 14.0', ' | Episode', 334)\n",
      "('Reward: 12.0', ' | Episode', 340)\n",
      "('Reward: 12.0', ' | Episode', 341)\n",
      "('Reward: 16.0', ' | Episode', 335)\n",
      "('Reward: 14.0', ' | Episode', 342)\n",
      "('Reward: 16.0', ' | Episode', 336)\n",
      "('Reward: 13.0', ' | Episode', 337)\n",
      "('Reward: 23.0', ' | Episode', 343)\n",
      "('Reward: 20.0', ' | Episode', 344)\n",
      "('Reward: 35.0', ' | Episode', 338)\n",
      "('Reward: 16.0', ' | Episode', 339)\n",
      "('Reward: 38.0', ' | Episode', 345)\n",
      "('Reward: 12.0', ' | Episode', 346)\n",
      "('Reward: 25.0', ' | Episode', 340)\n",
      "('Reward: 19.0', ' | Episode', 347)\n",
      "('Reward: 19.0', ' | Episode', 348)\n",
      "('Reward: 16.0', ' | Episode', 349)\n",
      "('Reward: 66.0', ' | Episode', 341)\n",
      "('Reward: 33.0', ' | Episode', 350)\n",
      "('Reward: 24.0', ' | Episode', 342)\n",
      "('Reward: 10.0', ' | Episode', 343)\n",
      "('Reward: 43.0', ' | Episode', 351)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Reward: 40.0', ' | Episode', 352)\n",
      "('Reward: 64.0', ' | Episode', 344)\n",
      "('Reward: 29.0', ' | Episode', 353)\n",
      "('Reward: 17.0', ' | Episode', 354)\n",
      "('Reward: 23.0', ' | Episode', 345)\n",
      "('Reward: 13.0', ' | Episode', 346)\n",
      "('Reward: 25.0', ' | Episode', 355)\n",
      "('Reward: 32.0', ' | Episode', 356)\n",
      "('Reward: 15.0', ' | Episode', 357)\n",
      "('Reward: 71.0', ' | Episode', 347)\n",
      "('Reward: 57.0', ' | Episode', 358)\n",
      "('Reward: 14.0', ' | Episode', 359)\n",
      "('Reward: 55.0', ' | Episode', 348)\n",
      "('Reward: 14.0', ' | Episode', 349)\n",
      "('Reward: 14.0', ' | Episode', 360)\n",
      "('Reward: 21.0', ' | Episode', 350)\n",
      "('Reward: 31.0', ' | Episode', 361)\n",
      "('Reward: 14.0', ' | Episode', 351)\n",
      "('Reward: 17.0', ' | Episode', 362)\n",
      "('Reward: 14.0', ' | Episode', 363)\n",
      "('Reward: 27.0', ' | Episode', 352)\n",
      "('Reward: 15.0', ' | Episode', 364)\n",
      "('Reward: 17.0', ' | Episode', 353)\n",
      "('Reward: 12.0', ' | Episode', 354)\n",
      "('Reward: 49.0', ' | Episode', 365)\n",
      "('Reward: 61.0', ' | Episode', 355)\n",
      "('Reward: 35.0', ' | Episode', 366)\n",
      "('Reward: 15.0', ' | Episode', 356)\n",
      "('Reward: 13.0', ' | Episode', 357)\n",
      "('Reward: 24.0', ' | Episode', 367)\n",
      "('Reward: 11.0', ' | Episode', 358)\n",
      "('Reward: 26.0', ' | Episode', 359)\n",
      "('Reward: 65.0', ' | Episode', 368)\n",
      "('Reward: 36.0', ' | Episode', 360)\n",
      "('Reward: 12.0', ' | Episode', 369)\n",
      "('Reward: 26.0', ' | Episode', 361)\n",
      "('Reward: 36.0', ' | Episode', 370)\n",
      "('Reward: 19.0', ' | Episode', 362)\n",
      "('Reward: 29.0', ' | Episode', 371)\n",
      "('Reward: 25.0', ' | Episode', 363)\n",
      "('Reward: 11.0', ' | Episode', 364)\n",
      "('Reward: 22.0', ' | Episode', 372)\n",
      "('Reward: 25.0', ' | Episode', 365)\n",
      "('Reward: 16.0', ' | Episode', 373)\n",
      "('Reward: 14.0', ' | Episode', 366)\n",
      "('Reward: 45.0', ' | Episode', 374)\n",
      "('Reward: 39.0', ' | Episode', 367)\n",
      "('Reward: 17.0', ' | Episode', 375)\n",
      "('Reward: 16.0', ' | Episode', 368)\n",
      "('Reward: 34.0', ' | Episode', 376)\n",
      "('Reward: 29.0', ' | Episode', 369)\n",
      "('Reward: 34.0', ' | Episode', 377)\n",
      "('Reward: 28.0', ' | Episode', 370)\n",
      "('Reward: 9.0', ' | Episode', 371)\n",
      "('Reward: 28.0', ' | Episode', 378)\n",
      "('Reward: 12.0', ' | Episode', 372)\n",
      "('Reward: 52.0', ' | Episode', 379)\n",
      "('Reward: 30.0', ' | Episode', 380)\n",
      "('Reward: 84.0', ' | Episode', 373)\n",
      "('Reward: 30.0', ' | Episode', 381)\n",
      "('Reward: 25.0', ' | Episode', 374)\n",
      "('Reward: 22.0', ' | Episode', 382)\n",
      "('Reward: 14.0', ' | Episode', 375)\n",
      "('Reward: 24.0', ' | Episode', 383)('Reward: 23.0', ' | Episode', 376)\n",
      "\n",
      "('Reward: 11.0', ' | Episode', 377)\n",
      "('Reward: 22.0', ' | Episode', 384)\n",
      "('Reward: 13.0', ' | Episode', 385)\n",
      "('Reward: 33.0', ' | Episode', 378)\n",
      "('Reward: 10.0', ' | Episode', 379)\n",
      "('Reward: 21.0', ' | Episode', 386)\n",
      "('Reward: 17.0', ' | Episode', 380)\n",
      "('Reward: 17.0', ' | Episode', 387)\n",
      "('Reward: 41.0', ' | Episode', 381)\n",
      "('Reward: 13.0', ' | Episode', 382)\n",
      "('Reward: 71.0', ' | Episode', 388)\n",
      "('Reward: 10.0', ' | Episode', 389)\n",
      "('Reward: 35.0', ' | Episode', 383)\n",
      "('Reward: 15.0', ' | Episode', 384)\n",
      "('Reward: 34.0', ' | Episode', 390)\n",
      "('Reward: 17.0', ' | Episode', 385)\n",
      "('Reward: 19.0', ' | Episode', 391)\n",
      "('Reward: 14.0', ' | Episode', 386)\n",
      "('Reward: 24.0', ' | Episode', 392)\n",
      "('Reward: 14.0', ' | Episode', 393)\n",
      "('Reward: 20.0', ' | Episode', 394)\n",
      "('Reward: 63.0', ' | Episode', 387)\n",
      "('Reward: 35.0', ' | Episode', 395)\n",
      "('Reward: 10.0', ' | Episode', 388)\n",
      "('Reward: 16.0', ' | Episode', 396)\n",
      "('Reward: 12.0', ' | Episode', 397)\n",
      "('Reward: 25.0', ' | Episode', 389)\n",
      "('Reward: 16.0', ' | Episode', 390)\n",
      "('Reward: 58.0', ' | Episode', 398)\n",
      "('Reward: 33.0', ' | Episode', 391)\n",
      "('Reward: 35.0', ' | Episode', 399)\n",
      "('Reward: 17.0', ' | Episode', 392)\n",
      "('Reward: 22.0', ' | Episode', 393)\n",
      "('Reward: 43.0', ' | Episode', 400)\n",
      "('Reward: 16.0', ' | Episode', 394)\n",
      "('Reward: 17.0', ' | Episode', 401)\n",
      "('Reward: 29.0', ' | Episode', 395)\n",
      "('Reward: 22.0', ' | Episode', 402)\n",
      "('Reward: 18.0', ' | Episode', 396)\n",
      "('Reward: 24.0', ' | Episode', 403)\n",
      "('Reward: 16.0', ' | Episode', 404)\n",
      "('Reward: 41.0', ' | Episode', 397)('Reward: 32.0', ' | Episode', 405)\n",
      "\n",
      "('Reward: 19.0', ' | Episode', 406)\n",
      "('Reward: 17.0', ' | Episode', 398)\n",
      "('Reward: 17.0', ' | Episode', 407)\n",
      "('Reward: 12.0', ' | Episode', 399)\n",
      "('Reward: 14.0', ' | Episode', 400)\n",
      "('Reward: 61.0', ' | Episode', 408)\n",
      "('Reward: 58.0', ' | Episode', 401)\n",
      "('Reward: 44.0', ' | Episode', 409)\n",
      "('Reward: 21.0', ' | Episode', 410)\n",
      "('Reward: 35.0', ' | Episode', 411)\n",
      "('Reward: 73.0', ' | Episode', 402)\n",
      "('Reward: 41.0', ' | Episode', 403)\n",
      "('Reward: 62.0', ' | Episode', 412)\n",
      "('Reward: 19.0', ' | Episode', 404)\n",
      "('Reward: 28.0', ' | Episode', 413)\n",
      "('Reward: 39.0', ' | Episode', 405)\n",
      "('Reward: 43.0', ' | Episode', 414)\n",
      "('Reward: 23.0', ' | Episode', 406)\n",
      "('Reward: 20.0', ' | Episode', 407)\n",
      "('Reward: 36.0', ' | Episode', 415)\n",
      "('Reward: 19.0', ' | Episode', 408)\n",
      "('Reward: 31.0', ' | Episode', 416)\n",
      "('Reward: 19.0', ' | Episode', 409)\n",
      "('Reward: 30.0', ' | Episode', 417)('Reward: 19.0', ' | Episode', 410)\n",
      "\n",
      "('Reward: 26.0', ' | Episode', 411)\n",
      "('Reward: 36.0', ' | Episode', 418)\n",
      "('Reward: 43.0', ' | Episode', 412)\n",
      "('Reward: 46.0', ' | Episode', 419)\n",
      "('Reward: 28.0', ' | Episode', 413)\n",
      "('Reward: 22.0', ' | Episode', 414)\n",
      "('Reward: 46.0', ' | Episode', 420)\n",
      "('Reward: 13.0', ' | Episode', 415)\n",
      "('Reward: 13.0', ' | Episode', 416)\n",
      "('Reward: 27.0', ' | Episode', 421)\n",
      "('Reward: 11.0', ' | Episode', 417)\n",
      "('Reward: 22.0', ' | Episode', 422)\n",
      "('Reward: 39.0', ' | Episode', 418)\n",
      "('Reward: 23.0', ' | Episode', 419)\n",
      "('Reward: 26.0', ' | Episode', 420)\n",
      "('Reward: 74.0', ' | Episode', 423)\n",
      "('Reward: 21.0', ' | Episode', 421)\n",
      "('Reward: 11.0', ' | Episode', 424)\n",
      "('Reward: 21.0', ' | Episode', 422)\n",
      "('Reward: 9.0', ' | Episode', 423)\n",
      "('Reward: 37.0', ' | Episode', 424)\n",
      "('Reward: 70.0', ' | Episode', 425)('Reward: 11.0', ' | Episode', 425)\n",
      "\n",
      "('Reward: 16.0', ' | Episode', 426)\n",
      "('Reward: 22.0', ' | Episode', 426)\n",
      "('Reward: 36.0', ' | Episode', 427)\n",
      "('Reward: 12.0', ' | Episode', 428)\n",
      "('Reward: 46.0', ' | Episode', 427)\n",
      "('Reward: 10.0', ' | Episode', 429)\n",
      "('Reward: 28.0', ' | Episode', 430)\n",
      "('Reward: 34.0', ' | Episode', 431)\n",
      "('Reward: 68.0', ' | Episode', 428)\n",
      "('Reward: 16.0', ' | Episode', 429)\n",
      "('Reward: 47.0', ' | Episode', 432)\n",
      "('Reward: 35.0', ' | Episode', 433)\n",
      "('Reward: 49.0', ' | Episode', 430)\n",
      "('Reward: 18.0', ' | Episode', 434)\n",
      "('Reward: 13.0', ' | Episode', 431)\n",
      "('Reward: 36.0', ' | Episode', 432)\n",
      "('Reward: 40.0', ' | Episode', 435)\n",
      "('Reward: 35.0', ' | Episode', 436)\n",
      "('Reward: 12.0', ' | Episode', 437)\n",
      "('Reward: 15.0', ' | Episode', 438)\n",
      "('Reward: 9.0', ' | Episode', 439)\n",
      "('Reward: 83.0', ' | Episode', 433)\n",
      "('Reward: 39.0', ' | Episode', 440)\n",
      "('Reward: 18.0', ' | Episode', 434)\n",
      "('Reward: 17.0', ' | Episode', 441)\n",
      "('Reward: 79.0', ' | Episode', 435)\n",
      "('Reward: 58.0', ' | Episode', 442)\n",
      "('Reward: 9.0', ' | Episode', 436)\n",
      "('Reward: 27.0', ' | Episode', 437)\n",
      "('Reward: 42.0', ' | Episode', 443)\n",
      "('Reward: 40.0', ' | Episode', 438)\n",
      "('Reward: 39.0', ' | Episode', 444)\n",
      "('Reward: 20.0', ' | Episode', 445)\n",
      "('Reward: 24.0', ' | Episode', 446)\n",
      "('Reward: 19.0', ' | Episode', 447)\n",
      "('Reward: 47.0', ' | Episode', 439)\n",
      "('Reward: 13.0', ' | Episode', 440)\n",
      "('Reward: 34.0', ' | Episode', 448)\n",
      "('Reward: 69.0', ' | Episode', 449)\n",
      "('Reward: 78.0', ' | Episode', 441)\n",
      "('Reward: 13.0', ' | Episode', 442)\n",
      "('Reward: 25.0', ' | Episode', 450)\n",
      "('Reward: 18.0', ' | Episode', 443)\n",
      "('Reward: 19.0', ' | Episode', 444)\n",
      "('Reward: 35.0', ' | Episode', 451)\n",
      "('Reward: 11.0', ' | Episode', 445)\n",
      "('Reward: 11.0', ' | Episode', 452)\n",
      "('Reward: 74.0', ' | Episode', 446)\n",
      "('Reward: 72.0', ' | Episode', 453)\n",
      "('Reward: 33.0', ' | Episode', 447)\n",
      "('Reward: 50.0', ' | Episode', 454)\n",
      "('Reward: 66.0', ' | Episode', 448)\n",
      "('Reward: 15.0', ' | Episode', 449)\n",
      "('Reward: 55.0', ' | Episode', 455)\n",
      "('Reward: 16.0', ' | Episode', 450)\n",
      "('Reward: 43.0', ' | Episode', 456)\n",
      "('Reward: 32.0', ' | Episode', 451)\n",
      "('Reward: 44.0', ' | Episode', 457)\n",
      "('Reward: 34.0', ' | Episode', 452)\n",
      "('Reward: 21.0', ' | Episode', 453)('Reward: 21.0', ' | Episode', 458)\n",
      "\n",
      "('Reward: 12.0', ' | Episode', 454)\n",
      " ('Reward: 16.0', ' | Episode', 459)\n",
      "('Reward: 47.0', ' | Episode', 460)\n",
      "('Reward: 58.0', ' | Episode', 461)\n",
      "('Reward: 102.0', ' | Episode', 455)\n",
      "('Reward: 84.0', ' | Episode', 462)\n",
      "('Reward: 76.0', ' | Episode', 456)\n",
      "('Reward: 33.0', ' | Episode', 463)\n",
      "('Reward: 36.0', ' | Episode', 457)\n",
      "('Reward: 13.0', ' | Episode', 464)\n",
      "('Reward: 22.0', ' | Episode', 458)\n",
      "('Reward: 25.0', ' | Episode', 465)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Reward: 41.0', ' | Episode', 459)\n",
      "('Reward: 68.0', ' | Episode', 466)\n",
      "('Reward: 39.0', ' | Episode', 460)\n",
      "('Reward: 11.0', ' | Episode', 461)\n",
      "('Reward: 12.0', ' | Episode', 462)\n",
      "('Reward: 36.0', ' | Episode', 467)\n",
      "('Reward: 18.0', ' | Episode', 463)\n",
      "('Reward: 40.0', ' | Episode', 468)\n",
      "('Reward: 28.0', ' | Episode', 464)\n",
      "('Reward: 23.0', ' | Episode', 469)\n",
      "('Reward: 23.0', ' | Episode', 465)\n",
      "('Reward: 24.0', ' | Episode', 466)\n",
      "('Reward: 61.0', ' | Episode', 470)\n",
      "('Reward: 29.0', ' | Episode', 471)\n",
      "('Reward: 71.0', ' | Episode', 467)\n",
      "('Reward: 12.0', ' | Episode', 468)\n",
      "('Reward: 34.0', ' | Episode', 472)\n",
      "('Reward: 15.0', ' | Episode', 473)\n",
      "('Reward: 23.0', ' | Episode', 469)\n",
      "('Reward: 13.0', ' | Episode', 474)\n",
      "('Reward: 9.0', ' | Episode', 475)\n",
      "('Reward: 25.0', ' | Episode', 470)\n",
      "('Reward: 28.0', ' | Episode', 471)\n",
      "('Reward: 16.0', ' | Episode', 472)\n",
      "('Reward: 46.0', ' | Episode', 476)\n",
      "('Reward: 15.0', ' | Episode', 473)\n",
      "('Reward: 20.0', ' | Episode', 474)\n",
      "('Reward: 22.0', ' | Episode', 477)\n",
      "('Reward: 13.0', ' | Episode', 475)\n",
      "('Reward: 17.0', ' | Episode', 476)\n",
      "('Reward: 52.0', ' | Episode', 478)\n",
      "('Reward: 30.0', ' | Episode', 477)\n",
      "('Reward: 37.0', ' | Episode', 479)\n",
      "('Reward: 25.0', ' | Episode', 478)\n",
      "('Reward: 14.0', ' | Episode', 479)\n",
      "('Reward: 35.0', ' | Episode', 480)\n",
      "('Reward: 59.0', ' | Episode', 480)\n",
      "('Reward: 22.0', ' | Episode', 481)\n",
      "('Reward: 14.0', ' | Episode', 482)\n",
      "('Reward: 54.0', ' | Episode', 481)\n",
      "('Reward: 33.0', ' | Episode', 482)\n",
      "('Reward: 69.0', ' | Episode', 483)\n",
      "('Reward: 40.0', ' | Episode', 484)\n",
      "('Reward: 68.0', ' | Episode', 483)\n",
      "('Reward: 15.0', ' | Episode', 485)\n",
      "('Reward: 40.0', ' | Episode', 484)\n",
      "('Reward: 29.0', ' | Episode', 486)\n",
      "('Reward: 26.0', ' | Episode', 485)\n",
      "('Reward: 21.0', ' | Episode', 487)\n",
      "('Reward: 14.0', ' | Episode', 486)\n",
      "('Reward: 31.0', ' | Episode', 487)\n",
      "('Reward: 14.0', ' | Episode', 488)\n",
      "('Reward: 42.0', ' | Episode', 488)\n",
      "('Reward: 12.0', ' | Episode', 489)\n",
      "('Reward: 36.0', ' | Episode', 489)\n",
      "('Reward: 34.0', ' | Episode', 490)\n",
      "('Reward: 49.0', ' | Episode', 490)\n",
      "('Reward: 27.0', ' | Episode', 491)\n",
      "('Reward: 27.0', ' | Episode', 491)\n",
      "('Reward: 43.0', ' | Episode', 492)\n",
      "('Reward: 25.0', ' | Episode', 492)\n",
      "('Reward: 32.0', ' | Episode', 493)\n",
      "('Reward: 32.0', ' | Episode', 494)\n",
      "('Reward: 47.0', ' | Episode', 493)\n",
      "('Reward: 17.0', ' | Episode', 494)\n",
      "('Reward: 32.0', ' | Episode', 495)\n",
      "('Reward: 28.0', ' | Episode', 496)\n",
      "('Reward: 11.0', ' | Episode', 497)\n",
      "('Reward: 73.0', ' | Episode', 495)\n",
      "('Reward: 17.0', ' | Episode', 498)\n",
      "('Reward: 21.0', ' | Episode', 496)\n",
      "('Reward: 16.0', ' | Episode', 499)\n",
      "('Reward: 30.0', ' | Episode', 500)\n",
      "('Reward: 48.0', ' | Episode', 497)\n",
      "('Reward: 14.0', ' | Episode', 501)\n",
      "('Reward: 32.0', ' | Episode', 498)\n",
      "('Reward: 21.0', ' | Episode', 499)\n",
      "('Reward: 41.0', ' | Episode', 502)\n",
      "('Reward: 30.0', ' | Episode', 500)\n",
      "('Reward: 35.0', ' | Episode', 503)\n",
      "('Reward: 10.0', ' | Episode', 504)\n",
      "('Reward: 32.0', ' | Episode', 501)\n",
      "('Reward: 42.0', ' | Episode', 505)\n",
      "('Reward: 52.0', ' | Episode', 502)\n",
      "('Reward: 57.0', ' | Episode', 506)\n",
      "('Reward: 60.0', ' | Episode', 503)\n",
      "('Reward: 80.0', ' | Episode', 507)\n",
      "('Reward: 84.0', ' | Episode', 504)\n",
      "('Reward: 45.0', ' | Episode', 508)\n",
      "('Reward: 29.0', ' | Episode', 505)\n",
      "('Reward: 13.0', ' | Episode', 509)\n",
      "('Reward: 45.0', ' | Episode', 506)\n",
      "('Reward: 55.0', ' | Episode', 510)\n",
      "('Reward: 46.0', ' | Episode', 511)\n",
      "('Reward: 86.0', ' | Episode', 507)\n",
      "('Reward: 14.0', ' | Episode', 512)\n",
      "('Reward: 41.0', ' | Episode', 508)\n",
      "('Reward: 29.0', ' | Episode', 513)\n",
      "('Reward: 20.0', ' | Episode', 514)\n",
      "('Reward: 31.0', ' | Episode', 509)\n",
      "('Reward: 44.0', ' | Episode', 515)\n",
      "('Reward: 51.0', ' | Episode', 510)\n",
      "('Reward: 24.0', ' | Episode', 511)\n",
      "('Reward: 42.0', ' | Episode', 516)\n",
      "('Reward: 27.0', ' | Episode', 517)\n",
      "('Reward: 34.0', ' | Episode', 512)\n",
      "('Reward: 33.0', ' | Episode', 518)\n",
      "('Reward: 26.0', ' | Episode', 513)\n",
      "('Reward: 37.0', ' | Episode', 519)\n",
      "('Reward: 33.0', ' | Episode', 514)\n",
      "('Reward: 25.0', ' | Episode', 520)\n",
      "('Reward: 15.0', ' | Episode', 515)\n",
      "('Reward: 18.0', ' | Episode', 521)\n",
      "('Reward: 60.0', ' | Episode', 516)\n",
      "('Reward: 77.0', ' | Episode', 522)\n",
      "('Reward: 9.0', ' | Episode', 523)\n",
      "('Reward: 29.0', ' | Episode', 517)\n",
      "('Reward: 50.0', ' | Episode', 518)\n",
      "('Reward: 64.0', ' | Episode', 524)\n",
      "('Reward: 64.0', ' | Episode', 525)\n",
      "('Reward: 94.0', ' | Episode', 519)\n",
      "('Reward: 32.0', ' | Episode', 526)\n",
      "('Reward: 30.0', ' | Episode', 520)\n",
      "('Reward: 51.0', ' | Episode', 527)\n",
      "('Reward: 88.0', ' | Episode', 521)\n",
      "('Reward: 28.0', ' | Episode', 522)\n",
      "('Reward: 154.0', ' | Episode', 528)\n",
      "('Reward: 50.0', ' | Episode', 523)\n",
      "('Reward: 47.0', ' | Episode', 524)\n",
      "('Reward: 101.0', ' | Episode', 529)\n",
      "('Reward: 53.0', ' | Episode', 525)\n",
      "('Reward: 64.0', ' | Episode', 526)\n",
      "('Reward: 82.0', ' | Episode', 530)\n",
      "('Reward: 34.0', ' | Episode', 531)\n",
      "('Reward: 51.0', ' | Episode', 527)\n",
      "('Reward: 18.0', ' | Episode', 532)\n",
      "('Reward: 36.0', ' | Episode', 533)\n",
      "('Reward: 22.0', ' | Episode', 534)\n",
      "('Reward: 69.0', ' | Episode', 528)\n",
      "('Reward: 16.0', ' | Episode', 535)\n",
      "('Reward: 11.0', ' | Episode', 536)\n",
      "('Reward: 25.0', ' | Episode', 529)\n",
      "('Reward: 19.0', ' | Episode', 530)\n",
      "('Reward: 13.0', ' | Episode', 531)\n",
      "('Reward: 78.0', ' | Episode', 537)\n",
      "('Reward: 58.0', ' | Episode', 538)\n",
      "('Reward: 89.0', ' | Episode', 532)\n",
      "('Reward: 15.0', ' | Episode', 533)\n",
      "('Reward: 34.0', ' | Episode', 539)\n",
      "('Reward: 32.0', ' | Episode', 534)\n",
      "('Reward: 38.0', ' | Episode', 540)\n",
      "('Reward: 39.0', ' | Episode', 535)\n",
      "('Reward: 30.0', ' | Episode', 541)\n",
      "('Reward: 29.0', ' | Episode', 536)\n",
      "('Reward: 27.0', ' | Episode', 542)\n",
      "('Reward: 26.0', ' | Episode', 537)\n",
      "('Reward: 21.0', ' | Episode', 543)\n",
      "('Reward: 18.0', ' | Episode', 538)\n",
      "('Reward: 66.0', ' | Episode', 539)\n",
      "('Reward: 103.0', ' | Episode', 544)\n",
      "('Reward: 17.0', ' | Episode', 545)\n",
      "('Reward: 29.0', ' | Episode', 540)\n",
      "('Reward: 10.0', ' | Episode', 546)\n",
      "('Reward: 29.0', ' | Episode', 541)\n",
      "('Reward: 56.0', ' | Episode', 547)\n",
      "('Reward: 35.0', ' | Episode', 542)\n",
      "('Reward: 42.0', ' | Episode', 543)\n",
      "('Reward: 52.0', ' | Episode', 548)\n",
      "('Reward: 54.0', ' | Episode', 549)\n",
      "('Reward: 78.0', ' | Episode', 544)\n",
      "('Reward: 32.0', ' | Episode', 550)\n",
      "('Reward: 46.0', ' | Episode', 545)\n",
      "('Reward: 77.0', ' | Episode', 551)\n",
      "('Reward: 63.0', ' | Episode', 546)\n",
      "('Reward: 18.0', ' | Episode', 547)\n",
      "('Reward: 41.0', ' | Episode', 552)\n",
      "('Reward: 15.0', ' | Episode', 548)\n",
      "('Reward: 74.0', ' | Episode', 553)\n",
      "('Reward: 81.0', ' | Episode', 549)\n",
      "('Reward: 27.0', ' | Episode', 554)\n",
      "('Reward: 32.0', ' | Episode', 550)\n",
      "('Reward: 15.0', ' | Episode', 551)\n",
      "('Reward: 73.0', ' | Episode', 555)\n",
      "('Reward: 44.0', ' | Episode', 552)\n",
      "('Reward: 93.0', ' | Episode', 556)\n",
      "('Reward: 82.0', ' | Episode', 553)\n",
      "('Reward: 13.0', ' | Episode', 557)\n",
      "('Reward: 42.0', ' | Episode', 558)\n",
      "('Reward: 57.0', ' | Episode', 554)\n",
      "('Reward: 14.0', ' | Episode', 559)\n",
      "('Reward: 16.0', ' | Episode', 555)\n",
      "('Reward: 9.0', ' | Episode', 556)\n",
      "('Reward: 49.0', ' | Episode', 560)\n",
      "('Reward: 94.0', ' | Episode', 557)\n",
      "('Reward: 71.0', ' | Episode', 561)\n",
      "('Reward: 20.0', ' | Episode', 558)\n",
      "('Reward: 34.0', ' | Episode', 562)\n",
      "('Reward: 68.0', ' | Episode', 563)\n",
      "('Reward: 96.0', ' | Episode', 559)\n",
      "('Reward: 43.0', ' | Episode', 564)\n",
      "('Reward: 45.0', ' | Episode', 565)\n",
      "('Reward: 66.0', ' | Episode', 560)\n",
      "('Reward: 46.0', ' | Episode', 566)\n",
      "('Reward: 47.0', ' | Episode', 561)\n",
      "('Reward: 40.0', ' | Episode', 567)\n",
      "('Reward: 34.0', ' | Episode', 568)\n",
      " ('Reward: 49.0', ' | Episode', 562)\n",
      "('Reward: 20.0', ' | Episode', 569)\n",
      "('Reward: 49.0', ' | Episode', 563)\n",
      "('Reward: 14.0', ' | Episode', 564)\n",
      "('Reward: 49.0', ' | Episode', 570)\n",
      "('Reward: 27.0', ' | Episode', 571)\n",
      "('Reward: 34.0', ' | Episode', 572)\n",
      "('Reward: 74.0', ' | Episode', 565)\n",
      "('Reward: 36.0', ' | Episode', 573)\n",
      "('Reward: 50.0', ' | Episode', 574)\n",
      "('Reward: 65.0', ' | Episode', 566)\n",
      "('Reward: 29.0', ' | Episode', 567)\n",
      "('Reward: 43.0', ' | Episode', 575)\n",
      "('Reward: 16.0', ' | Episode', 568)\n",
      "('Reward: 30.0', ' | Episode', 576)\n",
      "('Reward: 48.0', ' | Episode', 569)\n",
      "('Reward: 36.0', ' | Episode', 577)\n",
      "('Reward: 33.0', ' | Episode', 570)\n",
      "('Reward: 10.0', ' | Episode', 571)\n",
      "('Reward: 31.0', ' | Episode', 578)\n",
      "('Reward: 27.0', ' | Episode', 572)\n",
      "('Reward: 35.0', ' | Episode', 579)\n",
      "('Reward: 12.0', ' | Episode', 573)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Reward: 46.0', ' | Episode', 574)\n",
      "('Reward: 63.0', ' | Episode', 580)\n",
      "('Reward: 38.0', ' | Episode', 575)\n",
      "('Reward: 42.0', ' | Episode', 581)\n",
      "('Reward: 59.0', ' | Episode', 576)\n",
      "('Reward: 56.0', ' | Episode', 582)\n",
      "('Reward: 40.0', ' | Episode', 577)\n",
      "('Reward: 17.0', ' | Episode', 578)\n",
      "('Reward: 54.0', ' | Episode', 583)\n",
      "('Reward: 52.0', ' | Episode', 584)\n",
      "('Reward: 67.0', ' | Episode', 579)\n",
      "('Reward: 84.0', ' | Episode', 585)\n",
      "('Reward: 91.0', ' | Episode', 580)\n",
      "('Reward: 36.0', ' | Episode', 586)\n",
      "('Reward: 34.0', ' | Episode', 581)\n",
      "('Reward: 24.0', ' | Episode', 587)\n",
      "('Reward: 49.0', ' | Episode', 588)\n",
      "('Reward: 99.0', ' | Episode', 582)\n",
      "('Reward: 20.0', ' | Episode', 589)\n",
      "('Reward: 9.0', ' | Episode', 590)\n",
      "('Reward: 25.0', ' | Episode', 583)\n",
      "('Reward: 13.0', ' | Episode', 584)\n",
      "('Reward: 19.0', ' | Episode', 591)\n",
      "('Reward: 17.0', ' | Episode', 585)\n",
      "('Reward: 11.0', ' | Episode', 586)\n",
      "('Reward: 68.0', ' | Episode', 592)\n",
      "('Reward: 32.0', ' | Episode', 593)\n",
      "('Reward: 106.0', ' | Episode', 587)\n",
      "('Reward: 39.0', ' | Episode', 594)\n",
      "('Reward: 74.0', ' | Episode', 588)\n",
      "('Reward: 67.0', ' | Episode', 595)\n",
      "('Reward: 28.0', ' | Episode', 589)\n",
      "('Reward: 61.0', ' | Episode', 590)\n",
      "('Reward: 82.0', ' | Episode', 596)\n",
      "('Reward: 43.0', ' | Episode', 597)\n",
      "('Reward: 32.0', ' | Episode', 598)\n",
      "('Reward: 16.0', ' | Episode', 599)\n",
      "('Reward: 19.0', ' | Episode', 600)\n",
      "('Reward: 124.0', ' | Episode', 591)\n",
      "('Reward: 16.0', ' | Episode', 592)\n",
      "('Reward: 36.0', ' | Episode', 601)\n",
      "('Reward: 9.0', ' | Episode', 593)\n",
      "('Reward: 32.0', ' | Episode', 594)\n",
      "('Reward: 24.0', ' | Episode', 602)\n",
      "('Reward: 46.0', ' | Episode', 595)\n",
      "('Reward: 54.0', ' | Episode', 603)\n",
      "('Reward: 28.0', ' | Episode', 596)\n",
      "('Reward: 53.0', ' | Episode', 604)\n",
      "('Reward: 62.0', ' | Episode', 597)\n",
      "('Reward: 30.0', ' | Episode', 605)\n",
      "('Reward: 12.0', ' | Episode', 606)\n",
      "('Reward: 15.0', ' | Episode', 607)\n",
      "('Reward: 20.0', ' | Episode', 608)\n",
      "('Reward: 56.0', ' | Episode', 598)\n",
      "('Reward: 36.0', ' | Episode', 599)\n",
      "('Reward: 61.0', ' | Episode', 609)\n",
      "('Reward: 57.0', ' | Episode', 600)\n",
      "('Reward: 58.0', ' | Episode', 610)\n",
      "('Reward: 27.0', ' | Episode', 611)\n",
      "('Reward: 67.0', ' | Episode', 601)\n",
      "('Reward: 18.0', ' | Episode', 612)\n",
      "('Reward: 46.0', ' | Episode', 602)\n",
      "('Reward: 49.0', ' | Episode', 613)\n",
      "('Reward: 57.0', ' | Episode', 603)\n",
      "('Reward: 65.0', ' | Episode', 614)\n",
      "('Reward: 18.0', ' | Episode', 615)\n",
      "('Reward: 38.0', ' | Episode', 604)\n",
      "('Reward: 23.0', ' | Episode', 616)\n",
      "('Reward: 33.0', ' | Episode', 617)\n",
      "('Reward: 83.0', ' | Episode', 605)\n",
      "('Reward: 29.0', ' | Episode', 606)\n",
      "('Reward: 86.0', ' | Episode', 618)\n",
      "('Reward: 12.0', ' | Episode', 619)\n",
      "('Reward: 29.0', ' | Episode', 620)\n",
      "('Reward: 75.0', ' | Episode', 607)\n",
      "('Reward: 70.0', ' | Episode', 621)\n",
      "('Reward: 40.0', ' | Episode', 608)\n",
      "('Reward: 14.0', ' | Episode', 609)('Reward: 21.0', ' | Episode', 622)\n",
      "\n",
      "('Reward: 10.0', ' | Episode', 623)\n",
      "('Reward: 62.0', ' | Episode', 610)\n",
      "('Reward: 59.0', ' | Episode', 624)\n",
      "('Reward: 31.0', ' | Episode', 625)\n",
      "('Reward: 78.0', ' | Episode', 611)\n",
      "('Reward: 45.0', ' | Episode', 612)\n",
      "('Reward: 80.0', ' | Episode', 626)\n",
      "('Reward: 13.0', ' | Episode', 613)\n",
      "('Reward: 43.0', ' | Episode', 627)\n",
      "('Reward: 64.0', ' | Episode', 614)\n",
      "('Reward: 12.0', ' | Episode', 628)\n",
      "('Reward: 38.0', ' | Episode', 629)\n",
      "('Reward: 12.0', ' | Episode', 630)\n",
      "('Reward: 54.0', ' | Episode', 615)\n",
      "('Reward: 18.0', ' | Episode', 616)\n",
      "('Reward: 30.0', ' | Episode', 617)\n",
      "('Reward: 13.0', ' | Episode', 618)\n",
      "('Reward: 70.0', ' | Episode', 631)\n",
      "('Reward: 39.0', ' | Episode', 619)\n",
      "('Reward: 25.0', ' | Episode', 632)\n",
      "('Reward: 19.0', ' | Episode', 633)\n",
      "('Reward: 62.0', ' | Episode', 634)\n",
      "('Reward: 89.0', ' | Episode', 620)\n",
      "('Reward: 81.0', ' | Episode', 635)\n",
      "('Reward: 26.0', ' | Episode', 636)\n",
      "('Reward: 82.0', ' | Episode', 621)\n",
      "('Reward: 59.0', ' | Episode', 622)\n",
      "('Reward: 71.0', ' | Episode', 637)\n",
      "('Reward: 29.0', ' | Episode', 623)\n",
      "('Reward: 31.0', ' | Episode', 638)\n",
      "('Reward: 30.0', ' | Episode', 624)\n",
      "('Reward: 45.0', ' | Episode', 639)\n",
      "('Reward: 39.0', ' | Episode', 625)\n",
      "('Reward: 37.0', ' | Episode', 640)\n",
      "('Reward: 43.0', ' | Episode', 626)\n",
      "('Reward: 22.0', ' | Episode', 627)\n",
      "('Reward: 35.0', ' | Episode', 628)\n",
      "('Reward: 9.0', ' | Episode', 629)\n",
      "('Reward: 96.0', ' | Episode', 641)\n",
      "('Reward: 25.0', ' | Episode', 630)\n",
      "('Reward: 30.0', ' | Episode', 631)\n",
      "('Reward: 9.0', ' | Episode', 632)\n",
      "('Reward: 89.0', ' | Episode', 642)\n",
      "('Reward: 52.0', ' | Episode', 633)\n",
      "('Reward: 37.0', ' | Episode', 634)\n",
      "('Reward: 71.0', ' | Episode', 643)\n",
      "('Reward: 25.0', ' | Episode', 644)\n",
      "('Reward: 52.0', ' | Episode', 635)\n",
      "('Reward: 44.0', ' | Episode', 636)\n",
      "('Reward: 58.0', ' | Episode', 645)\n",
      "('Reward: 34.0', ' | Episode', 637)\n",
      "('Reward: 60.0', ' | Episode', 638)\n",
      "('Reward: 100.0', ' | Episode', 646)\n",
      "('Reward: 39.0', ' | Episode', 639)\n",
      "('Reward: 34.0', ' | Episode', 647)\n",
      "('Reward: 66.0', ' | Episode', 640)\n",
      "('Reward: 61.0', ' | Episode', 648)\n",
      "('Reward: 54.0', ' | Episode', 649)\n",
      "('Reward: 83.0', ' | Episode', 641)\n",
      "('Reward: 24.0', ' | Episode', 642)\n",
      "('Reward: 55.0', ' | Episode', 650)\n",
      "('Reward: 48.0', ' | Episode', 643)\n",
      "('Reward: 43.0', ' | Episode', 651)\n",
      "('Reward: 31.0', ' | Episode', 644)\n",
      "('Reward: 32.0', ' | Episode', 652)\n",
      "('Reward: 59.0', ' | Episode', 645)\n",
      "('Reward: 49.0', ' | Episode', 653)\n",
      "('Reward: 41.0', ' | Episode', 646)\n",
      "('Reward: 45.0', ' | Episode', 647)\n",
      "('Reward: 75.0', ' | Episode', 654)\n",
      "('Reward: 42.0', ' | Episode', 648)\n",
      "('Reward: 59.0', ' | Episode', 655)\n",
      "('Reward: 22.0', ' | Episode', 656)\n",
      "('Reward: 60.0', ' | Episode', 649)\n",
      "('Reward: 40.0', ' | Episode', 650)\n",
      "('Reward: 51.0', ' | Episode', 657)\n",
      "('Reward: 23.0', ' | Episode', 651)\n",
      "('Reward: 94.0', ' | Episode', 652)\n",
      "('Reward: 32.0', ' | Episode', 653)\n",
      "('Reward: 114.0', ' | Episode', 658)\n",
      "('Reward: 54.0', ' | Episode', 654)\n",
      "('Reward: 39.0', ' | Episode', 659)\n",
      "('Reward: 74.0', ' | Episode', 660)\n",
      "('Reward: 116.0', ' | Episode', 655)\n",
      "('Reward: 18.0', ' | Episode', 656)\n",
      "('Reward: 53.0', ' | Episode', 661)\n",
      "('Reward: 55.0', ' | Episode', 657)\n",
      "('Reward: 50.0', ' | Episode', 662)\n",
      "('Reward: 48.0', ' | Episode', 663)\n",
      "('Reward: 66.0', ' | Episode', 658)\n",
      "('Reward: 44.0', ' | Episode', 659)\n",
      "('Reward: 84.0', ' | Episode', 664)\n",
      "('Reward: 44.0', ' | Episode', 660)\n",
      "('Reward: 22.0', ' | Episode', 661)\n",
      "('Reward: 53.0', ' | Episode', 665)\n",
      "('Reward: 28.0', ' | Episode', 662)\n",
      "('Reward: 23.0', ' | Episode', 666)\n",
      "('Reward: 31.0', ' | Episode', 663)\n",
      "('Reward: 30.0', ' | Episode', 664)\n",
      "('Reward: 74.0', ' | Episode', 667)\n",
      "('Reward: 49.0', ' | Episode', 665)\n",
      "('Reward: 65.0', ' | Episode', 668)\n",
      "('Reward: 66.0', ' | Episode', 666)\n",
      "('Reward: 25.0', ' | Episode', 667)\n",
      "('Reward: 118.0', ' | Episode', 669)\n",
      "('Reward: 73.0', ' | Episode', 668)\n",
      "('Reward: 36.0', ' | Episode', 670)\n",
      "('Reward: 18.0', ' | Episode', 669)\n",
      "('Reward: 42.0', ' | Episode', 670)\n",
      "('Reward: 93.0', ' | Episode', 671)\n",
      "('Reward: 37.0', ' | Episode', 671)\n",
      "('Reward: 22.0', ' | Episode', 672)\n",
      "('Reward: 14.0', ' | Episode', 672)\n",
      "('Reward: 14.0', ' | Episode', 673)\n",
      "('Reward: 12.0', ' | Episode', 673)\n",
      "('Reward: 60.0', ' | Episode', 674)\n",
      "('Reward: 20.0', ' | Episode', 675)\n",
      "('Reward: 85.0', ' | Episode', 674)\n",
      "('Reward: 64.0', ' | Episode', 676)\n",
      "('Reward: 50.0', ' | Episode', 675)\n",
      "('Reward: 45.0', ' | Episode', 676)\n",
      "('Reward: 64.0', ' | Episode', 677)\n",
      "('Reward: 75.0', ' | Episode', 677)\n",
      "('Reward: 88.0', ' | Episode', 678)\n",
      "('Reward: 135.0', ' | Episode', 678)\n",
      "('Reward: 51.0', ' | Episode', 679)\n",
      "('Reward: 52.0', ' | Episode', 679)\n",
      "('Reward: 62.0', ' | Episode', 680)\n",
      "('Reward: 80.0', ' | Episode', 680)\n",
      "('Reward: 40.0', ' | Episode', 681)\n",
      "('Reward: 10.0', ' | Episode', 682)\n",
      "('Reward: 41.0', ' | Episode', 683)\n",
      "('Reward: 77.0', ' | Episode', 684)\n",
      "('Reward: 151.0', ' | Episode', 681)\n",
      "('Reward: 36.0', ' | Episode', 682)\n",
      "('Reward: 65.0', ' | Episode', 685)\n",
      "('Reward: 60.0', ' | Episode', 683)\n",
      "('Reward: 74.0', ' | Episode', 686)\n",
      "('Reward: 62.0', ' | Episode', 684)\n",
      "('Reward: 48.0', ' | Episode', 687)\n",
      "('Reward: 65.0', ' | Episode', 685)\n",
      "('Reward: 73.0', ' | Episode', 688)\n",
      "('Reward: 35.0', ' | Episode', 686)\n",
      "('Reward: 43.0', ' | Episode', 687)\n",
      "('Reward: 56.0', ' | Episode', 689)\n",
      "('Reward: 24.0', ' | Episode', 688)\n",
      "('Reward: 20.0', ' | Episode', 689)\n",
      "('Reward: 78.0', ' | Episode', 690)\n",
      "('Reward: 54.0', ' | Episode', 690)\n",
      "('Reward: 41.0', ' | Episode', 691)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Reward: 40.0', ' | Episode', 692)\n",
      "('Reward: 92.0', ' | Episode', 691)\n",
      "('Reward: 37.0', ' | Episode', 692)\n",
      "('Reward: 69.0', ' | Episode', 693)\n",
      "('Reward: 63.0', ' | Episode', 694)\n",
      "('Reward: 76.0', ' | Episode', 693)\n",
      "('Reward: 64.0', ' | Episode', 695)\n",
      "('Reward: 33.0', ' | Episode', 696)\n",
      "('Reward: 87.0', ' | Episode', 694)\n",
      "('Reward: 79.0', ' | Episode', 697)\n",
      "('Reward: 104.0', ' | Episode', 695)\n",
      "('Reward: 49.0', ' | Episode', 696)\n",
      "('Reward: 104.0', ' | Episode', 698)\n",
      "('Reward: 36.0', ' | Episode', 697)\n",
      "('Reward: 21.0', ' | Episode', 698)\n",
      "('Reward: 38.0', ' | Episode', 699)\n",
      "('Reward: 55.0', ' | Episode', 699)\n",
      "('Reward: 102.0', ' | Episode', 700)\n",
      "('Reward: 61.0', ' | Episode', 700)\n",
      "('Reward: 28.0', ' | Episode', 701)\n",
      "('Reward: 55.0', ' | Episode', 702)\n",
      "('Reward: 101.0', ' | Episode', 701)\n",
      "('Reward: 45.0', ' | Episode', 703)\n",
      "('Reward: 25.0', ' | Episode', 704)\n",
      "('Reward: 80.0', ' | Episode', 702)\n",
      "('Reward: 44.0', ' | Episode', 705)\n",
      "('Reward: 50.0', ' | Episode', 703)\n",
      "('Reward: 72.0', ' | Episode', 706)\n",
      "('Reward: 59.0', ' | Episode', 707)\n",
      "('Reward: 31.0', ' | Episode', 708)\n",
      "('Reward: 163.0', ' | Episode', 704)\n",
      "('Reward: 63.0', ' | Episode', 709)\n",
      "('Reward: 34.0', ' | Episode', 705)\n",
      "('Reward: 22.0', ' | Episode', 710)\n",
      "('Reward: 22.0', ' | Episode', 711)\n",
      "('Reward: 67.0', ' | Episode', 706)\n",
      "('Reward: 30.0', ' | Episode', 712)\n",
      "('Reward: 61.0', ' | Episode', 707)\n",
      "('Reward: 89.0', ' | Episode', 713)\n",
      "('Reward: 35.0', ' | Episode', 714)\n",
      "('Reward: 47.0', ' | Episode', 715)\n",
      "('Reward: 16.0', ' | Episode', 716)\n",
      "('Reward: 61.0', ' | Episode', 717)\n",
      "('Reward: 200.0', ' | Episode', 708)\n",
      "('Reward: 29.0', ' | Episode', 709)\n",
      "('Reward: 40.0', ' | Episode', 710)\n",
      "('Reward: 106.0', ' | Episode', 718)\n",
      "('Reward: 93.0', ' | Episode', 711)\n",
      "('Reward: 56.0', ' | Episode', 719)\n",
      "('Reward: 99.0', ' | Episode', 712)\n",
      "('Reward: 41.0', ' | Episode', 713)\n",
      "('Reward: 131.0', ' | Episode', 720)\n",
      "('Reward: 94.0', ' | Episode', 714)\n",
      "('Reward: 112.0', ' | Episode', 721)\n",
      "('Reward: 47.0', ' | Episode', 715)\n",
      "('Reward: 21.0', ' | Episode', 722)\n",
      "('Reward: 43.0', ' | Episode', 716)\n",
      "('Reward: 13.0', ' | Episode', 717)\n",
      "('Reward: 53.0', ' | Episode', 723)\n",
      "('Reward: 42.0', ' | Episode', 718)\n",
      "('Reward: 54.0', ' | Episode', 724)\n",
      "('Reward: 50.0', ' | Episode', 719)\n",
      "('Reward: 84.0', ' | Episode', 725)\n",
      "('Reward: 121.0', ' | Episode', 720)\n",
      "('Reward: 99.0', ' | Episode', 721)\n",
      "('Reward: 156.0', ' | Episode', 726)\n",
      "('Reward: 31.0', ' | Episode', 722)\n",
      "('Reward: 21.0', ' | Episode', 723)\n",
      "('Reward: 79.0', ' | Episode', 727)\n",
      "('Reward: 55.0', ' | Episode', 724)\n",
      "('Reward: 19.0', ' | Episode', 725)\n",
      "('Reward: 66.0', ' | Episode', 728)\n",
      "('Reward: 52.0', ' | Episode', 726)\n",
      "('Reward: 116.0', ' | Episode', 727)\n",
      "('Reward: 127.0', ' | Episode', 729)\n",
      "('Reward: 82.0', ' | Episode', 728)\n",
      "('Reward: 51.0', ' | Episode', 730)\n",
      "('Reward: 45.0', ' | Episode', 729)\n",
      "('Reward: 20.0', ' | Episode', 730)\n",
      "('Reward: 20.0', ' | Episode', 731)\n",
      "('Reward: 132.0', ' | Episode', 731)\n",
      "('Reward: 103.0', ' | Episode', 732)\n",
      "('Reward: 39.0', ' | Episode', 733)\n",
      "('Reward: 130.0', ' | Episode', 732)\n",
      "('Reward: 51.0', ' | Episode', 734)\n",
      "('Reward: 17.0', ' | Episode', 733)\n",
      "('Reward: 35.0', ' | Episode', 735)\n",
      "('Reward: 10.0', ' | Episode', 736)\n",
      "('Reward: 73.0', ' | Episode', 734)\n",
      "('Reward: 58.0', ' | Episode', 737)\n",
      "('Reward: 38.0', ' | Episode', 735)\n",
      "('Reward: 26.0', ' | Episode', 736)\n",
      "('Reward: 22.0', ' | Episode', 737)\n",
      "('Reward: 108.0', ' | Episode', 738)\n",
      "('Reward: 79.0', ' | Episode', 738)\n",
      "('Reward: 74.0', ' | Episode', 739)\n",
      "('Reward: 31.0', ' | Episode', 740)\n",
      "('Reward: 125.0', ' | Episode', 739)\n",
      "('Reward: 94.0', ' | Episode', 741)\n",
      "('Reward: 36.0', ' | Episode', 742)\n",
      "('Reward: 73.0', ' | Episode', 740)\n",
      "('Reward: 74.0', ' | Episode', 743)\n",
      "('Reward: 67.0', ' | Episode', 744)\n",
      "('Reward: 150.0', ' | Episode', 741)\n",
      "('Reward: 117.0', ' | Episode', 745)\n",
      "('Reward: 26.0', ' | Episode', 746)\n",
      "('Reward: 123.0', ' | Episode', 742)\n",
      "('Reward: 36.0', ' | Episode', 747)\n",
      "('Reward: 59.0', ' | Episode', 743)\n",
      "('Reward: 78.0', ' | Episode', 748)\n",
      "('Reward: 56.0', ' | Episode', 744)\n",
      "('Reward: 54.0', ' | Episode', 749)\n",
      "('Reward: 26.0', ' | Episode', 750)\n",
      "('Reward: 131.0', ' | Episode', 745)\n",
      "('Reward: 70.0', ' | Episode', 751)\n",
      "('Reward: 64.0', ' | Episode', 746)\n",
      "('Reward: 85.0', ' | Episode', 752)\n",
      "('Reward: 17.0', ' | Episode', 753)\n",
      "('Reward: 108.0', ' | Episode', 747)\n",
      "('Reward: 87.0', ' | Episode', 754)\n",
      "('Reward: 66.0', ' | Episode', 748)\n",
      "('Reward: 62.0', ' | Episode', 755)\n",
      "('Reward: 85.0', ' | Episode', 756)\n",
      "('Reward: 140.0', ' | Episode', 749)\n",
      "('Reward: 12.0', ' | Episode', 750)\n",
      "('Reward: 39.0', ' | Episode', 757)\n",
      "('Reward: 38.0', ' | Episode', 758)\n",
      "('Reward: 200.0', ' | Episode', 751)\n",
      "('Reward: 190.0', ' | Episode', 759)\n",
      "('Reward: 45.0', ' | Episode', 752)\n",
      "('Reward: 39.0', ' | Episode', 760)\n",
      "('Reward: 51.0', ' | Episode', 761)\n",
      "('Reward: 115.0', ' | Episode', 753)\n",
      "('Reward: 23.0', ' | Episode', 754)\n",
      "('Reward: 63.0', ' | Episode', 762)\n",
      "('Reward: 52.0', ' | Episode', 763)\n",
      "('Reward: 152.0', ' | Episode', 755)\n",
      "('Reward: 81.0', ' | Episode', 764)\n",
      "('Reward: 65.0', ' | Episode', 756)\n",
      "('Reward: 105.0', ' | Episode', 765)\n",
      "('Reward: 84.0', ' | Episode', 757)\n",
      "('Reward: 43.0', ' | Episode', 766)\n",
      "('Reward: 16.0', ' | Episode', 767)\n",
      "('Reward: 48.0', ' | Episode', 758)\n",
      "('Reward: 44.0', ' | Episode', 768)\n",
      "('Reward: 32.0', ' | Episode', 759)\n",
      "('Reward: 76.0', ' | Episode', 769)\n",
      "('Reward: 85.0', ' | Episode', 760)\n",
      "('Reward: 66.0', ' | Episode', 770)\n",
      "('Reward: 29.0', ' | Episode', 771)\n",
      "('Reward: 24.0', ' | Episode', 772)\n",
      "('Reward: 92.0', ' | Episode', 761)\n",
      "('Reward: 60.0', ' | Episode', 773)\n",
      "('Reward: 64.0', ' | Episode', 762)\n",
      "('Reward: 88.0', ' | Episode', 774)\n",
      "('Reward: 72.0', ' | Episode', 763)\n",
      "('Reward: 57.0', ' | Episode', 764)\n",
      "('Reward: 90.0', ' | Episode', 775)\n",
      "('Reward: 80.0', ' | Episode', 765)\n",
      "('Reward: 62.0', ' | Episode', 776)\n",
      "('Reward: 25.0', ' | Episode', 777)\n",
      "('Reward: 38.0', ' | Episode', 766)\n",
      "('Reward: 29.0', ' | Episode', 778)\n",
      "('Reward: 33.0', ' | Episode', 767)\n",
      "('Reward: 55.0', ' | Episode', 768)\n",
      "('Reward: 31.0', ' | Episode', 769)\n",
      "('Reward: 129.0', ' | Episode', 779)\n",
      "('Reward: 78.0', ' | Episode', 770)\n",
      "('Reward: 69.0', ' | Episode', 771)\n",
      "('Reward: 142.0', ' | Episode', 780)\n",
      "('Reward: 83.0', ' | Episode', 772)\n",
      "('Reward: 88.0', ' | Episode', 781)\n",
      "('Reward: 179.0', ' | Episode', 773)\n",
      "('Reward: 141.0', ' | Episode', 782)\n",
      "('Reward: 83.0', ' | Episode', 774)\n",
      "('Reward: 62.0', ' | Episode', 775)\n",
      "('Reward: 144.0', ' | Episode', 783)\n",
      "('Reward: 92.0', ' | Episode', 776)\n",
      "('Reward: 111.0', ' | Episode', 784)\n",
      "('Reward: 65.0', ' | Episode', 785)\n",
      "('Reward: 89.0', ' | Episode', 777)\n",
      "('Reward: 41.0', ' | Episode', 786)\n",
      "('Reward: 47.0', ' | Episode', 778)\n",
      "('Reward: 51.0', ' | Episode', 787)\n",
      "('Reward: 52.0', ' | Episode', 779)\n",
      "('Reward: 30.0', ' | Episode', 780)\n",
      "('Reward: 37.0', ' | Episode', 788)\n",
      "('Reward: 22.0', ' | Episode', 781)\n",
      "('Reward: 58.0', ' | Episode', 789)\n",
      "('Reward: 114.0', ' | Episode', 782)\n",
      "('Reward: 122.0', ' | Episode', 790)\n",
      "('Reward: 45.0', ' | Episode', 791)\n",
      "('Reward: 78.0', ' | Episode', 783)\n",
      "('Reward: 63.0', ' | Episode', 792)\n",
      "('Reward: 69.0', ' | Episode', 784)\n",
      "('Reward: 59.0', ' | Episode', 785)\n",
      "('Reward: 66.0', ' | Episode', 793)\n",
      "('Reward: 27.0', ' | Episode', 794)\n",
      "('Reward: 95.0', ' | Episode', 786)\n",
      "('Reward: 38.0', ' | Episode', 795)\n",
      "('Reward: 39.0', ' | Episode', 787)\n",
      "('Reward: 63.0', ' | Episode', 796)\n",
      "('Reward: 54.0', ' | Episode', 788)\n",
      "('Reward: 63.0', ' | Episode', 797)\n",
      "('Reward: 36.0', ' | Episode', 798)\n",
      "('Reward: 96.0', ' | Episode', 789)\n",
      "('Reward: 61.0', ' | Episode', 790)\n",
      "('Reward: 139.0', ' | Episode', 799)\n",
      "('Reward: 90.0', ' | Episode', 791)\n",
      "('Reward: 20.0', ' | Episode', 800)\n",
      "('Reward: 36.0', ' | Episode', 792)\n",
      "('Reward: 85.0', ' | Episode', 793)\n",
      "('Reward: 106.0', ' | Episode', 801)\n",
      "('Reward: 41.0', ' | Episode', 794)\n",
      "('Reward: 31.0', ' | Episode', 795)\n",
      "('Reward: 91.0', ' | Episode', 802)\n",
      "('Reward: 70.0', ' | Episode', 796)\n",
      "('Reward: 57.0', ' | Episode', 803)\n",
      "('Reward: 12.0', ' | Episode', 797)\n",
      "('Reward: 74.0', ' | Episode', 798)\n",
      "('Reward: 70.0', ' | Episode', 804)\n",
      "('Reward: 35.0', ' | Episode', 805)\n",
      "('Reward: 136.0', ' | Episode', 799)\n",
      "('Reward: 137.0', ' | Episode', 806)\n",
      "('Reward: 32.0', ' | Episode', 800)\n",
      "('Reward: 86.0', ' | Episode', 807)\n",
      "('Reward: 103.0', ' | Episode', 801)\n",
      "('Reward: 14.0', ' | Episode', 802)\n",
      "('Reward: 28.0', ' | Episode', 803)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Reward: 135.0', ' | Episode', 808)\n",
      "('Reward: 26.0', ' | Episode', 809)('Reward: 94.0', ' | Episode', 804)\n",
      "\n",
      "('Reward: 34.0', ' | Episode', 810)\n",
      "('Reward: 59.0', ' | Episode', 805)\n",
      "('Reward: 50.0', ' | Episode', 806)\n",
      "('Reward: 102.0', ' | Episode', 811)\n",
      "('Reward: 12.0', ' | Episode', 812)\n",
      "('Reward: 132.0', ' | Episode', 807)\n",
      "('Reward: 48.0', ' | Episode', 808)\n",
      "('Reward: 64.0', ' | Episode', 809)\n",
      "('Reward: 200.0', ' | Episode', 813)\n",
      "('Reward: 94.0', ' | Episode', 810)\n",
      "('Reward: 150.0', ' | Episode', 814)\n",
      "('Reward: 85.0', ' | Episode', 811)\n",
      "('Reward: 43.0', ' | Episode', 812)\n",
      "('Reward: 27.0', ' | Episode', 813)\n",
      "('Reward: 149.0', ' | Episode', 815)\n",
      "('Reward: 49.0', ' | Episode', 814)\n",
      "('Reward: 70.0', ' | Episode', 816)\n",
      "('Reward: 104.0', ' | Episode', 815)\n",
      "('Reward: 69.0', ' | Episode', 817)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-20:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/schnack/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/schnack/anaconda2/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"<ipython-input-14-4d76e79971fe>\", line 33, in <lambda>\n",
      "    worker_work = lambda: worker.work(GAMMA, sess, coord)\n",
      "  File \"<ipython-input-13-28a60037dcdd>\", line 119, in work\n",
      "    self.local_AC.state_in[1]: rnn_state[1]})\n",
      "  File \"/home/schnack/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 895, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/schnack/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/schnack/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"/home/schnack/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "CancelledError: Run call was cancelled\n",
      "Exception in thread Thread-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/schnack/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/schnack/anaconda2/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"<ipython-input-14-4d76e79971fe>\", line 33, in <lambda>\n",
      "    worker_work = lambda: worker.work(GAMMA, sess, coord)\n",
      "  File \"<ipython-input-13-28a60037dcdd>\", line 119, in work\n",
      "    self.local_AC.state_in[1]: rnn_state[1]})\n",
      "  File \"/home/schnack/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 895, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/schnack/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/schnack/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"/home/schnack/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "CancelledError: Run call was cancelled\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-44cda3e31e6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/schnack/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflags_passthrough\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-4d76e79971fe>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mworker_threads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/schnack/.local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.pyc\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, threads, stop_grace_period_secs, ignore_live_threads)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;31m# Wait for all threads to stop or for request_stop() to be called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/schnack/.local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.pyc\u001b[0m in \u001b[0;36mwait_for_stop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    309\u001b[0m       \u001b[0mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCoordinator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtold\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0mexpired\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mregister_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/schnack/anaconda2/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cond\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/schnack/anaconda2/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                     \u001b[0mdelay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0m_sleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgotit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feeling on how the results could look like; here is a graph of the 12 workers on our 8 kernel CPU that we used to train Pong on our server with the [universe starter agent](https://github.com/openai/universe-starter-agent). After ~4 hours of training we got an average score of +15. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/universea3c_pong_reward.png\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GA3C - GPU & CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/a3c_gpu.png\" width=650/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font>This topic was heavily influenced by this [paper](https://arxiv.org/pdf/1611.06256.pdf) with the related [implementation](https://github.com/NVlabs/GA3C). Our implementation is simplified of course but should give you an idea on how it works. We won't have exactly the same structure as in the picture above, but that's fine.   </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym, time, random, threading\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font> As you already can see we will use another framework: keras. It is already included in tensorflow and makes creating neural networks a bit easier. You could still implement them in raw tensorflow the same way. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-- constants\n",
    "ENV = 'CartPole-v0'\n",
    "\n",
    "RUN_TIME = 30 # How Long we let it run in s\n",
    "THREADS = 12 # Number of threads \n",
    "OPTIMIZERS = 2 # How many parallel optimizers that are working on the Brain\n",
    "THREAD_DELAY = 0.005 # Thread delay for more than 1 thread per kernel\n",
    "\n",
    "GAMMA = 0.99 # Discount factor\n",
    "\n",
    "N_STEP_RETURN = 8 # N-step return for discounted reward\n",
    "GAMMA_N = GAMMA ** N_STEP_RETURN # gamma ^n_step_return\n",
    "\n",
    "EPS_START = 0.4 # epsilon exploration\n",
    "EPS_STOP  = .15\n",
    "EPS_STEPS = 75000\n",
    "\n",
    "MIN_BATCH = 32 # minimal batch size for training\n",
    "LEARNING_RATE = 1e-2 # learning rate\n",
    "\n",
    "LOSS_V = .5 # v loss coefficient\n",
    "LOSS_ENTROPY = .01 # entropy coefficient\n",
    "\n",
    "frames = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    train_queue = [ [], [], [], [], [] ] # s, a, r, s', a' terminal mask\n",
    "    lock_queue = threading.Lock()\n",
    "\n",
    "    def __init__(self):\n",
    "        self.session = tf.Session()\n",
    "        K.set_session(self.session)\n",
    "        K.manual_variable_initialization(True)\n",
    "\n",
    "        self.model = self._build_model()\n",
    "        self.graph = self._build_graph(self.model)\n",
    "\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        self.default_graph = tf.get_default_graph()\n",
    "\n",
    "        self.default_graph.finalize() # avoid modifications\n",
    "\n",
    "    def _build_model(self):\n",
    "\n",
    "        l_input = Input( batch_shape=(None, NUM_STATE) )\n",
    "        l_dense = Dense(16, activation='relu')(l_input)\n",
    "\n",
    "        out_actions = Dense(NUM_ACTIONS, activation='softmax')(l_dense)\n",
    "        out_value   = Dense(1, activation='linear')(l_dense)\n",
    "\n",
    "        model = Model(inputs=[l_input], outputs=[out_actions, out_value])\n",
    "        model._make_predict_function() # have to initialize before threading\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _build_graph(self, model):\n",
    "        s_t = tf.placeholder(tf.float32, shape=(None, NUM_STATE))\n",
    "        a_t = tf.placeholder(tf.float32, shape=(None, NUM_ACTIONS))\n",
    "        r_t = tf.placeholder(tf.float32, shape=(None, 1)) # not immediate, but discounted n step reward\n",
    "\n",
    "        p, v = model(s_t)\n",
    "\n",
    "        log_prob = tf.log( tf.reduce_sum(p * a_t, axis=1, keep_dims=True) + 1e-10)\n",
    "        advantage = r_t - v\n",
    "\n",
    "        loss_policy = - log_prob * tf.stop_gradient(advantage)  # maximize policy\n",
    "        loss_value  = LOSS_V * tf.square(advantage) # minimize value error\n",
    "        entropy = LOSS_ENTROPY * tf.reduce_sum(p * tf.log(p + 1e-10), axis=1, keep_dims=True) # maximize entropy (regularization)\n",
    "\n",
    "        loss_total = tf.reduce_mean(loss_policy + loss_value + entropy)\n",
    "\n",
    "        optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, decay=.99)\n",
    "        minimize = optimizer.minimize(loss_total)\n",
    "\n",
    "        return s_t, a_t, r_t, minimize\n",
    "\n",
    "    def optimize(self):\n",
    "        if len(self.train_queue[0]) < MIN_BATCH:\n",
    "            time.sleep(0) # yield\n",
    "            return\n",
    "\n",
    "        with self.lock_queue:\n",
    "            if len(self.train_queue[0]) < MIN_BATCH: # more thread could have passed without lock\n",
    "                return  # we can't yield inside lock\n",
    "\n",
    "            s, a, r, s_, s_mask = self.train_queue\n",
    "            self.train_queue = [ [], [], [], [], [] ]\n",
    "\n",
    "        s = np.vstack(s)\n",
    "        a = np.vstack(a)\n",
    "        r = np.vstack(r)\n",
    "        s_ = np.vstack(s_)\n",
    "        s_mask = np.vstack(s_mask)\n",
    "\n",
    "        if len(s) > 5*MIN_BATCH: print(\"Optimizer alert! Minimizing batch of %d\" % len(s))\n",
    "\n",
    "        v = self.predict_v(s_)\n",
    "        r = r + GAMMA_N * v * s_mask  # set v to 0 where s_ is terminal state\n",
    "\n",
    "        s_t, a_t, r_t, minimize = self.graph\n",
    "        self.session.run(minimize, feed_dict={s_t: s, a_t: a, r_t: r})\n",
    "\n",
    "    def train_push(self, s, a, r, s_):\n",
    "        with self.lock_queue:\n",
    "            self.train_queue[0].append(s)\n",
    "            self.train_queue[1].append(a)\n",
    "            self.train_queue[2].append(r)\n",
    "\n",
    "            if s_ is None:\n",
    "                self.train_queue[3].append(NONE_STATE)\n",
    "                self.train_queue[4].append(0.)\n",
    "            else:\n",
    "                self.train_queue[3].append(s_)\n",
    "                self.train_queue[4].append(1.)\n",
    "\n",
    "    def predict(self, s):\n",
    "        with self.default_graph.as_default():\n",
    "            #Keras runs the graph once\n",
    "            p, v = self.model.predict(s)\n",
    "            return p, v\n",
    "\n",
    "    def predict_p(self, s):\n",
    "        with self.default_graph.as_default():\n",
    "            p, v = self.model.predict(s)\n",
    "            return p\n",
    "\n",
    "    def predict_v(self, s):\n",
    "        with self.default_graph.as_default():\n",
    "            p, v = self.model.predict(s)    \n",
    "            return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, eps_start, eps_end, eps_steps):\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end   = eps_end\n",
    "        self.eps_steps = eps_steps\n",
    "\n",
    "        self.memory = [] # used for n_step return\n",
    "        self.R = 0\n",
    "        \n",
    "    def getEpsilon(self):\n",
    "        if(frames >= self.eps_steps):\n",
    "            return self.eps_end\n",
    "        else:\n",
    "            return self.eps_start + frames * (self.eps_end - self.eps_start) / self.eps_steps\t# linearly interpolate\n",
    "\n",
    "    def act(self, s):\n",
    "        eps = self.getEpsilon()\t\t\t\n",
    "        global frames; frames = frames + 1\n",
    "\n",
    "        if random.random() < eps:\n",
    "            return random.randint(0, NUM_ACTIONS-1)\n",
    "\n",
    "        else:\n",
    "            s = np.array([s])\n",
    "            p = brain.predict_p(s)[0]\n",
    "\n",
    "            # a = np.argmax(p) # greedy\n",
    "            a = np.random.choice(NUM_ACTIONS, p=p)\n",
    "\n",
    "            return a\n",
    "\n",
    "    def train(self, s, a, r, s_):\n",
    "        def get_sample(memory, n):\n",
    "            s, a, _, _  = memory[0]\n",
    "            _, _, _, s_ = memory[n-1]\n",
    "\n",
    "            return s, a, self.R, s_\n",
    "\n",
    "        a_cats = np.zeros(NUM_ACTIONS)\t# turn action into one-hot representation\n",
    "        a_cats[a] = 1 \n",
    "\n",
    "        self.memory.append( (s, a_cats, r, s_) )\n",
    "\n",
    "        self.R = ( self.R + r * GAMMA_N ) / GAMMA\n",
    "\n",
    "        if s_ is None:\n",
    "            while len(self.memory) > 0:\n",
    "                n = len(self.memory)\n",
    "                s, a, r, s_ = get_sample(self.memory, n)\n",
    "                brain.train_push(s, a, r, s_)\n",
    "\n",
    "                self.R = ( self.R - self.memory[0][2] ) / GAMMA\n",
    "                self.memory.pop(0)\t\t\n",
    "\n",
    "            self.R = 0\n",
    "\n",
    "        if len(self.memory) >= N_STEP_RETURN:\n",
    "            s, a, r, s_ = get_sample(self.memory, N_STEP_RETURN)\n",
    "            brain.train_push(s, a, r, s_)\n",
    "\n",
    "            self.R = self.R - self.memory[0][2]\n",
    "            self.memory.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Environment(threading.Thread):\n",
    "    stop_signal = False\n",
    "\n",
    "    def __init__(self, render=False, eps_start=EPS_START, eps_end=EPS_STOP, eps_steps=EPS_STEPS):\n",
    "        threading.Thread.__init__(self)\n",
    "\n",
    "        self.render = render\n",
    "        self.env = gym.make(ENV)\n",
    "        self.agent = Agent(eps_start, eps_end, eps_steps)\n",
    "        self.episode_number = 0\n",
    "        \n",
    "    def runEpisode(self):\n",
    "        s = self.env.reset()\n",
    "        \n",
    "        R = 0\n",
    "        while True:         \n",
    "            time.sleep(THREAD_DELAY) # yield \n",
    "\n",
    "            if self.render: self.env.render()\n",
    "\n",
    "            a = self.agent.act(s)\n",
    "            s_, r, done, info = self.env.step(a)\n",
    "\n",
    "            if done: # terminal state\n",
    "                s_ = None\n",
    "\n",
    "            self.agent.train(s, a, r, s_)\n",
    "\n",
    "            s = s_\n",
    "            R += r\n",
    "\n",
    "            if done or self.stop_signal:\n",
    "                break\n",
    "                \n",
    "        self.episode_number += 1\n",
    "        print(self.episode_number, \" | Total Reward:\", R)\n",
    "\n",
    "    def run(self):\n",
    "        while not self.stop_signal:\n",
    "            self.runEpisode()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_signal = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Optimizer(threading.Thread):\n",
    "    stop_signal = False\n",
    "\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "\n",
    "    def run(self):\n",
    "        while not self.stop_signal:\n",
    "            brain.optimize()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_signal = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-21 17:05:07,421] Making new env: CartPole-v0\n",
      "[2017-09-21 17:05:07,813] Making new env: CartPole-v0\n",
      "[2017-09-21 17:05:07,816] Making new env: CartPole-v0\n",
      "[2017-09-21 17:05:07,822] Making new env: CartPole-v0\n",
      "[2017-09-21 17:05:07,825] Making new env: CartPole-v0\n",
      "[2017-09-21 17:05:07,828] Making new env: CartPole-v0\n",
      "[2017-09-21 17:05:07,832] Making new env: CartPole-v0\n",
      "[2017-09-21 17:05:07,835] Making new env: CartPole-v0\n",
      "[2017-09-21 17:05:07,838] Making new env: CartPole-v0\n",
      "[2017-09-21 17:05:07,841] Making new env: CartPole-v0\n",
      "[2017-09-21 17:05:07,845] Making new env: CartPole-v0\n",
      "[2017-09-21 17:05:07,848] Making new env: CartPole-v0\n",
      "[2017-09-21 17:05:07,851] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, ' | Total Reward:', 15.0)\n",
      "(1, ' | Total Reward:', 15.0)\n",
      "(1, ' | Total Reward:', 16.0)\n",
      "(1, ' | Total Reward:', 17.0)\n",
      "(1, ' | Total Reward:', 12.0)\n",
      "(1, ' | Total Reward:', 20.0)\n",
      "(1, ' | Total Reward:', 16.0)\n",
      "(1, ' | Total Reward:', 21.0)\n",
      "(1, ' | Total Reward:', 23.0)\n",
      "(2, ' | Total Reward:', 11.0)\n",
      "(2, ' | Total Reward:', 13.0)\n",
      "(2, ' | Total Reward:', 23.0)\n",
      "(1, ' | Total Reward:', 33.0)\n",
      "(3, ' | Total Reward:', 17.0)\n",
      "(3, ' | Total Reward:', 17.0)\n",
      "(2, ' | Total Reward:', 30.0)\n",
      "(2, ' | Total Reward:', 28.0)\n",
      "(1, ' | Total Reward:', 39.0)\n",
      "(2, ' | Total Reward:', 11.0)\n",
      "(2, ' | Total Reward:', 44.0)\n",
      "(3, ' | Total Reward:', 12.0)\n",
      "(2, ' | Total Reward:', 38.0)\n",
      "(3, ' | Total Reward:', 14.0)(1, ' | Total Reward:', 52.0)\n",
      "\n",
      "(3, ' | Total Reward:', 23.0)\n",
      "(2, ' | Total Reward:', 36.0)\n",
      "(2, ' | Total Reward:', 17.0)\n",
      "(2, ' | Total Reward:', 44.0)\n",
      "(4, ' | Total Reward:', 27.0)\n",
      "(3, ' | Total Reward:', 14.0)\n",
      "(4, ' | Total Reward:', 12.0)\n",
      "(3, ' | Total Reward:', 24.0)\n",
      "(5, ' | Total Reward:', 10.0)\n",
      "(4, ' | Total Reward:', 26.0)\n",
      "(4, ' | Total Reward:', 43.0)\n",
      "(4, ' | Total Reward:', 30.0)\n",
      "(3, ' | Total Reward:', 38.0)\n",
      "(3, ' | Total Reward:', 29.0)\n",
      "(3, ' | Total Reward:', 26.0)\n",
      "(4, ' | Total Reward:', 21.0)\n",
      "(5, ' | Total Reward:', 14.0)\n",
      "(5, ' | Total Reward:', 10.0)\n",
      "(5, ' | Total Reward:', 14.0)\n",
      "(4, ' | Total Reward:', 24.0)\n",
      "(5, ' | Total Reward:', 27.0)\n",
      "(4, ' | Total Reward:', 13.0)\n",
      "(6, ' | Total Reward:', 28.0)\n",
      "(4, ' | Total Reward:', 19.0)\n",
      "(5, ' | Total Reward:', 18.0)\n",
      "(6, ' | Total Reward:', 22.0)\n",
      "(6, ' | Total Reward:', 20.0)\n",
      "(6, ' | Total Reward:', 24.0)\n",
      "(5, ' | Total Reward:', 13.0)\n",
      "(6, ' | Total Reward:', 27.0)\n",
      "(6, ' | Total Reward:', 12.0)\n",
      "(5, ' | Total Reward:', 31.0)\n",
      "(7, ' | Total Reward:', 26.0)\n",
      "(7, ' | Total Reward:', 14.0)\n",
      "(6, ' | Total Reward:', 16.0)\n",
      "(7, ' | Total Reward:', 27.0)\n",
      "(4, ' | Total Reward:', 54.0)(7, ' | Total Reward:', 21.0)\n",
      "\n",
      "(8, ' | Total Reward:', 14.0)\n",
      "(6, ' | Total Reward:', 17.0)\n",
      "(7, ' | Total Reward:', 24.0)\n",
      "(5, ' | Total Reward:', 49.0)\n",
      "(7, ' | Total Reward:', 24.0)\n",
      "(3, ' | Total Reward:', 82.0)\n",
      "(8, ' | Total Reward:', 12.0)(7, ' | Total Reward:', 17.0)\n",
      "\n",
      "(2, ' | Total Reward:', 93.0)\n",
      "(9, ' | Total Reward:', 16.0)\n",
      "(6, ' | Total Reward:', 15.0)\n",
      "(3, ' | Total Reward:', 10.0)\n",
      "(8, ' | Total Reward:', 33.0)\n",
      "(9, ' | Total Reward:', 20.0)\n",
      "(10, ' | Total Reward:', 14.0)\n",
      "(5, ' | Total Reward:', 28.0)\n",
      "(7, ' | Total Reward:', 30.0)\n",
      "(8, ' | Total Reward:', 24.0)\n",
      "(8, ' | Total Reward:', 31.0)\n",
      "(7, ' | Total Reward:', 16.0)\n",
      "(4, ' | Total Reward:', 29.0)\n",
      "(8, ' | Total Reward:', 46.0)\n",
      "(9, ' | Total Reward:', 14.0)\n",
      "(4, ' | Total Reward:', 15.0)\n",
      "(6, ' | Total Reward:', 13.0)\n",
      "(8, ' | Total Reward:', 32.0)\n",
      "(9, ' | Total Reward:', 10.0)\n",
      "(11, ' | Total Reward:', 16.0)\n",
      "(10, ' | Total Reward:', 23.0)\n",
      "(8, ' | Total Reward:', 14.0)\n",
      "(8, ' | Total Reward:', 21.0)\n",
      "(9, ' | Total Reward:', 19.0)\n",
      "(5, ' | Total Reward:', 15.0)\n",
      "(12, ' | Total Reward:', 14.0)\n",
      "(9, ' | Total Reward:', 13.0)\n",
      " (11, ' | Total Reward:', 15.0)\n",
      "(10, ' | Total Reward:', 19.0)\n",
      "(9, ' | Total Reward:', 24.0)\n",
      "(9, ' | Total Reward:', 26.0)\n",
      "(7, ' | Total Reward:', 38.0)\n",
      "(6, ' | Total Reward:', 27.0)\n",
      "(13, ' | Total Reward:', 21.0)\n",
      "(10, ' | Total Reward:', 20.0)\n",
      "(11, ' | Total Reward:', 16.0)\n",
      "(10, ' | Total Reward:', 49.0)\n",
      "(9, ' | Total Reward:', 57.0)\n",
      "(8, ' | Total Reward:', 14.0)\n",
      "(10, ' | Total Reward:', 38.0)\n",
      "(10, ' | Total Reward:', 22.0)\n",
      "(11, ' | Total Reward:', 14.0)\n",
      "(5, ' | Total Reward:', 74.0)\n",
      "(11, ' | Total Reward:', 22.0)\n",
      "(9, ' | Total Reward:', 21.0)\n",
      "(12, ' | Total Reward:', 34.0)\n",
      "(10, ' | Total Reward:', 48.0)\n",
      "(11, ' | Total Reward:', 27.0)\n",
      "(11, ' | Total Reward:', 24.0)\n",
      "(12, ' | Total Reward:', 29.0)\n",
      "(10, ' | Total Reward:', 27.0)\n",
      "(6, ' | Total Reward:', 16.0)\n",
      "(14, ' | Total Reward:', 40.0)\n",
      "(12, ' | Total Reward:', 13.0)\n",
      "(12, ' | Total Reward:', 11.0)\n",
      "(13, ' | Total Reward:', 20.0)\n",
      "(7, ' | Total Reward:', 14.0)\n",
      "(13, ' | Total Reward:', 16.0)\n",
      "(13, ' | Total Reward:', 11.0)\n",
      "(11, ' | Total Reward:', 26.0)\n",
      "(7, ' | Total Reward:', 62.0)\n",
      "(14, ' | Total Reward:', 16.0)\n",
      "(15, ' | Total Reward:', 39.0)\n",
      "(8, ' | Total Reward:', 28.0)\n",
      "(11, ' | Total Reward:', 38.0)\n",
      "(8, ' | Total Reward:', 18.0)\n",
      "(12, ' | Total Reward:', 100.0)\n",
      "(14, ' | Total Reward:', 32.0)\n",
      "(12, ' | Total Reward:', 30.0)\n",
      "(16, ' | Total Reward:', 16.0)\n",
      "(15, ' | Total Reward:', 24.0)\n",
      "(12, ' | Total Reward:', 57.0)\n",
      "(13, ' | Total Reward:', 60.0)\n",
      "(12, ' | Total Reward:', 15.0)\n",
      "(13, ' | Total Reward:', 20.0)\n",
      "(15, ' | Total Reward:', 19.0)\n",
      "(13, ' | Total Reward:', 14.0)\n",
      "(9, ' | Total Reward:', 26.0)\n",
      "(17, ' | Total Reward:', 21.0)\n",
      "(13, ' | Total Reward:', 21.0)\n",
      "(9, ' | Total Reward:', 33.0)\n",
      "(10, ' | Total Reward:', 95.0)\n",
      "(13, ' | Total Reward:', 35.0)\n",
      "(14, ' | Total Reward:', 22.0)\n",
      "(16, ' | Total Reward:', 41.0)\n",
      "(14, ' | Total Reward:', 32.0)\n",
      "(14, ' | Total Reward:', 92.0)\n",
      "(14, ' | Total Reward:', 17.0)\n",
      "(15, ' | Total Reward:', 19.0)\n",
      "(18, ' | Total Reward:', 38.0)\n",
      "(10, ' | Total Reward:', 39.0)\n",
      "(16, ' | Total Reward:', 64.0)\n",
      "(14, ' | Total Reward:', 80.0)\n",
      "(15, ' | Total Reward:', 41.0)\n",
      "(15, ' | Total Reward:', 28.0)\n",
      "(17, ' | Total Reward:', 30.0)\n",
      "(10, ' | Total Reward:', 80.0)\n",
      "(16, ' | Total Reward:', 44.0)\n",
      "(15, ' | Total Reward:', 61.0)\n",
      "(14, ' | Total Reward:', 94.0)\n",
      "(16, ' | Total Reward:', 34.0)\n",
      "(17, ' | Total Reward:', 75.0)\n",
      "(17, ' | Total Reward:', 11.0)\n",
      "(11, ' | Total Reward:', 30.0)\n",
      "(15, ' | Total Reward:', 57.0)\n",
      "(19, ' | Total Reward:', 79.0)\n",
      "(18, ' | Total Reward:', 36.0)\n",
      "(17, ' | Total Reward:', 44.0)\n",
      "(15, ' | Total Reward:', 32.0)\n",
      "(16, ' | Total Reward:', 65.0)\n",
      "(18, ' | Total Reward:', 36.0)\n",
      "(11, ' | Total Reward:', 130.0)\n",
      "(16, ' | Total Reward:', 53.0)\n",
      "(20, ' | Total Reward:', 32.0)\n",
      "(11, ' | Total Reward:', 113.0)\n",
      "(18, ' | Total Reward:', 31.0)\n",
      "(17, ' | Total Reward:', 36.0)\n",
      "(19, ' | Total Reward:', 43.0)\n",
      "(16, ' | Total Reward:', 53.0)\n",
      "(21, ' | Total Reward:', 35.0)\n",
      "(17, ' | Total Reward:', 21.0)\n",
      "(18, ' | Total Reward:', 120.0)\n",
      "(12, ' | Total Reward:', 41.0)\n",
      "(16, ' | Total Reward:', 79.0)\n",
      "(19, ' | Total Reward:', 46.0)\n",
      "(17, ' | Total Reward:', 63.0)\n",
      "(12, ' | Total Reward:', 101.0)\n",
      "(19, ' | Total Reward:', 22.0)\n",
      "(13, ' | Total Reward:', 28.0)\n",
      "(18, ' | Total Reward:', 18.0)\n",
      "(22, ' | Total Reward:', 45.0)\n",
      " (17, ' | Total Reward:', 27.0)\n",
      "(20, ' | Total Reward:', 27.0)\n",
      "(13, ' | Total Reward:', 25.0)\n",
      "(18, ' | Total Reward:', 75.0)\n",
      "(18, ' | Total Reward:', 13.0)\n",
      "(19, ' | Total Reward:', 18.0)\n",
      "(20, ' | Total Reward:', 83.0)\n",
      "(19, ' | Total Reward:', 20.0)\n",
      "(20, ' | Total Reward:', 24.0)\n",
      "(19, ' | Total Reward:', 38.0)\n",
      "(19, ' | Total Reward:', 146.0)\n",
      "(21, ' | Total Reward:', 19.0)\n",
      "(14, ' | Total Reward:', 69.0)\n",
      "(20, ' | Total Reward:', 76.0)\n",
      "(23, ' | Total Reward:', 69.0)\n",
      "(21, ' | Total Reward:', 66.0)\n",
      "(20, ' | Total Reward:', 49.0)\n",
      "(15, ' | Total Reward:', 32.0)\n",
      "(20, ' | Total Reward:', 54.0)\n",
      "(20, ' | Total Reward:', 69.0)\n",
      "(22, ' | Total Reward:', 47.0)\n",
      "(12, ' | Total Reward:', 200.0)(21, ' | Total Reward:', 59.0)\n",
      "\n",
      "(18, ' | Total Reward:', 166.0)\n",
      "(13, ' | Total Reward:', 15.0)\n",
      "(21, ' | Total Reward:', 59.0)\n",
      "(16, ' | Total Reward:', 48.0)\n",
      "(19, ' | Total Reward:', 12.0)\n",
      "(21, ' | Total Reward:', 42.0)\n",
      "(21, ' | Total Reward:', 36.0)\n",
      "(22, ' | Total Reward:', 21.0)\n",
      "(17, ' | Total Reward:', 32.0)\n",
      "(22, ' | Total Reward:', 38.0)\n",
      "(24, ' | Total Reward:', 111.0)\n",
      "(14, ' | Total Reward:', 195.0)\n",
      "(14, ' | Total Reward:', 65.0)\n",
      "(21, ' | Total Reward:', 200.0)\n",
      "(22, ' | Total Reward:', 167.0)\n",
      "(22, ' | Total Reward:', 96.0)\n",
      "(23, ' | Total Reward:', 45.0)\n",
      "(22, ' | Total Reward:', 110.0)\n",
      "(18, ' | Total Reward:', 68.0)\n",
      "(20, ' | Total Reward:', 88.0)\n",
      "(15, ' | Total Reward:', 44.0)\n",
      "(21, ' | Total Reward:', 15.0)\n",
      "(23, ' | Total Reward:', 45.0)\n",
      "(22, ' | Total Reward:', 43.0)\n",
      "(23, ' | Total Reward:', 145.0)(16, ' | Total Reward:', 21.0)\n",
      "\n",
      "(23, ' | Total Reward:', 114.0)\n",
      "(15, ' | Total Reward:', 96.0)\n",
      "(23, ' | Total Reward:', 38.0)\n",
      "(19, ' | Total Reward:', 94.0)\n",
      "(23, ' | Total Reward:', 102.0)\n",
      "(17, ' | Total Reward:', 91.0)\n",
      "(22, ' | Total Reward:', 101.0)\n",
      "(25, ' | Total Reward:', 176.0)\n",
      "(24, ' | Total Reward:', 75.0)\n",
      "(24, ' | Total Reward:', 141.0)\n",
      "(24, ' | Total Reward:', 93.0)\n",
      "(24, ' | Total Reward:', 119.0)\n",
      "(23, ' | Total Reward:', 200.0)\n",
      "(25, ' | Total Reward:', 59.0)\n",
      "(25, ' | Total Reward:', 48.0)\n",
      "(24, ' | Total Reward:', 20.0)\n",
      "(16, ' | Total Reward:', 163.0)\n",
      "(24, ' | Total Reward:', 200.0)\n",
      "(23, ' | Total Reward:', 109.0)\n",
      "(20, ' | Total Reward:', 136.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, ' | Total Reward:', 127.0)\n",
      "(25, ' | Total Reward:', 127.0)\n",
      "(21, ' | Total Reward:', 68.0)\n",
      "(18, ' | Total Reward:', 200.0)\n",
      "(24, ' | Total Reward:', 200.0)\n",
      "(26, ' | Total Reward:', 120.0)\n",
      "(25, ' | Total Reward:', 118.0)\n",
      "(24, ' | Total Reward:', 108.0)\n",
      "(25, ' | Total Reward:', 200.0)\n",
      "(25, ' | Total Reward:', 121.0)\n",
      "(19, ' | Total Reward:', 44.0)\n",
      "(27, ' | Total Reward:', 52.0)\n",
      "(26, ' | Total Reward:', 200.0)\n",
      "(27, ' | Total Reward:', 129.0)\n",
      "(28, ' | Total Reward:', 32.0)\n",
      "(17, ' | Total Reward:', 200.0)\n",
      "(27, ' | Total Reward:', 48.0)\n",
      "(26, ' | Total Reward:', 108.0)\n",
      "(26, ' | Total Reward:', 112.0)\n",
      "(26, ' | Total Reward:', 200.0)\n",
      "(22, ' | Total Reward:', 200.0)\n",
      "(25, ' | Total Reward:', 200.0)\n",
      "(26, ' | Total Reward:', 200.0)\n",
      "(25, ' | Total Reward:', 200.0)\n",
      "(23, ' | Total Reward:', 56.0)\n",
      "(20, ' | Total Reward:', 200.0)\n",
      "(26, ' | Total Reward:', 27.0)\n",
      "(27, ' | Total Reward:', 92.0)\n",
      "(28, ' | Total Reward:', 163.0)\n",
      "(27, ' | Total Reward:', 73.0)\n",
      "(29, ' | Total Reward:', 199.0)\n",
      "(26, ' | Total Reward:', 106.0)\n",
      "(18, ' | Total Reward:', 200.0)\n",
      "(28, ' | Total Reward:', 200.0)\n",
      "(27, ' | Total Reward:', 76.0)\n",
      "(19, ' | Total Reward:', 26.0)\n",
      "(27, ' | Total Reward:', 200.0)\n",
      "(27, ' | Total Reward:', 199.0)\n",
      "(29, ' | Total Reward:', 104.0)\n",
      "(21, ' | Total Reward:', 119.0)\n",
      "(28, ' | Total Reward:', 18.0)\n",
      "(28, ' | Total Reward:', 145.0)\n",
      "(24, ' | Total Reward:', 157.0)\n",
      "(29, ' | Total Reward:', 46.0)\n",
      "(20, ' | Total Reward:', 118.0)\n",
      "(25, ' | Total Reward:', 74.0)\n",
      "(29, ' | Total Reward:', 155.0)\n",
      "(30, ' | Total Reward:', 200.0)\n",
      "(28, ' | Total Reward:', 150.0)\n",
      "(28, ' | Total Reward:', 200.0)\n",
      "(30, ' | Total Reward:', 32.0)\n",
      "(30, ' | Total Reward:', 148.0)\n",
      "(27, ' | Total Reward:', 200.0)\n",
      "(28, ' | Total Reward:', 191.0)\n",
      "(22, ' | Total Reward:', 200.0)\n",
      "(29, ' | Total Reward:', 200.0)\n",
      "(30, ' | Total Reward:', 167.0)\n",
      "(29, ' | Total Reward:', 107.0)\n",
      "(31, ' | Total Reward:', 123.0)\n",
      "(31, ' | Total Reward:', 33.0)\n",
      "(26, ' | Total Reward:', 163.0)\n",
      "(21, ' | Total Reward:', 179.0)\n",
      "(29, ' | Total Reward:', 170.0)\n",
      "(29, ' | Total Reward:', 162.0)\n",
      "(31, ' | Total Reward:', 165.0)\n",
      "(32, ' | Total Reward:', 56.0)\n",
      "(31, ' | Total Reward:', 200.0)\n",
      "(28, ' | Total Reward:', 193.0)\n",
      "(32, ' | Total Reward:', 72.0)\n",
      "(32, ' | Total Reward:', 51.0)\n",
      "(23, ' | Total Reward:', 168.0)\n",
      "(30, ' | Total Reward:', 146.0)\n",
      "(22, ' | Total Reward:', 132.0)\n",
      "(30, ' | Total Reward:', 200.0)\n",
      "(33, ' | Total Reward:', 88.0)\n",
      "(33, ' | Total Reward:', 67.0)\n",
      "(30, ' | Total Reward:', 135.0)\n",
      "(33, ' | Total Reward:', 102.0)\n",
      "(32, ' | Total Reward:', 133.0)\n",
      "(27, ' | Total Reward:', 200.0)\n",
      "(30, ' | Total Reward:', 200.0)\n",
      "(29, ' | Total Reward:', 200.0)\n",
      "(23, ' | Total Reward:', 143.0)\n",
      "(31, ' | Total Reward:', 119.0)\n",
      "(24, ' | Total Reward:', 200.0)\n",
      "(33, ' | Total Reward:', 104.0)\n",
      "(31, ' | Total Reward:', 200.0)\n",
      "(31, ' | Total Reward:', 182.0)\n",
      "(34, ' | Total Reward:', 200.0)\n",
      "(34, ' | Total Reward:', 195.0)\n",
      "(34, ' | Total Reward:', 153.0)\n",
      "(32, ' | Total Reward:', 75.0)\n",
      "(24, ' | Total Reward:', 155.0)(28, ' | Total Reward:', 200.0)\n",
      "\n",
      "(32, ' | Total Reward:', 123.0)\n",
      "(31, ' | Total Reward:', 200.0)\n",
      "(32, ' | Total Reward:', 40.0)\n",
      "(30, ' | Total Reward:', 200.0)\n",
      "(35, ' | Total Reward:', 161.0)\n",
      "(25, ' | Total Reward:', 181.0)\n",
      "(35, ' | Total Reward:', 159.0)\n",
      "(32, ' | Total Reward:', 200.0)\n",
      "(34, ' | Total Reward:', 200.0)\n",
      "(31, ' | Total Reward:', 50.0)\n",
      "(35, ' | Total Reward:', 200.0)\n",
      "(33, ' | Total Reward:', 200.0)\n",
      "(36, ' | Total Reward:', 132.0)\n",
      "(29, ' | Total Reward:', 200.0)\n",
      "(25, ' | Total Reward:', 200.0)\n",
      "(33, ' | Total Reward:', 200.0)\n",
      "(26, ' | Total Reward:', 159.0)\n",
      "(37, ' | Total Reward:', 29.0)\n",
      "(32, ' | Total Reward:', 140.0)\n",
      "(33, ' | Total Reward:', 180.0)\n",
      "(33, ' | Total Reward:', 200.0)\n",
      "(35, ' | Total Reward:', 171.0)\n",
      "(36, ' | Total Reward:', 199.0)\n",
      "(36, ' | Total Reward:', 163.0)\n",
      "(38, ' | Total Reward:', 131.0)\n",
      "(34, ' | Total Reward:', 200.0)\n",
      "(30, ' | Total Reward:', 183.0)\n",
      "(34, ' | Total Reward:', 147.0)\n",
      "(26, ' | Total Reward:', 200.0)\n",
      "(34, ' | Total Reward:', 200.0)\n",
      "(36, ' | Total Reward:', 161.0)\n",
      "(27, ' | Total Reward:', 200.0)\n",
      "(37, ' | Total Reward:', 168.0)\n",
      "(33, ' | Total Reward:', 200.0)\n",
      "(34, ' | Total Reward:', 200.0)\n",
      "(39, ' | Total Reward:', 116.0)\n",
      "(28, ' | Total Reward:', 51.0)\n",
      "(37, ' | Total Reward:', 200.0)\n",
      "(31, ' | Total Reward:', 134.0)\n",
      "(27, ' | Total Reward:', 138.0)\n",
      "(35, ' | Total Reward:', 200.0)\n",
      "(37, ' | Total Reward:', 173.0)\n",
      "(35, ' | Total Reward:', 200.0)\n",
      "(38, ' | Total Reward:', 95.0)\n",
      "(36, ' | Total Reward:', 33.0)\n",
      "(35, ' | Total Reward:', 200.0)\n",
      "(38, ' | Total Reward:', 200.0)(38, ' | Total Reward:', 51.0)\n",
      "\n",
      "(35, ' | Total Reward:', 178.0)\n",
      "(34, ' | Total Reward:', 200.0)\n",
      "(40, ' | Total Reward:', 200.0)\n",
      "(29, ' | Total Reward:', 200.0)\n",
      "(32, ' | Total Reward:', 200.0)\n",
      "(36, ' | Total Reward:', 121.0)\n",
      "(28, ' | Total Reward:', 200.0)\n",
      "(36, ' | Total Reward:', 120.0)\n",
      "(35, ' | Total Reward:', 85.0)\n",
      "(37, ' | Total Reward:', 141.0)\n",
      "(39, ' | Total Reward:', 126.0)\n",
      "(36, ' | Total Reward:', 140.0)\n",
      "(39, ' | Total Reward:', 196.0)\n",
      "(37, ' | Total Reward:', 39.0)\n",
      "(39, ' | Total Reward:', 200.0)\n",
      "(41, ' | Total Reward:', 200.0)\n",
      "(30, ' | Total Reward:', 200.0)\n",
      "(29, ' | Total Reward:', 152.0)\n",
      "(37, ' | Total Reward:', 164.0)\n",
      "(38, ' | Total Reward:', 156.0)\n",
      "(37, ' | Total Reward:', 195.0)\n",
      "(33, ' | Total Reward:', 200.0)\n",
      "(36, ' | Total Reward:', 194.0)\n",
      "(38, ' | Total Reward:', 148.0)\n",
      "(40, ' | Total Reward:', 200.0)\n",
      "(40, ' | Total Reward:', 182.0)\n",
      "(37, ' | Total Reward:', 58.0)\n",
      "(42, ' | Total Reward:', 134.0)\n",
      "(34, ' | Total Reward:', 67.0)\n",
      "(30, ' | Total Reward:', 114.0)\n",
      " (41, ' | Total Reward:', 23.0)(31, ' | Total Reward:', 108.0)\n",
      "(40, ' | Total Reward:', 140.0)\n",
      "\n",
      "(39, ' | Total Reward:', 99.0)\n",
      "(41, ' | Total Reward:', 31.0)\n",
      "(38, ' | Total Reward:', 101.0)(38, ' | Total Reward:', 66.0)\n",
      "(39, ' | Total Reward:', 38.0)\n",
      "\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "env_test = Environment(render=True, eps_start=0., eps_end=0.)\n",
    "NUM_STATE = env_test.env.observation_space.shape[0]\n",
    "NUM_ACTIONS = env_test.env.action_space.n\n",
    "NONE_STATE = np.zeros(NUM_STATE)\n",
    "\n",
    "brain = Brain() # brain is global in A3C\n",
    "\n",
    "envs = [Environment() for i in range(THREADS)]\n",
    "opts = [Optimizer() for i in range(OPTIMIZERS)]\n",
    "\n",
    "for o in opts:\n",
    "    o.start()\n",
    "\n",
    "for e in envs:\n",
    "    e.start()\n",
    "\n",
    "time.sleep(RUN_TIME)\n",
    "\n",
    "for e in envs:\n",
    "    e.stop()\n",
    "for e in envs:\n",
    "    e.join()\n",
    "\n",
    "for o in opts:\n",
    "    o.stop()\n",
    "for o in opts:\n",
    "    o.join()\n",
    "\n",
    "print(\"Training finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, ' | Total Reward:', 200.0)\n",
      "(2, ' | Total Reward:', 200.0)\n",
      "(3, ' | Total Reward:', 200.0)\n",
      "(4, ' | Total Reward:', 200.0)\n",
      "(5, ' | Total Reward:', 200.0)\n",
      "(6, ' | Total Reward:', 200.0)\n",
      "(7, ' | Total Reward:', 200.0)\n",
      "(8, ' | Total Reward:', 200.0)\n",
      "(9, ' | Total Reward:', 200.0)\n",
      "(10, ' | Total Reward:', 200.0)\n",
      "(11, ' | Total Reward:', 200.0)\n",
      "(12, ' | Total Reward:', 200.0)\n",
      "(13, ' | Total Reward:', 200.0)\n",
      "(14, ' | Total Reward:', 200.0)\n",
      "(15, ' | Total Reward:', 200.0)\n",
      "(16, ' | Total Reward:', 200.0)\n",
      "(17, ' | Total Reward:', 200.0)\n",
      "(18, ' | Total Reward:', 200.0)\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "argument 2: <type 'exceptions.TypeError'>: wrong type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d66dceee573d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-d03fa8a26669>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_signal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunEpisode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d03fa8a26669>\u001b[0m in \u001b[0;36mrunEpisode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTHREAD_DELAY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# yield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/schnack/.local/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/schnack/.local/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/schnack/.local/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/schnack/.local/lib/python2.7/site-packages/gym/envs/classic_control/cartpole.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/schnack/.local/lib/python2.7/site-packages/gym/envs/classic_control/rendering.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/schnack/.local/lib/python2.7/site-packages/pyglet/window/xlib/__init__.pyc\u001b[0m in \u001b[0;36mdispatch_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;31m# Check for the events specific to this window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         while xlib.XCheckWindowEvent(_x_display, _window,\n\u001b[0;32m--> 853\u001b[0;31m                                      0x1ffffff, byref(e)):\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0;31m# Key events are filtered by the xlib window event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# handler so they get a shot at the prefiltered event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument 2: <type 'exceptions.TypeError'>: wrong type"
     ]
    }
   ],
   "source": [
    "env_test.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on things will get too complicated to simplify and present the code, still a good place to start would be the open ai [baselines](https://github.com/openai/baselines). This is a growing collection  of different very good RL algorithms. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
