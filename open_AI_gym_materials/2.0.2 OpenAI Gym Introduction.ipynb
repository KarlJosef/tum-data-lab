{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width:900px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "           }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "            }\n",
       "    h4{\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "             }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width:900px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "     \ttext-align: justify;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "    .prompt{\n",
       "            display: None;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 22pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "          }\n",
       "    \n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                TeX: {\n",
       "                    extensions: [\"AMSmath.js\"]\n",
       "                },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [['$','$']],\n",
       "                    displayMath: [['$$','$$']],\n",
       "                    processEscapes: true\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML  #For a more pleasing rendering...\n",
    "HTML(open(\"styles/custom.css\").read()) #When run in your local notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.1 OpenAI Gym Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font> The OpenAI Gym [Documentation](https://gym.openai.com/docs) is a good point to start. We will recapitulate the most part here and take a closer look in the environments we will use later on. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we generate an environment, for example Pong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-22 10:32:23,011] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('Pong-v0') # create the environment in this case 'Pong-V0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can generate some random actions: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly because we are in a IPython notebook it is not possible to display the games in the cell directly. We will render about 500 frames of the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Local Notebook\n",
    "env.reset() # Always reset the environment in the beginning\n",
    "for _ in range(500):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every game has different action spaces, so all the possible actions you could take. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n"
     ]
    }
   ],
   "source": [
    " print env.action_space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After you take a step with your chosen action you get an observation, reward, done and info back. That is exactly what we need to work with the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space:  Box(210, 160, 3)\n",
      "Observation:  [[[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ..., \n",
      "  [144  72  17]\n",
      "  [144  72  17]\n",
      "  [144  72  17]]\n",
      "\n",
      " [[144  72  17]\n",
      "  [144  72  17]\n",
      "  [144  72  17]\n",
      "  ..., \n",
      "  [144  72  17]\n",
      "  [144  72  17]\n",
      "  [144  72  17]]\n",
      "\n",
      " [[144  72  17]\n",
      "  [144  72  17]\n",
      "  [144  72  17]\n",
      "  ..., \n",
      "  [144  72  17]\n",
      "  [144  72  17]\n",
      "  [144  72  17]]\n",
      "\n",
      " ..., \n",
      " [[236 236 236]\n",
      "  [236 236 236]\n",
      "  [236 236 236]\n",
      "  ..., \n",
      "  [236 236 236]\n",
      "  [236 236 236]\n",
      "  [236 236 236]]\n",
      "\n",
      " [[236 236 236]\n",
      "  [236 236 236]\n",
      "  [236 236 236]\n",
      "  ..., \n",
      "  [236 236 236]\n",
      "  [236 236 236]\n",
      "  [236 236 236]]\n",
      "\n",
      " [[236 236 236]\n",
      "  [236 236 236]\n",
      "  [236 236 236]\n",
      "  ..., \n",
      "  [236 236 236]\n",
      "  [236 236 236]\n",
      "  [236 236 236]]]\n",
      "Reward:  0.0\n",
      "Done:  False\n",
      "Info:  {'ale.lives': 0}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = env.action_space.sample()\n",
    "observation, reward, done, info = env.step(action)\n",
    "print \"Observation space: \", env.observation_space\n",
    "print \"Observation: \", observation\n",
    "print \"Reward: \", reward\n",
    "print \"Done: \", done\n",
    "print \"Info: \", info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>**observation (object)**: an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "$$ $$\n",
    "**reward (float)**: amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "$$ $$\n",
    "**done (boolean)**: whether it's time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "$$ $$\n",
    "**info (dict)**: diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment's last state change). However, official evaluations of your agent are not allowed to use this for learning.\n",
    "$$ $$\n",
    "quoted from the OpenAi Gym [documentation](https://gym.openai.com/docs). </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Ablauf_reinforcement_learning.PNG\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play on your own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running the notebook locally and want to try playing the game on your own, you just have to install the latest version of [pygame](http://www.pygame.org/wiki/GettingStarted#Pygame%20Installation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-22 10:32:56,470] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym.utils.play as pl\n",
    "\n",
    "env = gym.make('Pong-v0')\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pl.play(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant keys are different for every game, sadly there is no way to translate them automatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to show you briefly the environments we will be using. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font> The objective of this game is to cross a frozen lake from $(S)$ to $(G)$. However, not the whole lake is frozen $(F)$ yet, there are some holes $(H)$. To make this game harder, a wind blows the player in any direction from time to time. For this environment, the Q-Learning is the better strategy as it takes the wind into account. The agent now has to learn to find a way from $(S)$ to $(G)$. It can chose between the **four actions** up, down, left, and right. The reward is only given at the very end, when the agents reaches $(G)$. The observation is the current field you are on. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/FrozenLake.PNG\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cart Pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font> A  pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center. Your observation consists of the carts coordinate and speed and also the poles angle and speed. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cartpole.gif\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Atari environments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>There are six different actions in Atari games:\n",
    "$$ $$\n",
    "0 - NOOP   \n",
    "1 - FIRE   \n",
    "2 - RIGHT   \n",
    "3 - LEFT   \n",
    "4 - RIGHTFIRE   \n",
    "5 - LEFTFIRE   \n",
    "$$ $$\n",
    "However, for some of our enviroments, i.e. Pong, we only need the actions RIGHT and LEFT, which is encoded as 2 and 3 respectively. The observation is always a 210x160x3 Array: the RBG image of the screen. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pong\n",
    "<font> Maximize your score in the Atari 2600 game Pong. In this environment each action is repeatedly performed for a duration of $k$ frames, where $k$ is uniformly sampled from ${2,3,4}$. That means we will have random frame skips, will make the whole game much more difficult. \n",
    "$$ $$\n",
    "This game simulates table tennis. The player, controls a paddle to hit the ball back and forth. If one misses the ball gains the other player a point.\n",
    "As you can see, the reward is only given if the other player wins the game, not if you only hit the ball correctly. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/pong.gif\" width=200/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>In order to optimize your code it is worth it to preprocess your observations befor feeding it in the neural network. We will shortly describe our preprocessing for the atari games. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font> For Pong for example we only want to see the important data; so we **crop** the sides and **downsample** the picture with an factor of 2. Then we can set the **color** to 0 for unimportant things like the background and 1 for the paddles and the ball. \n",
    "$$ $$\n",
    "Our output will then be an 1D vector in order to be supported by our network.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-22 10:34:34,294] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = gym.make('Pong-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # Crop\n",
    "    I = I[::2,::2,0] # Downsample by factor of 2\n",
    "    I[I == 144] = 0  # Erase background (background type 1)\n",
    "    I[I == 109] = 0  # Erase background (background type 2)\n",
    "    I[I != 0] = 1    # Everything else (paddles, ball) just set to 1\n",
    "    return I #.astype(np.float).ravel()\n",
    "            # Normally return this as a 1D vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA0VJREFUeJzt3LFtwzAQQFHL8BSewksEniBTZgIhS2gKjRGmSBMgAVKF\nLP57pSCA13xcQ3AbY1yAluvqAYD5hA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4JuMw97ub66Jgj/7P3j\nbfvrHxsfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFoWfj7eVz281h1PKTZ+BAk\nfAgSPgQJH4KmPr313fP+WHU05Nn4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9B\nwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQ\nJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQP\nQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CHotnoAKNjP48e35/2xYJIvNj4ECR+ChA9B\nwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkFd2YYKV\nL+r+xsaHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5Dw\nIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJ\nH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ\n8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAhSPgQJHwIEj4E\nCR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFD\nkPAhSPgQJHwIEj4ECR+ChA9Bwocg4UOQ8CFI+BAkfAgSPgQJH4K2McbqGYDJbHwIEj4ECR+ChA9B\nwocg4UOQ8CFI+BAkfAgSPgQJH4KED0HChyDhQ5DwIUj4ECR8CBI+BAkfgoQPQcKHIOFDkPAh6BOt\nZRT5YB9ZzAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fee90880e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(prepro(env.reset()))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we dont just feed the single frame into the model, we also substract the previous frame(s) in order to get a sense of motion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font> On our remote server it is not possible to render the images directly inline, we still can obtain the RGB-information of the image if they are our observation. Sadly there are few of them, but you can have a look at the atari games for example. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA3ZJREFUeJzt3TFu02AYgGEbcQHwVVjavRKIwyBxEASHQQzd24WrmCuY\nDYm0QiFvG9vp82yJHOlbXv129Nsel2UZgNO9WnsA2DsRQSQiiEQEkYggEhFEIoJIRBCJCKLXaw8w\nDMMwjuM/t018ff/mXKPAH59+/BqPOW4TEe0hkpvrqwff3d7drzDJ/vz8/PG/f/Puy/dnmOR5OJ2D\nSEQQiQiiTVwT8bI8dr1zynXTVliJIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQ2YDK2e15\ns+ljrEQQiQgiEUE0buH9RN8+vF1/CDhw7NN+rEQQiQiiTZzOzfO8/hBwYJomp3NwDiKCSEQQiQgi\nEUEkIohEBJGIIBIRRJvYsWADKtXhjX5P8aY9G1DhTEQEkYggEhFEIoJIRBB57hwX4Sn+0j6VlQgi\nEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBNHFPKjk5vrq\nr8+3d/crTcJLYyWCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRBdzU56b\n8FiLlQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGI\nIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEk\nIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQ\niQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIR\nRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohE\nBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQjcuyrD3DMM/z+kPAgWmaxmOOsxJBJCKI\nRASRiCASEUQigkhEEIkIIhFBJCKIRASRiCASEUQigkhEEIkIok3clAd7ZiWCSEQQiQgiEUEkIohE\nBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIPoNh9I9IJXwClgA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1b978467d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make('Pong-v0')\n",
    "env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "plt.axis('off')\n",
    "for _ in range(100):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
