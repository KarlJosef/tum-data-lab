{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width:900px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "           }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "            }\n",
       "    h4{\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "             }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width:900px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "     \ttext-align: justify;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "    .prompt{\n",
       "            display: None;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 22pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "          }\n",
       "    \n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                TeX: {\n",
       "                    extensions: [\"AMSmath.js\"]\n",
       "                },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [['$','$']],\n",
       "                    displayMath: [['$$','$$']],\n",
       "                    processEscapes: true\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML  #For a more pleasing rendering...\n",
    "HTML(open(\"styles/custom.css\").read()) #When run in your local notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will introduce the topic with two **Bandit Problems** at first and then get started with the **openAI Gym**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./images/example_structure_policy.png\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: n-armed Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a slot machine with $n$ arms, in this case $4$. Every arm has different chances to get a positiv reward; our goal is to maximize the reward over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font>All we need to focus on is learning which rewards we get for each of the possible actions, and ensuring we chose the optimal ones. In the context of reinforcement learning, this is called learning a policy. We are going to be using a method called policy gradients, where our simple neural network learns a policy for picking actions by adjusting itâ€™s weights through gradient descent using feedback from the environment </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List out our bandit arms. \n",
    "# Currently arm 4 (index #3) is set to most often provide a positive reward.\n",
    "bandit_arms = [0.2,0.5,0.3,-2] # Define probabilitys\n",
    "num_arms = len(bandit_arms) # Number of possible arms\n",
    "\n",
    "# Probability of getting a positiv reward\n",
    "def pullBandit(bandit):\n",
    "    # Get a random number.\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        # Return a positive reward.\n",
    "        return 1\n",
    "    else:\n",
    "        # Return a negative reward.\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# These two lines established the feed-forward part of the network. \n",
    "# Note that it is just one layer and we dont have any states as input. \n",
    "weights = tf.Variable(tf.ones([num_arms]))\n",
    "output = tf.nn.softmax(weights)\n",
    "\n",
    "# The next six lines establish the training proceedure. \n",
    "# We feed the reward and chosen action into the network\n",
    "# to compute the loss, and use it to update the network.\n",
    "reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
    "\n",
    "responsible_output = tf.slice(output,action_holder,[1])\n",
    "loss = -(tf.log(responsible_output)*reward_holder)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward for the 4 arms of the bandit: [ 1.  0.  0.  0.]\n",
      "Running reward for the 4 arms of the bandit: [ -2. -14.  -3.  18.]\n",
      "Running reward for the 4 arms of the bandit: [ -9. -11. -12.  45.]\n",
      "Running reward for the 4 arms of the bandit: [-21. -11. -14.  67.]\n",
      "Running reward for the 4 arms of the bandit: [-23. -20. -16.  92.]\n",
      "Running reward for the 4 arms of the bandit: [ -25.  -25.  -20.  121.]\n",
      "Running reward for the 4 arms of the bandit: [ -34.  -28.  -29.  162.]\n",
      "Running reward for the 4 arms of the bandit: [ -39.  -35.  -35.  200.]\n",
      "Running reward for the 4 arms of the bandit: [ -40.  -37.  -42.  236.]\n",
      "Running reward for the 4 arms of the bandit: [ -43.  -42.  -62.  270.]\n",
      "\n",
      "The agent thinks arm 4 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 1000 # Set total number of episodes to train agent on.\n",
    "total_reward = np.zeros(num_arms) # Set scoreboard for bandit arms to 0.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        \n",
    "        probs = sess.run(output) # Compute probabilitys\n",
    "        action = np.random.choice(range(num_arms),p=probs) # Chose one arm with the probability\n",
    "\n",
    "        reward = pullBandit(bandit_arms[action]) \n",
    "        # Get our reward from picking one of the bandit arms.\n",
    "        \n",
    "        # Update the network.\n",
    "        _, ww = sess.run([update,weights], feed_dict={reward_holder:[reward],action_holder:[action]})\n",
    "        \n",
    "        # Update our running memory with scores.\n",
    "        total_reward[action] += reward\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"Running reward for the \" + str(num_arms) + \" arms of the bandit: \" + str(total_reward))\n",
    "        i+=1\n",
    "        \n",
    "print(\"\\nThe agent thinks arm \" + str(np.argmax(ww)+1) + \" is the most promising....\")\n",
    "if np.argmax(ww) == np.argmax(-np.array(bandit_arms)):\n",
    "    print(\"...and it was right!\")\n",
    "else:\n",
    "    print(\"...and it was wrong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting weights to choose an arm.\n",
      "[ 0.8347196   0.80655986  0.76708001  1.57525516]\n"
     ]
    }
   ],
   "source": [
    "print 'Resulting weights to choose an arm.'\n",
    "print ww"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Example: contextual Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>Contextual Bandits introduce the concept of the state. The state consists of a description of the environment that the agent can use to take more informed actions. In our problem, instead of a single bandit, there can now be multiple bandits. The state of the environment tells us which bandit we are dealing with, and the goal of the agent is to learn the best action not just for a single bandit, but for any number of them. Since each bandit will have different reward probabilities for each arm, our agent will need to learn to condition its action on the state of the environment. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>Here we define our contextual bandits. In this example, we are using **three four-armed bandits**. What this means is that each bandit has four arms that can be pulled. Each bandit has different success probabilities for each arm, and as such requires different actions to obtain the best result. Every episode we are in a random **state**, means we are infront of a random bandit. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state = 0\n",
    "#List out our bandits. Currently arms 4, 2, and 1 (respectively) are the most optimal.\n",
    "bandits = np.array([[0.2,0,-0.0,-5],[0.1,-5,1,0.25],[-5,5,5,5]])\n",
    "num_bandits = bandits.shape[0] # Number of bandits\n",
    "num_actions = bandits.shape[1] # Number of arms per bandit\n",
    "        \n",
    "def getBandit(): # Random state at the beginning\n",
    "    state = np.random.randint(0,len(bandits)) #Returns a random state for each episode.\n",
    "    return state\n",
    "        \n",
    "def pullArm(action):\n",
    "    #Get a random number.\n",
    "    bandit_prob = bandits[state,action]\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit_prob:\n",
    "          #return a positive reward.\n",
    "        return 1\n",
    "    else:\n",
    "        #return a negative reward.\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #Clear the Tensorflow graph.\n",
    "\n",
    "#These lines established the feed-forward part of the network. The agent takes a state and produces an action.\n",
    "state_in= tf.placeholder(shape=[1],dtype=tf.int32)\n",
    "state_in_OH = slim.one_hot_encoding(state_in,num_bandits) # Easier to handle encoding \n",
    "output = slim.fully_connected(state_in_OH,num_actions, biases_initializer=None,activation_fn=tf.nn.sigmoid,weights_initializer=tf.ones_initializer())\n",
    "\n",
    "output = tf.reshape(output,[-1])\n",
    "chosen_action = tf.argmax(output,0)\n",
    "\n",
    "# Here we used TensorFlows contrib.slim framework, which allowed us to create a one layer\n",
    "# network in one line. This should give you a taste what possibilities are offered; we sadly\n",
    "# can't present all of them. \n",
    "\n",
    "\n",
    "# The next six lines establish the training proceedure.\n",
    "# We feed the reward and chosen action into the network\n",
    "# to compute the loss, and use it to update the network.\n",
    "reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
    "responsible_weight = tf.slice(output,action_holder,[1])\n",
    "\n",
    "loss = -(tf.log(responsible_weight)*reward_holder)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for each of the 3 bandits: [ 0.    0.    0.25]\n",
      "Mean reward for each of the 3 bandits: [ 16.75  36.5   34.5 ]\n",
      "Mean reward for each of the 3 bandits: [ 60.5   69.    68.75]\n",
      "Mean reward for each of the 3 bandits: [ 102.    104.5   106.75]\n",
      "Mean reward for each of the 3 bandits: [ 139.25  144.    139.5 ]\n",
      "Mean reward for each of the 3 bandits: [ 178.5   179.5   176.25]\n",
      "Mean reward for each of the 3 bandits: [ 216.5   218.5   213.75]\n",
      "Mean reward for each of the 3 bandits: [ 254.5   254.25  254.5 ]\n",
      "Mean reward for each of the 3 bandits: [ 295.75  292.    285.  ]\n",
      "Mean reward for each of the 3 bandits: [ 333.75  331.5   319.  ]\n",
      "Mean reward for each of the 3 bandits: [ 373.5   366.    353.75]\n",
      "Mean reward for each of the 3 bandits: [ 411.5   403.5   386.75]\n",
      "Mean reward for each of the 3 bandits: [ 447.    445.25  423.  ]\n",
      "Mean reward for each of the 3 bandits: [ 491.5   476.5   456.75]\n",
      "Mean reward for each of the 3 bandits: [ 529.75  513.    493.5 ]\n",
      "Mean reward for each of the 3 bandits: [ 565.25  550.25  530.25]\n",
      "Mean reward for each of the 3 bandits: [ 600.25  593.    563.  ]\n",
      "Mean reward for each of the 3 bandits: [ 641.75  629.    593.5 ]\n",
      "Mean reward for each of the 3 bandits: [ 683.75  663.75  629.25]\n",
      "Mean reward for each of the 3 bandits: [ 721.    707.5   661.75]\n",
      "The agent thinks action 4 for bandit 1 is the most promising....\n",
      "...and it was right!\n",
      "The agent thinks action 2 for bandit 2 is the most promising....\n",
      "...and it was right!\n",
      "The agent thinks action 1 for bandit 3 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "weights = tf.trainable_variables()[0] # The weights we will evaluate to look into the network.\n",
    "\n",
    "total_episodes = 10000 # Set total number of episodes to train agent on.\n",
    "total_reward = np.zeros([num_bandits,num_actions]) # Set scoreboard for bandits to 0.\n",
    "e = 0.1 # Set the chance of taking a random action. e-greedy approach\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        state = getBandit() # Get a state from the environment.\n",
    "        \n",
    "        # Choose either a random action or one from our network.\n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(num_actions)\n",
    "        else:\n",
    "            action = sess.run(chosen_action,feed_dict={state_in:[state]})\n",
    "        \n",
    "        reward = pullArm(action) # Get our reward for taking an action given a bandit.\n",
    "        \n",
    "        # Update the network.\n",
    "        feed_dict={reward_holder:[reward],action_holder:[action],state_in:[state]}\n",
    "        _,ww = sess.run([update,weights], feed_dict=feed_dict)\n",
    "         \n",
    "        # Update our running memory with scores.\n",
    "        total_reward[state,action] += reward\n",
    "        if i % 500 == 0:\n",
    "            print(\"Mean reward for each of the \" + str(num_bandits) + \" bandits: \" + str(np.mean(total_reward,axis=1)))\n",
    "        i+=1\n",
    "for a in range(num_bandits):\n",
    "    print(\"The agent thinks action \" + str(np.argmax(ww[a])+1) + \" for bandit \" + str(a+1) + \" is the most promising....\")\n",
    "    if np.argmax(ww[a]) == np.argmin(bandits[a]):\n",
    "        print(\"...and it was right!\")\n",
    "    else:\n",
    "        print(\"...and it was wrong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting weights to choose an arm.\n",
      "The row index is the bandit number and the collumn the arm number\n",
      "[[ 0.99677056  0.99650079  1.00107944  1.63615358]\n",
      " [ 0.99946415  1.63713121  0.98513019  0.99136895]\n",
      " [ 1.63468564  0.97667587  0.97831613  0.97776961]]\n"
     ]
    }
   ],
   "source": [
    "print 'Resulting weights to choose an arm.'\n",
    "print 'The row index is the bandit number and the collumn the arm number'\n",
    "print ww"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Simple Policy solving CartPole-Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now with a similar idea we can solve our first environment the only difference this time: our state is determined by our previous action.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-26 16:07:38,206] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "def discount_rewards(r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 16 # number of hidden layer neurons\n",
    "learning_rate = 1e-2 # feel free to play with this to train faster or more stably.\n",
    "gamma = 0.99 # discount factor for reward\n",
    "\n",
    "D = 4 # input dimensionality\n",
    "\n",
    "logs_path = '/tmp/logs_pole/pole_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#This defines the network as it goes from taking an observation of the environment to \n",
    "#giving a probability of chosing to the action of moving left or right.\n",
    "observations = tf.placeholder(tf.float32, [None,D] , name=\"observations\")\n",
    "\n",
    "with tf.name_scope('Model'):\n",
    "    W1 = tf.get_variable(\"W1\", shape=[D, H],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "    layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "    W2 = tf.get_variable(\"W2\", shape=[H, 1],\n",
    "               initializer=tf.contrib.layers.xavier_initializer())\n",
    "    score = tf.matmul(layer1,W2)\n",
    "    probability = tf.nn.sigmoid(score)\n",
    "\n",
    "#From here we define the parts of the network needed for learning a good policy.\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "\n",
    "# The loss function. This sends the weights in the direction of making actions \n",
    "# that gave good advantage (reward over time) more likely, and actions that didn't less likely.\n",
    "# If input_y is 0 -> log-likelyhood is log(probability)\n",
    "# If input_y is 1 -> log-lik is log(1-probability)\n",
    "loglik = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "loss = -tf.reduce_mean(loglik * advantages) \n",
    "    \n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "\n",
    "\n",
    "# We just apply gradients after every episode \n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate) # Our optimizer\n",
    "updateGrads = adam.apply_gradients(zip(newGrads,tvars))\n",
    "\n",
    "true_reward = tf.placeholder(tf.float32,name=\"true_reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Data construction for tensorboard output\n",
    "\n",
    "# Create a summary to monitor loss tensor\n",
    "tf.summary.scalar(\"Loss\", -loss)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Reward\", true_reward)\n",
    "\n",
    "# Create summaries to visualize weights\n",
    "#for var in tf.trainable_variables():\n",
    "#    tf.summary.histogram(var.name, var)\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for episode 2 : 19.0\n",
      "Reward for episode 3 : 28.0\n",
      "Reward for episode 4 : 26.0\n",
      "Reward for episode 5 : 23.0\n",
      "Reward for episode 6 : 62.0\n",
      "Reward for episode 7 : 23.0\n",
      "Reward for episode 8 : 19.0\n",
      "Reward for episode 9 : 21.0\n",
      "Reward for episode 10 : 26.0\n",
      "Reward for episode 11 : 17.0\n",
      "Reward for episode 12 : 12.0\n",
      "Reward for episode 13 : 24.0\n",
      "Reward for episode 14 : 41.0\n",
      "Reward for episode 15 : 49.0\n",
      "Reward for episode 16 : 22.0\n",
      "Reward for episode 17 : 35.0\n",
      "Reward for episode 18 : 9.0\n",
      "Reward for episode 19 : 12.0\n",
      "Reward for episode 20 : 30.0\n",
      "Reward for episode 21 : 45.0\n",
      "Reward for episode 22 : 13.0\n",
      "Reward for episode 23 : 13.0\n",
      "Reward for episode 24 : 39.0\n",
      "Reward for episode 25 : 21.0\n",
      "Reward for episode 26 : 12.0\n",
      "Reward for episode 27 : 9.0\n",
      "Reward for episode 28 : 23.0\n",
      "Reward for episode 29 : 39.0\n",
      "Reward for episode 30 : 20.0\n",
      "Reward for episode 31 : 35.0\n",
      "Reward for episode 32 : 39.0\n",
      "Reward for episode 33 : 67.0\n",
      "Reward for episode 34 : 15.0\n",
      "Reward for episode 35 : 15.0\n",
      "Reward for episode 36 : 23.0\n",
      "Reward for episode 37 : 24.0\n",
      "Reward for episode 38 : 76.0\n",
      "Reward for episode 39 : 22.0\n",
      "Reward for episode 40 : 19.0\n",
      "Reward for episode 41 : 18.0\n",
      "Reward for episode 42 : 29.0\n",
      "Reward for episode 43 : 32.0\n",
      "Reward for episode 44 : 16.0\n",
      "Reward for episode 45 : 72.0\n",
      "Reward for episode 46 : 41.0\n",
      "Reward for episode 47 : 74.0\n",
      "Reward for episode 48 : 71.0\n",
      "Reward for episode 49 : 23.0\n",
      "Reward for episode 50 : 15.0\n",
      "Reward for episode 51 : 19.0\n",
      "Reward for episode 52 : 22.0\n",
      "Reward for episode 53 : 37.0\n",
      "Reward for episode 54 : 92.0\n",
      "Reward for episode 55 : 12.0\n",
      "Reward for episode 56 : 39.0\n",
      "Reward for episode 57 : 35.0\n",
      "Reward for episode 58 : 23.0\n",
      "Reward for episode 59 : 35.0\n",
      "Reward for episode 60 : 41.0\n",
      "Reward for episode 61 : 24.0\n",
      "Reward for episode 62 : 17.0\n",
      "Reward for episode 63 : 19.0\n",
      "Reward for episode 64 : 33.0\n",
      "Reward for episode 65 : 18.0\n",
      "Reward for episode 66 : 35.0\n",
      "Reward for episode 67 : 59.0\n",
      "Reward for episode 68 : 25.0\n",
      "Reward for episode 69 : 35.0\n",
      "Reward for episode 70 : 40.0\n",
      "Reward for episode 71 : 68.0\n",
      "Reward for episode 72 : 14.0\n",
      "Reward for episode 73 : 20.0\n",
      "Reward for episode 74 : 34.0\n",
      "Reward for episode 75 : 20.0\n",
      "Reward for episode 76 : 42.0\n",
      "Reward for episode 77 : 41.0\n",
      "Reward for episode 78 : 39.0\n",
      "Reward for episode 79 : 11.0\n",
      "Reward for episode 80 : 33.0\n",
      "Reward for episode 81 : 17.0\n",
      "Reward for episode 82 : 40.0\n",
      "Reward for episode 83 : 12.0\n",
      "Reward for episode 84 : 28.0\n",
      "Reward for episode 85 : 12.0\n",
      "Reward for episode 86 : 16.0\n",
      "Reward for episode 87 : 21.0\n",
      "Reward for episode 88 : 9.0\n",
      "Reward for episode 89 : 73.0\n",
      "Reward for episode 90 : 81.0\n",
      "Reward for episode 91 : 22.0\n",
      "Reward for episode 92 : 41.0\n",
      "Reward for episode 93 : 41.0\n",
      "Reward for episode 94 : 19.0\n",
      "Reward for episode 95 : 49.0\n",
      "Reward for episode 96 : 59.0\n",
      "Reward for episode 97 : 19.0\n",
      "Reward for episode 98 : 30.0\n",
      "Reward for episode 99 : 22.0\n",
      "Reward for episode 100 : 69.0\n",
      "Reward for episode 101 : 15.0\n",
      "Reward for episode 102 : 26.0\n",
      "Reward for episode 103 : 28.0\n",
      "Reward for episode 104 : 32.0\n",
      "Reward for episode 105 : 44.0\n",
      "Reward for episode 106 : 20.0\n",
      "Reward for episode 107 : 28.0\n",
      "Reward for episode 108 : 30.0\n",
      "Reward for episode 109 : 51.0\n",
      "Reward for episode 110 : 37.0\n",
      "Reward for episode 111 : 23.0\n",
      "Reward for episode 112 : 22.0\n",
      "Reward for episode 113 : 48.0\n",
      "Reward for episode 114 : 52.0\n",
      "Reward for episode 115 : 23.0\n",
      "Reward for episode 116 : 34.0\n",
      "Reward for episode 117 : 20.0\n",
      "Reward for episode 118 : 22.0\n",
      "Reward for episode 119 : 24.0\n",
      "Reward for episode 120 : 18.0\n",
      "Reward for episode 121 : 89.0\n",
      "Reward for episode 122 : 49.0\n",
      "Reward for episode 123 : 37.0\n",
      "Reward for episode 124 : 50.0\n",
      "Reward for episode 125 : 55.0\n",
      "Reward for episode 126 : 21.0\n",
      "Reward for episode 127 : 34.0\n",
      "Reward for episode 128 : 52.0\n",
      "Reward for episode 129 : 36.0\n",
      "Reward for episode 130 : 42.0\n",
      "Reward for episode 131 : 35.0\n",
      "Reward for episode 132 : 24.0\n",
      "Reward for episode 133 : 24.0\n",
      "Reward for episode 134 : 27.0\n",
      "Reward for episode 135 : 32.0\n",
      "Reward for episode 136 : 55.0\n",
      "Reward for episode 137 : 17.0\n",
      "Reward for episode 138 : 50.0\n",
      "Reward for episode 139 : 54.0\n",
      "Reward for episode 140 : 63.0\n",
      "Reward for episode 141 : 49.0\n",
      "Reward for episode 142 : 30.0\n",
      "Reward for episode 143 : 33.0\n",
      "Reward for episode 144 : 34.0\n",
      "Reward for episode 145 : 58.0\n",
      "Reward for episode 146 : 42.0\n",
      "Reward for episode 147 : 33.0\n",
      "Reward for episode 148 : 52.0\n",
      "Reward for episode 149 : 33.0\n",
      "Reward for episode 150 : 57.0\n",
      "Reward for episode 151 : 15.0\n",
      "Reward for episode 152 : 128.0\n",
      "Reward for episode 153 : 23.0\n",
      "Reward for episode 154 : 58.0\n",
      "Reward for episode 155 : 44.0\n",
      "Reward for episode 156 : 39.0\n",
      "Reward for episode 157 : 36.0\n",
      "Reward for episode 158 : 51.0\n",
      "Reward for episode 159 : 37.0\n",
      "Reward for episode 160 : 48.0\n",
      "Reward for episode 161 : 49.0\n",
      "Reward for episode 162 : 41.0\n",
      "Reward for episode 163 : 29.0\n",
      "Reward for episode 164 : 17.0\n",
      "Reward for episode 165 : 26.0\n",
      "Reward for episode 166 : 47.0\n",
      "Reward for episode 167 : 60.0\n",
      "Reward for episode 168 : 37.0\n",
      "Reward for episode 169 : 29.0\n",
      "Reward for episode 170 : 37.0\n",
      "Reward for episode 171 : 27.0\n",
      "Reward for episode 172 : 45.0\n",
      "Reward for episode 173 : 53.0\n",
      "Reward for episode 174 : 65.0\n",
      "Reward for episode 175 : 56.0\n",
      "Reward for episode 176 : 78.0\n",
      "Reward for episode 177 : 95.0\n",
      "Reward for episode 178 : 37.0\n",
      "Reward for episode 179 : 35.0\n",
      "Reward for episode 180 : 68.0\n",
      "Reward for episode 181 : 16.0\n",
      "Reward for episode 182 : 38.0\n",
      "Reward for episode 183 : 41.0\n",
      "Reward for episode 184 : 24.0\n",
      "Reward for episode 185 : 66.0\n",
      "Reward for episode 186 : 87.0\n",
      "Reward for episode 187 : 29.0\n",
      "Reward for episode 188 : 18.0\n",
      "Reward for episode 189 : 72.0\n",
      "Reward for episode 190 : 94.0\n",
      "Reward for episode 191 : 49.0\n",
      "Reward for episode 192 : 50.0\n",
      "Reward for episode 193 : 164.0\n",
      "Reward for episode 194 : 134.0\n",
      "Reward for episode 195 : 73.0\n",
      "Reward for episode 196 : 70.0\n",
      "Reward for episode 197 : 70.0\n",
      "Reward for episode 198 : 116.0\n",
      "Reward for episode 199 : 118.0\n",
      "Reward for episode 200 : 67.0\n",
      "Reward for episode 201 : 51.0\n",
      "Reward for episode 202 : 69.0\n",
      "Reward for episode 203 : 50.0\n",
      "Reward for episode 204 : 48.0\n",
      "Reward for episode 205 : 74.0\n",
      "Reward for episode 206 : 74.0\n",
      "Reward for episode 207 : 58.0\n",
      "Reward for episode 208 : 92.0\n",
      "Reward for episode 209 : 119.0\n",
      "Reward for episode 210 : 76.0\n",
      "Reward for episode 211 : 92.0\n",
      "Reward for episode 212 : 105.0\n",
      "Reward for episode 213 : 117.0\n",
      "Reward for episode 214 : 75.0\n",
      "Reward for episode 215 : 34.0\n",
      "Reward for episode 216 : 90.0\n",
      "Reward for episode 217 : 73.0\n",
      "Reward for episode 218 : 35.0\n",
      "Reward for episode 219 : 57.0\n",
      "Reward for episode 220 : 90.0\n",
      "Reward for episode 221 : 74.0\n",
      "Reward for episode 222 : 108.0\n",
      "Reward for episode 223 : 109.0\n",
      "Reward for episode 224 : 73.0\n",
      "Reward for episode 225 : 101.0\n",
      "Reward for episode 226 : 82.0\n",
      "Reward for episode 227 : 72.0\n",
      "Reward for episode 228 : 89.0\n",
      "Reward for episode 229 : 67.0\n",
      "Reward for episode 230 : 152.0\n",
      "Reward for episode 231 : 62.0\n",
      "Reward for episode 232 : 65.0\n",
      "Reward for episode 233 : 58.0\n",
      "Reward for episode 234 : 79.0\n",
      "Reward for episode 235 : 110.0\n",
      "Reward for episode 236 : 43.0\n",
      "Reward for episode 237 : 98.0\n",
      "Reward for episode 238 : 93.0\n",
      "Reward for episode 239 : 54.0\n",
      "Reward for episode 240 : 72.0\n",
      "Reward for episode 241 : 114.0\n",
      "Reward for episode 242 : 102.0\n",
      "Reward for episode 243 : 56.0\n",
      "Reward for episode 244 : 153.0\n",
      "Reward for episode 245 : 151.0\n",
      "Reward for episode 246 : 172.0\n",
      "Reward for episode 247 : 190.0\n",
      "Reward for episode 248 : 148.0\n",
      "Reward for episode 249 : 114.0\n",
      "Reward for episode 250 : 130.0\n",
      "Reward for episode 251 : 71.0\n",
      "Reward for episode 252 : 200.0\n",
      "Reward for episode 253 : 58.0\n",
      "Reward for episode 254 : 155.0\n",
      "Reward for episode 255 : 63.0\n",
      "Reward for episode 256 : 200.0\n",
      "Reward for episode 257 : 97.0\n",
      "Reward for episode 258 : 30.0\n",
      "Reward for episode 259 : 200.0\n",
      "Reward for episode 260 : 119.0\n",
      "Reward for episode 261 : 120.0\n",
      "Reward for episode 262 : 57.0\n",
      "Reward for episode 263 : 67.0\n",
      "Reward for episode 264 : 135.0\n",
      "Reward for episode 265 : 138.0\n",
      "Reward for episode 266 : 190.0\n",
      "Reward for episode 267 : 159.0\n",
      "Reward for episode 268 : 190.0\n",
      "Reward for episode 269 : 123.0\n",
      "Reward for episode 270 : 179.0\n",
      "Reward for episode 271 : 200.0\n",
      "Reward for episode 272 : 175.0\n",
      "Reward for episode 273 : 193.0\n",
      "Reward for episode 274 : 200.0\n",
      "Reward for episode 275 : 189.0\n",
      "Reward for episode 276 : 200.0\n",
      "Reward for episode 277 : 125.0\n",
      "Reward for episode 278 : 136.0\n",
      "Reward for episode 279 : 111.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for episode 280 : 200.0\n",
      "Reward for episode 281 : 73.0\n",
      "Reward for episode 282 : 127.0\n",
      "Reward for episode 283 : 172.0\n",
      "Reward for episode 284 : 89.0\n",
      "Reward for episode 285 : 200.0\n",
      "Reward for episode 286 : 200.0\n",
      "Reward for episode 287 : 200.0\n",
      "Reward for episode 288 : 184.0\n",
      "Reward for episode 289 : 169.0\n",
      "Reward for episode 290 : 132.0\n",
      "Reward for episode 291 : 200.0\n",
      "Reward for episode 292 : 200.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c9bf57854ba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Run the policy network and get an action to take.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtfprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtfprob\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/schnack/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/schnack/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/schnack/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/schnack/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/schnack/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xs,drs,ys = [],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "total_episodes = 400\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "# Launch the graph\n",
    "sess.run(init)\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "observation = env.reset() # Obtain an initial observation of the environment\n",
    "    \n",
    "while episode_number < total_episodes:\n",
    "            \n",
    "    # Make sure the observation is in a shape the network can handle.\n",
    "    x = np.reshape(observation,[1,D])\n",
    "    \n",
    "    # Run the policy network and get an action to take. \n",
    "    tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "    action = 1 if np.random.uniform() < tfprob else 0\n",
    "        \n",
    "    xs.append(x) # observation\n",
    "    y = 1 if action == 0 else 0 # a \"fake label\"\n",
    "    ys.append(y)\n",
    "\n",
    "    # step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "    if done: \n",
    "        episode_number += 1\n",
    "        # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "        epx = np.vstack(xs)\n",
    "        epy = np.vstack(ys)\n",
    "        epr = np.vstack(drs)\n",
    "        xs,drs,ys = [],[],[] # reset array memory\n",
    "\n",
    "        # compute the discounted reward backwards through time\n",
    "        discounted_epr = discount_rewards(epr)\n",
    "        # size the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr //= np.std(discounted_epr)\n",
    "            \n",
    "        summary = sess.run(merged_summary_op, feed_dict={observations: epx, input_y: epy, advantages: discounted_epr, true_reward: reward_sum})\n",
    "        # Write logs at every iteration\n",
    "        summary_writer.add_summary(summary, episode_number)\n",
    "        \n",
    "        \n",
    "        # If we have completed enough episodes, then update the policy network with our gradients.\n",
    "        sess.run(updateGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        \n",
    "        print 'Reward for episode',  episode_number,':', float(reward_sum)\n",
    "                \n",
    "        reward_sum = 0\n",
    "            \n",
    "        observation = env.reset()\n",
    "        \n",
    "print(episode_number,'Episodes completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reward_total = 0\n",
    "t = 10\n",
    "for _ in range(t):\n",
    "    observation = env.reset()\n",
    "    reward_sum = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        x = np.reshape(observation, [1, D])\n",
    "        tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0    \n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        \n",
    "        if done:\n",
    "            reward_total += reward_sum\n",
    "            break\n",
    "\n",
    "print 'Average score: ', reward_total/t            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A more complex task: solving Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_obs = 80 * 80           # dimensionality of observations\n",
    "h = 200                 # number of hidden layer neurons\n",
    "n_actions = 3            # number of available actions\n",
    "learning_rate = 3e-3      # Learning Rate\n",
    "gamma = .99               # discount factor for reward\n",
    "decay = 0.99              # decay rate for RMSProp gradients\n",
    "index = 1\n",
    "save_path= '/tmp/pong-attempt/models/{}/pong.ckpt'.format(index) # Save path\n",
    "logs_path = '/tmp/pong-attempt/logs/{}/'.format(index) #log path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# downsampling\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0  # erase background (background type 1)\n",
    "    I[I == 109] = 0  # erase background (background type 2)\n",
    "    I[I != 0] = 1    # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gamespace \n",
    "env = gym.make(\"Pong-v0\") # environment info\n",
    "observation = env.reset()\n",
    "prev_x = None\n",
    "xs,rs,ys = [],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# initialize model\n",
    "tf_model = {}\n",
    "with tf.variable_scope('layer_one',reuse=False):\n",
    "    xavier_l1 = tf.truncated_normal_initializer(mean=0, stddev=1./np.sqrt(n_obs), dtype=tf.float32)\n",
    "    tf_model['W1'] = tf.get_variable(\"W1\", [n_obs, h], initializer=xavier_l1)\n",
    "with tf.variable_scope('layer_two',reuse=False):\n",
    "    xavier_l2 = tf.truncated_normal_initializer(mean=0, stddev=1./np.sqrt(h), dtype=tf.float32)\n",
    "    tf_model['W2'] = tf.get_variable(\"W2\", [h,n_actions], initializer=xavier_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf operations\n",
    "def tf_discount_rewards(tf_r): #tf_r ~ [game_steps,1]\n",
    "    discount_f = lambda a, v: a*gamma + v;\n",
    "    tf_r_reverse = tf.scan(discount_f, tf.reverse(tf_r,[True, False]))\n",
    "    tf_discounted_r = tf.reverse(tf_r_reverse,[True, False])\n",
    "    return tf_discounted_r\n",
    "\n",
    "def tf_policy_forward(x): #x ~ [1,D]\n",
    "    h = tf.matmul(x, tf_model['W1'])\n",
    "    h = tf.nn.relu(h)\n",
    "    logp = tf.matmul(h, tf_model['W2'])\n",
    "    p = tf.nn.softmax(logp)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf placeholders\n",
    "tf_x = tf.placeholder(dtype=tf.float32, shape=[None, n_obs],name=\"tf_x\")\n",
    "tf_y = tf.placeholder(dtype=tf.float32, shape=[None, n_actions],name=\"tf_y\")\n",
    "tf_epr = tf.placeholder(dtype=tf.float32, shape=[None,1], name=\"tf_epr\")\n",
    "true_reward = tf.placeholder(tf.float32,name=\"true_reward\")\n",
    "\n",
    "# tf reward processing (need tf_discounted_epr for policy gradient wizardry)\n",
    "tf_discounted_epr = tf_discount_rewards(tf_epr)\n",
    "tf_mean, tf_variance= tf.nn.moments(tf_discounted_epr, [0], shift=None, name=\"reward_moments\")\n",
    "tf_discounted_epr -= tf_mean\n",
    "tf_discounted_epr /= tf.sqrt(tf_variance + 1e-6)\n",
    "\n",
    "# tf optimizer op\n",
    "tf_aprob = tf_policy_forward(tf_x)\n",
    "loss = tf.nn.l2_loss(tf_y-tf_aprob)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=decay)\n",
    "tf_grads = optimizer.compute_gradients(loss, var_list=tf.trainable_variables(), grad_loss=tf_discounted_epr)\n",
    "train_op = optimizer.apply_gradients(tf_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf graph initialization\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no saved model to load. starting new session\n"
     ]
    }
   ],
   "source": [
    "# try load saved model\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "load_was_success = True # yes, I'm being optimistic\n",
    "try:\n",
    "    save_dir = '/'.join(save_path.split('/')[:-1])\n",
    "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "    load_path = ckpt.model_checkpoint_path\n",
    "    saver.restore(sess, load_path)\n",
    "except:\n",
    "    print \"no saved model to load. starting new session\"\n",
    "    load_was_success = False\n",
    "else:\n",
    "    print \"loaded model: {}\".format(load_path)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    episode_number = int(load_path.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver()\n",
    "# Data construction for tensorboard output\n",
    "# Create a summary to monitor loss tensor\n",
    "tf.summary.scalar(\"Loss\", loss)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Reward\", true_reward)\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "sess.run(init)\n",
    "\n",
    "# op to write logs to Tensorboard\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "while True:\n",
    "    # preprocess the observation, set input to network to be difference image\n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(n_obs)\n",
    "    prev_x = cur_x\n",
    "\n",
    "    # stochastically sample a policy from the network\n",
    "    feed = {tf_x: np.reshape(x, (1,-1))}\n",
    "    aprob = sess.run(tf_aprob,feed_dict = feed) \n",
    "    aprob = aprob[0,:]\n",
    "    action = np.random.choice(n_actions, p=aprob)\n",
    "    label = np.zeros_like(aprob) ; label[action] = 1\n",
    "\n",
    "    # step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(action+1)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    # record game history\n",
    "    xs.append(x) ; ys.append(label) ; rs.append(reward)\n",
    "    \n",
    "    if done:\n",
    "        # update running reward\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        \n",
    "        # parameter update\n",
    "        feed = {tf_x: np.vstack(xs), tf_epr: np.vstack(rs), tf_y: np.vstack(ys), true_reward: reward_sum}\n",
    "        \n",
    "        summary = sess.run(merged_summary_op, feed_dict=feed )\n",
    "        # Write logs at every iteration\n",
    "        summary_writer.add_summary(summary, episode_number)\n",
    "\n",
    "        _ = sess.run(train_op,feed)\n",
    "        \n",
    "        # print progress console\n",
    "        if episode_number % 10 == 0:\n",
    "            print 'ep {}: reward: {}, mean reward: {:3f}'.format(episode_number, reward_sum, running_reward)\n",
    "        #else:\n",
    "            #print '\\tep {}: reward: {}'.format(episode_number, reward_sum)\n",
    "        \n",
    "        # bookkeeping\n",
    "        xs,rs,ys = [],[],[] # reset game history\n",
    "        episode_number += 1 # the Next Episode\n",
    "        observation = env.reset() # reset env\n",
    "        reward_sum = 0\n",
    "        if episode_number % 50 == 0:\n",
    "            saver.save(sess, save_path, global_step=episode_number)\n",
    "            print \"SAVED MODEL #{}\".format(episode_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained this model on our laptop and achieved an stable average score of +5 after ~2 days of training. To get this simple implementation better more fine tuning is needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./images/pg_pong_reward.png\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/pg_pong_loss.png\" height=\"400\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
