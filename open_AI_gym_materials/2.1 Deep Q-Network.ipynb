{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width:900px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "           }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "            }\n",
       "    h4{\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "             }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width:900px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "     \ttext-align: justify;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "    .prompt{\n",
       "            display: None;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 22pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "          }\n",
       "    \n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                TeX: {\n",
       "                    extensions: [\"AMSmath.js\"]\n",
       "                },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [['$','$']],\n",
       "                    displayMath: [['$$','$$']],\n",
       "                    processEscapes: true\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML  #For a more pleasing rendering...\n",
    "HTML(open(\"styles/custom.css\").read()) #When run in your local notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Deep) Q Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font> A good (and simple) problem for Q-Learning is the FrozenLake environment. The objective of this game is to cross a frozen lake from $(S)$ to $(G)$. However, not the whole lake is frozen $(F)$ yet, there are some holes $(H)$. \n",
    "To make this game harder, a wind blows the player in any direction from time to time. \n",
    "For this environment, the Q-Learning is the better strategy as it takes the wind into account.\n",
    "The agent now has to learn to find a way from $(S)$ to $(G)$. It can chose between the four actions up, down, left, and right. The reward is only given at the very end, when the agents reaches $(G)$.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/FrozenLake.PNG\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>As you can see in the picture above, we have 4x4 possible states.\n",
    "In each of these states, the agent can decide between 4 actions, resulting into a 16x4 Q-Table (16 state lines and 4 actions columns). We start with initializing the Q-table with all zeroes, and updating it accordingly to the rewards given over time.\n",
    "$$ $$\n",
    "For updating our Q-table we use the Bellman equation.\n",
    "This equation states, that the expected final reward is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state. The parameter y is a discount variable. It allows us to decide how much influence the possible future rewards ought to have compared to the prestent reward. \n",
    "By updating in this way, the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state. \n",
    "$$ $$\n",
    "<center>Eq 1. $Q(s,a) = r + γ(max(Q(s’,a’))$ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Table Learning Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import a few things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-22 11:08:57,210] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "import gym # Load the OpenAI Gym\n",
    "import numpy as np # NumPy is the fundamental package for scientific computing with Python\n",
    "\n",
    "env = gym.make('FrozenLake-v0') # Load the enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can implement the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# j =  number of steps\n",
    "# lr = learning rate\n",
    "# y = discount variable\n",
    "# s = state\n",
    "# r = rewards\n",
    "# d = done-Bit\n",
    "# a = action\n",
    "\n",
    "\n",
    "# Initialize table with all zeros with the right dimensions\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "\n",
    "# Set learning parameters \n",
    "# You can play around with them if you want. \n",
    "# The best in our case were: learning_rate= .8, y=.95, and num_episodes=2000\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 2000\n",
    "\n",
    "# Create lists to contain total rewards and steps per episode\n",
    "jList = [] # StepList\n",
    "rList = [] # RewardList\n",
    "\n",
    "# Go through your episodes\n",
    "for i in range(num_episodes): \n",
    "    s = env.reset() # Reset environment and get first new observation\n",
    "    rSum = 0 # Reset reward sum\n",
    "    d = False # Reset the done-Bit\n",
    "    j = 0 # Reset number of steps\n",
    "    \n",
    "    # The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1 \n",
    "        # Choose an action by greedily picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        # Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        # Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rSum += r\n",
    "        s = s1\n",
    "        if d == True: # Are we done?\n",
    "            break\n",
    "    jList.append(j)\n",
    "    rList.append(rSum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Table Values: \n",
      "\n",
      "[[  2.19803186e-01   1.11506483e-02   6.07465281e-03   1.39243329e-03]\n",
      " [  0.00000000e+00   1.25383087e-03   1.29681141e-03   1.57496428e-01]\n",
      " [  1.08148574e-01   3.63110963e-04   2.27412733e-03   0.00000000e+00]\n",
      " [  8.78259523e-06   2.45722606e-03   9.38829145e-06   0.00000000e+00]\n",
      " [  3.62448302e-01   1.45889548e-03   3.46049598e-04   3.31631954e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  7.60871473e-02   1.45111347e-06   2.46350444e-05   2.34747909e-05]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   1.76364561e-04   3.04297410e-03   3.19149583e-01]\n",
      " [  3.21614751e-04   5.85274413e-01   0.00000000e+00   0.00000000e+00]\n",
      " [  3.92597888e-01   2.56844701e-04   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  6.01027645e-04   0.00000000e+00   8.60240599e-01   7.90825849e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   9.94326684e-01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print \"Final Q-Table Values: \\n\"\n",
    "print Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font> Now we want to see our results. For this, we first need to include matplotlib.\n",
    "It of interest to see how many rewards we got over time for each episode, as well as how many steps we took on average. If the agent actually learned something, we should see an increase.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward over time: 977.0\n",
      "Average steps taken: 38\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEtQxhhjAun/AZqIP1VEGPTFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2eabd01c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Reward over time: \" +  str(sum(rList))\n",
    "print \"Average steps taken: \" +  str(sum(jList)/num_episodes)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Reward over time: ')\n",
    "plt.plot(np.cumsum(rList))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Average steps taken:')\n",
    "plt.plot(np.cumsum(jList)[1:]/range(len(jList))[1:])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font> You may ask: 'But what about the real world? It is kind of hard to put it into a table.'\n",
    "This is where Q-Networks come in. \n",
    "Q-Networks act as function approximators, so any possible state can be represented as a vector, which in turn can be used to map the Q-values.\n",
    "$$ $$\n",
    "In the case of the FrozenLake example, we will be using a one-layer network which takes the state encoded in a one-hot vector (1x16), and produces a vector of 4 Q-values, one for each action. Such a simple network acts kind of like a glorified table, with the network weights serving as the old cells. \n",
    "$$ $$\n",
    "The key difference is that we can easily expand the Tensorflow network with added layers, activation functions, and different input types, whereas all that is impossible with a regular table. \n",
    "$$ $$\n",
    "The method of updating is a little different as well. Instead of directly updating our table, with a network we will be using backpropagation and a loss function. \n",
    "$$ $$\n",
    "As loss function we use the sum of squared of the errors (SSE). This loss function is a common choice. Here, the difference between the current predicted Q-values, and the “target” value is computed and the gradients passed through the network. Our Q-target for the chosen action is equivalent to the Q-value computed in Eq1. above.\n",
    "$$ $$\n",
    "<center> Eq2. $Loss = \\sum(Q-target - Q)²$   </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Network Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import a few things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-21 16:25:29,322] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random #random numbers\n",
    "import tensorflow as tf #tensorflow\n",
    "\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the tensorflow graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# These lines establish the feed-forward part of the network used to choose actions\n",
    "inputs1 = tf.placeholder(shape=[1,16],dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([16,4],0,0.01))\n",
    "Qout = tf.matmul(inputs1,W)\n",
    "predict = tf.argmax(Qout,1)\n",
    "\n",
    "# Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "nextQ = tf.placeholder(shape=[1,4],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout)) # Loss function\n",
    "trainer = tf.train.GradientDescentOptimizer(lr) \n",
    "updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# e = chance of random action\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Set learning parameters\n",
    "y = .99\n",
    "e = .1\n",
    "lr = .1\n",
    "num_episodes = 2000\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        rSum = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        \n",
    "        #The Q-Network\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            a,allQ = sess.run([predict,Qout],feed_dict={inputs1:np.identity(16)[s:s+1]})\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = env.action_space.sample()\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a[0])\n",
    "            #Obtain the Q' values by feeding the new state through our network\n",
    "            Q1 = sess.run(Qout,feed_dict={inputs1:np.identity(16)[s1:s1+1]})\n",
    "            #Obtain maxQ' and set our target value for chosen action.\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0,a[0]] = r + y*maxQ1\n",
    "            #Train our network using target and predicted Q values\n",
    "            _,W1 = sess.run([updateModel,W],feed_dict={inputs1:np.identity(16)[s:s+1],nextQ:targetQ})\n",
    "            rSum += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                #Reduce chance of random action as we train the model.\n",
    "                e = 1./((i/50) + 10)\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rSum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward over time: 40.0\n",
      "Average steps taken: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAacAAAEYCAYAAAD4czk4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nljtOTmZmljv/H8fZjMNp6CGaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2e9d3c8f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Reward over time: \" +  str(sum(rList))\n",
    "print \"Average steps taken: \" +  str(sum(jList)/num_episodes)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Reward over time: ')\n",
    "plt.plot(np.cumsum(rList))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Average steps taken:')\n",
    "plt.plot(np.cumsum(jList)[1:]/range(len(jList))[1:])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font> As you can see, networks are not necessarly better than Q-tables. However, if we add another layer and change our network into a deep Q network, it tends to work better than other solutions.\n",
    "To do this, we need to:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Change our single-layer network into a multi-layer convolutional network\n",
    "    \n",
    "2. Implement Experience Replay\n",
    "    \n",
    "3. Use a second “target” network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>The Experience Ray will allow our network to train itself with its own memories, while the second target network will compute the target Q-values during the updates.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>Going from a single-layer network to a multi-layer convolutional network.\n",
    "Implementing Experience Replay, which will allow our network to train itself using stored memories from it’s experience.\n",
    "Utilizing a second “target” network, which we will use to compute target Q-values during our updates.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Change our single-layer network into a multi-layer convolutional network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Deep-Q-layer.png\" width=700/>\n",
    "\n",
    "To construct a convolutional layer in TensorFlow see the [documentation](https://www.tensorflow.org/versions/r0.12/api_docs/python/contrib.layers/higher_level_ops_for_building_neural_network_layers_#convolution2d) for further information. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Implement Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>As pointed out above, the Experience Replay allows the network to train from its experience. By storing the experience and drawing it randomly, we gain a more robust network. It is important to choose randomly as otherwise the data is highly correlated and we will likely overfit and fail to generalize. Each of these experiences are stored as a tuple of $e_t=(s_t,a_t,r_t,s_{t+1})$\n",
    "$$ $$\n",
    "An other point to consider is: Shoud we stick with a way that worked well, or try to find a better solution?\n",
    "This is called the explore-exploit dilemma – should you exploit the known working strategy or explore other, possibly better strategies. </font>\n",
    "\n",
    "**Advantages of Experience Replay**\n",
    "\n",
    "* More efficient use of previous experience, by learning with it multiple times.\n",
    "\n",
    "* Better convergence behaviour when training a function approximator.\n",
    "\n",
    "**Disadvantage of Experience Replay**\n",
    "\n",
    "* It is harder to use multi-step learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Use a second “target” network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font> The third major addition to the DQN that makes it unique is the utilization of a second network during the training procedure. This second network is used to generate the target-Q values that will be used to compute the loss for every action during training. Why not use just use one network for both estimations? The issue is that at every step of training, the Q-network’s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control. The network can become destabilized by falling into feedback loops between the target and estimated Q-values. In order to mitigate that risk, the target network’s weights are fixed, and only periodically or slowly updated to the primary Q-networks values. In this way training can proceed in a more stable manner.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A DQN Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show you the algorithm we'll move on to an other problem as the frozen lake enviroment is too easy and we are bound to overfit. However, the CartPole enviroment works well enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-21 16:52:16,894] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experience Replay\n",
    "\n",
    "This class allows us to store experiences and sample then randomly to train the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 10000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])\n",
    "    \n",
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Q_Network():\n",
    "    def __init__(self):\n",
    "        #These lines establish the feed-forward part of the network used to choose actions\n",
    "        self.inputs = tf.placeholder(shape=[None,4],dtype=tf.float32)\n",
    "        self.Temp = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "        self.keep_per = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "\n",
    "        hidden = slim.fully_connected(self.inputs,16,activation_fn=tf.nn.tanh,biases_initializer=None)\n",
    "        hidden = slim.dropout(hidden,self.keep_per)\n",
    "        self.Q_out = slim.fully_connected(hidden,2,activation_fn=None,biases_initializer=None)\n",
    "        \n",
    "        self.predict = tf.argmax(self.Q_out,1)\n",
    "        self.Q_dist = tf.nn.softmax(self.Q_out/self.Temp)\n",
    "        \n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,2,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Q_out, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        self.nextQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        loss = tf.reduce_sum(tf.square(self.nextQ - self.Q))\n",
    "        trainer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        self.updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set learning parameters\n",
    "y = .99 #Discount factor.\n",
    "num_episodes = 20000 #Total number of episodes to train network for.\n",
    "tau = 0.001 #Amount to update target network at each step.\n",
    "batch_size = 32 #Size of training batch\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "anneling_steps = 200000 #How many steps of training to reduce startE to endE.\n",
    "pre_train_steps = 50000 #Number of steps used before training updates begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/schnack/.local/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-21 16:52:17,540] From /home/schnack/.local/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: 22.08 Episode 100 from 20000\n",
      "Mean Reward: 21.25 Episode 200 from 20000\n",
      "Mean Reward: 22.11 Episode 300 from 20000\n",
      "Mean Reward: 23.72 Episode 400 from 20000\n",
      "Mean Reward: 22.55 Episode 500 from 20000\n",
      "Mean Reward: 22.78 Episode 600 from 20000\n",
      "Mean Reward: 23.14 Episode 700 from 20000\n",
      "Mean Reward: 22.68 Episode 800 from 20000\n",
      "Mean Reward: 21.33 Episode 900 from 20000\n",
      "Mean Reward: 22.83 Episode 1000 from 20000\n",
      "Mean Reward: 24.38 Episode 1100 from 20000\n",
      "Mean Reward: 22.98 Episode 1200 from 20000\n",
      "Mean Reward: 24.4 Episode 1300 from 20000\n",
      "Mean Reward: 23.96 Episode 1400 from 20000\n",
      "Mean Reward: 23.66 Episode 1500 from 20000\n",
      "Mean Reward: 21.4 Episode 1600 from 20000\n",
      "Mean Reward: 22.33 Episode 1700 from 20000\n",
      "Mean Reward: 23.35 Episode 1800 from 20000\n",
      "Mean Reward: 23.51 Episode 1900 from 20000\n",
      "Mean Reward: 21.98 Episode 2000 from 20000\n",
      "Mean Reward: 22.33 Episode 2100 from 20000\n",
      "Mean Reward: 20.68 Episode 2200 from 20000\n",
      "Mean Reward: 21.28 Episode 2300 from 20000\n",
      "Mean Reward: 22.15 Episode 2400 from 20000\n",
      "Mean Reward: 22.25 Episode 2500 from 20000\n",
      "Mean Reward: 23.06 Episode 2600 from 20000\n",
      "Mean Reward: 22.0 Episode 2700 from 20000\n",
      "Mean Reward: 22.31 Episode 2800 from 20000\n",
      "Mean Reward: 22.06 Episode 2900 from 20000\n",
      "Mean Reward: 21.88 Episode 3000 from 20000\n",
      "Mean Reward: 21.96 Episode 3100 from 20000\n",
      "Mean Reward: 23.09 Episode 3200 from 20000\n",
      "Mean Reward: 25.67 Episode 3300 from 20000\n",
      "Mean Reward: 25.55 Episode 3400 from 20000\n",
      "Mean Reward: 24.53 Episode 3500 from 20000\n",
      "Mean Reward: 30.53 Episode 3600 from 20000\n",
      "Mean Reward: 29.55 Episode 3700 from 20000\n",
      "Mean Reward: 32.38 Episode 3800 from 20000\n",
      "Mean Reward: 35.24 Episode 3900 from 20000\n",
      "Mean Reward: 32.65 Episode 4000 from 20000\n",
      "Mean Reward: 36.77 Episode 4100 from 20000\n",
      "Mean Reward: 39.77 Episode 4200 from 20000\n",
      "Mean Reward: 49.06 Episode 4300 from 20000\n",
      "Mean Reward: 51.29 Episode 4400 from 20000\n",
      "Mean Reward: 53.81 Episode 4500 from 20000\n",
      "Mean Reward: 65.3 Episode 4600 from 20000\n",
      "Mean Reward: 62.65 Episode 4700 from 20000\n",
      "Mean Reward: 67.0 Episode 4800 from 20000\n",
      "Mean Reward: 72.65 Episode 4900 from 20000\n",
      "Mean Reward: 85.18 Episode 5000 from 20000\n",
      "Mean Reward: 85.86 Episode 5100 from 20000\n",
      "Mean Reward: 91.7 Episode 5200 from 20000\n",
      "Mean Reward: 94.7 Episode 5300 from 20000\n",
      "Mean Reward: 90.5 Episode 5400 from 20000\n",
      "Mean Reward: 98.83 Episode 5500 from 20000\n",
      "Mean Reward: 98.94 Episode 5600 from 20000\n",
      "Mean Reward: 105.7 Episode 5700 from 20000\n",
      "Mean Reward: 116.84 Episode 5800 from 20000\n",
      "Mean Reward: 121.41 Episode 5900 from 20000\n",
      "Mean Reward: 136.21 Episode 6000 from 20000\n",
      "Mean Reward: 142.85 Episode 6100 from 20000\n",
      "Mean Reward: 146.46 Episode 6200 from 20000\n",
      "Mean Reward: 148.71 Episode 6300 from 20000\n",
      "Mean Reward: 158.63 Episode 6400 from 20000\n",
      "Mean Reward: 163.54 Episode 6500 from 20000\n",
      "Mean Reward: 160.88 Episode 6600 from 20000\n",
      "Mean Reward: 171.37 Episode 6700 from 20000\n",
      "Mean Reward: 164.11 Episode 6800 from 20000\n",
      "Mean Reward: 169.79 Episode 6900 from 20000\n",
      "Mean Reward: 173.75 Episode 7000 from 20000\n",
      "Mean Reward: 172.81 Episode 7100 from 20000\n",
      "Mean Reward: 169.14 Episode 7200 from 20000\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "q_net = Q_Network()\n",
    "target_net = Q_Network()\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "trainables = tf.trainable_variables()\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "jMeans = []\n",
    "rList = []\n",
    "rMeans = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    updateTarget(targetOps,sess)\n",
    "    e = startE\n",
    "    stepDrop = (startE - endE)/anneling_steps\n",
    "    total_steps = 0\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        while j < 999:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                a,allQ = sess.run([q_net.predict,q_net.Q_out],feed_dict={q_net.inputs:[s],q_net.keep_per:1.0})\n",
    "                a = a[0]\n",
    "         \n",
    "                \n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            myBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5]))\n",
    "            \n",
    "            if e > endE and total_steps > pre_train_steps:\n",
    "                e -= stepDrop\n",
    "            \n",
    "            if total_steps > pre_train_steps and total_steps % 5 == 0:\n",
    "                #We use Double-DQN training algorithm\n",
    "                trainBatch = myBuffer.sample(batch_size)\n",
    "                Q1 = sess.run(q_net.predict,feed_dict={q_net.inputs:np.vstack(trainBatch[:,3]),q_net.keep_per:1.0})\n",
    "                Q2 = sess.run(target_net.Q_out,feed_dict={target_net.inputs:np.vstack(trainBatch[:,3]),target_net.keep_per:1.0})\n",
    "                end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                doubleQ = Q2[range(batch_size),Q1]\n",
    "                targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                _ = sess.run(q_net.updateModel,feed_dict={q_net.inputs:np.vstack(trainBatch[:,0]),q_net.nextQ:targetQ,q_net.keep_per:1.0,q_net.actions:trainBatch[:,1]})\n",
    "                updateTarget(targetOps,sess)\n",
    "\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            total_steps += 1\n",
    "            if d == True:\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        if i % 100 == 0 and i != 0:\n",
    "            r_mean = np.mean(rList[-100:])\n",
    "            j_mean = np.mean(jList[-100:])\n",
    "            print(\"Mean Reward: \" + str(r_mean) + \" Episode \" + str(i) + \" from \"+ str(num_episodes))\n",
    "            rMeans.append(r_mean)\n",
    "            jMeans.append(j_mean)\n",
    "print(\"Average reward: \" + str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Average Reward: \" + str(sum(rList)/num_episodes)\n",
    "\n",
    "plt.title('Reward over time: ')\n",
    "plt.plot(rMeans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
